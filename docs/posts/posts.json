[
  {
    "path": "posts/2024-12-27-week03-03/",
    "title": "Compound Figures",
    "description": "Showing different variables across subpanels.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\nFaceting is useful whenever we want different rows of the data to appear in\ndifferent panels. What if we want to compare different columns, or work with\nseveral datasets? A more general alternative is to use compound plots. The idea\nis to construct plots separately and then combine them only at the very end.\nThe main advantage of compound plots is that individual panels can be\ntailored to specific visual comparisons, but relationships across panels can\nalso be studied. For example, the plot below shows change in the total number\nand composition of undergraduate majors over the last few decades. In principle,\nthe same information could be communicated using a stacked area plot\n(geom_area). However, comparing the percentages for 1970 and 2015 is much more\nstraightforward using a line plot, and we can still see changes in the overall\nnumber of degrees using the area plot.\n\n\n\nFor reference, here is a non-compound display of the same information.\n\n\n\nThere are a few considerations that can substantially improve the quality of\na compound plot,\nConsistent visual encodings for shared variables\nClear, but unobtrusive annotation\nProper alignment in figure baselines\nWe will discuss each point separately.\nThe figures below are compound plots of a dataset of athlete physiology. They\nare very similar, but the second is better because it enforces a more strict\nconsistency in encodings across panels. Specifically, the male / female variable\nis (1) encoded using the same color scheme across all panels and (2) ordered so\nthat female repeatedly appears on the right of male.\n\n\n\nThe improved, visually consistent approach is given below.\n\n\n\nEffective annotation can be used to refer to different subpanels of the data\nwithout drawing too much attention to itself. Labels should be visible but\nsubtle – not too large, similar fonts as the figures, and logically ordered\n((a) on top left). A nice heuristic is to think of these annotations like page\nnumbers. They are useful for making references, but aren’t something that is\nactively read.\n\n\n\nFor alignment, we will want figure baselines / borders to be consistent.\nMisalignment can be distracting. This is primarily a problem when compound plots\nare made from manually. If we follow the programmatic approaches discussed in\nthe next lecture, we won’t have this issue.\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week03-03/2024-12-27-week03-03_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:20:00+02:00",
    "input_file": "2024-12-27-week03-03.knit.md",
    "preview_width": 1920,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week03-04/",
    "title": "Patchwork",
    "description": "Implementing compound figures in R",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\nIn the last set of notes we discussed principles for designing effective\ncompound figures. In these notes, we’ll review the patchwork R package, which\ncan be used to implement compound figures.\nThis package creates a simple syntax for combining figures,\np1 + p2 concatenates two figures horizontally\np1 / p2 concatenates two figures vertically\nThis idea is simple, but becomes very powerful once we realize that we can\ndefine a whole algebra on plot layouts,\np1 + p2 + p3 concatenates three figures horizontally\np1 / p2 / p3 concatenates three figures vertically\n(p1 + p2) / p3 Concatenates the first two figures horizontally, and places the third below both.\n…\n\nBefore we illustrate the use of this package, let’s read in the athletes data\nfrom the previous notes. The code below constructs the three component plots\nthat we want to combine. Though it looks like a lot of code, it’s just because\nwe are making several plots and styling each one of them. Conceptually, this is\nthe same type of ggplot2 code that we have been using all semester – the only\ndifference is that we save all the figure objects into one list, instead of\nprinting them right away.\n\n\nlibrary(tidyverse)\n library(patchwork)\n \n athletes <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/data/athletes.csv\") %>%\n   filter(sport %in% c(\"basketball\", \"field\", \"rowing\", \"swimming\", \"tennis\", \"track (400m)\")) %>%\n   mutate(sex = recode(sex, \"m\" = \"male\", \"f\" = \"female\"))\n \n p <- list()\n p[[\"bar\"]] <- ggplot(count(athletes, sex)) +\n   geom_bar(aes(sex, n, fill = sex), stat = \"identity\") +\n   scale_y_continuous(expand = c(0, 0)) +\n   scale_fill_brewer(palette = \"Set1\") +\n   labs(y = \"number\")\n \n p[[\"scatter\"]] <- ggplot(athletes) +\n   geom_point(aes(rcc, wcc, col = sex)) +\n   scale_color_brewer(palette = \"Set1\") +\n   theme(legend.position = \"none\") +\n   labs(x = \"RBC count\", y = \"WBC Count\")\n \n p[[\"box\"]] <- ggplot(athletes) +\n   geom_boxplot(aes(sport, pcBfat, col = sex, fill = sex), alpha = 0.5) +\n   scale_color_brewer(palette = \"Set1\") +\n   scale_fill_brewer(palette = \"Set1\") +\n   theme(legend.position = \"none\") +\n   labs(y = \"% body fat\", x = NULL)\n\n\nNow, we use patchwork to combine the subplots using the different\ncombinations discussed above.\n\n\np[[\"bar\"]] + p[[\"scatter\"]] + p[[\"box\"]]\n\n\np[[\"bar\"]] / p[[\"scatter\"]] / p[[\"box\"]]\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]]\n\n\n\nA corollary of using the same encodings across panels is that it should be\npossible to share legends across the entire compound figure. This is most\nconcisely done by setting plot_layout(legend = \"collect\"). For example,\ncompare the athlete physiology dataset with and without the collected legends,\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]] + theme(legend.position = \"left\")) / p[[\"box\"]] # turns legends back on\n\n\n\nThe version with the legends collected is given below.\n\n\n(p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n       plot_layout(guides = \"collect\") &\n       plot_annotation(theme = theme(legend.position = \"bottom\"))\n\n\n\nFor annotation, we can add a title to each figure individually using\nggtitle(), before they are combined into the compound figure. The size and\nfont of the titles can be adjusted by using the theme(title = element_text(...)) option. For example, the code below adds the a - c titles\nfor each subpanel.\n\n\np[[\"bar\"]] <- p[[\"bar\"]] + ggtitle(\"a\")\n p[[\"scatter\"]] <- p[[\"scatter\"]] + ggtitle(\"b\")\n p[[\"box\"]] <- p[[\"box\"]] + ggtitle(\"c\")\n \n (p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n   plot_layout(guides = \"collect\") &\n   plot_annotation(theme = theme(legend.position = \"bottom\", title = element_text(size = 10)))\n\n\n\nPatchwork handles alignment in the background, but sometimes we might want to\nhave control over the relative sizes of different panels. For this, we can again\nuse the plot_layout function, this time using the height and width arguments.\nFor example, the two examples change the height and widths of the first\ncomponent in the layout.\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]] + plot_layout(widths = c(1, 3))) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\")\n\n\n    (p[[\"bar\"]] + p[[\"scatter\"]]) / p[[\"box\"]] +\n      plot_layout(guides = \"collect\", heights = c(1, 3))\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week03-04/2024-12-27-week03-04_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:20:10+02:00",
    "input_file": "2024-12-27-week03-04.knit.md",
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2024-12-27-week04-01/",
    "title": "Elements of a Shiny App",
    "description": "Vocabulary used by the R Shiny Library, and a few example apps.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nRecording, Code\n\n\n\nAll Shiny apps are made up from the same few building blocks. These notes\nreview the main types of blocks. When reading code from more complex apps, it\ncan be helpful to try to classify pieces of the code into these types of blocks.\nThe highest level breakdown of Shiny app code is between ui and server\ncomponents. The ui controls what the app looks like. It stands for “User\nInterface.” The server controls what the app does. For example, the app\nbelow defines a title and textbox where users can type. But it does not do\nanything, since the server is empty.\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\")  # first arg is ID, second is label\n )\n \n server <- function(input, output) {}\n app <- shinyApp(ui, server)\n\n\n\nThe UI elements can be further broken down into Inputs, Outputs, and\nDescriptors1, all grouped together by an organizing\nlayout function. Inputs are UI elements that users can manipulate to prompt\ncertain types of computation. Outputs are parts of the interface that reflects\nthe result of a server computation. Descriptors are parts of the page that\naren’t involved in computation, but which provide narrative structure and guide\nthe user.\nFor example, in the toy app above, titlePage is a descriptor providing\nsome title text. textInput is an input element allowing users to enter\ntext. fluidPage is a layout function that arranges these elements on a\ncontinuous page (some other layout functions are sidebarLayout,\nnavbarPage, flowLayout, …)\nAn important point is that all input and output elements must be given a\nunique ID. This is always the first argument of a *Input or *Output function\ndefined in Shiny. The ID tags are how different parts of the application are\nable to refer to one another. For example, if we wanted to refer to the text the\nuser entered in the application above, we could refer to the name ID.\nLet’s see how to (1) make user inputs cause some sort of computation and (2)\nhave the result of that computation appear to the user. For (1), we will add a\nrenderText element to the server. All render* functions do two things,\nThey make inputs from the ui available for computation.\nThey generate HTML code that allows the results of the computation to\nappear in a UI output.\nFor (2), we will add a textOutput element to the ui layout defined\nabove. Let’s look at the code,\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\"),\n   textOutput(\"printed_name\")\n )\n \n server <- function(input, output) {\n   output$printed_name <- renderText({\n     paste0(\"Welcome to shiny, \", input$name, \"!\")\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nThere are a few points worth noting. First, the renderText component was\nable to refer to the value entered in the textbox using input$name. This was\npossible because name was the ID that we gave to the textInput component. It\nwould not have worked if we had used input$text outside of a render*\nfunction: this is what we mean by the render* functions making the UI inputs\navailable for computation. Finally, we were able to refer to the rendered output\nin the UI by adding a textOutput component. By giving this component the id\nprinted_name, we were able to tell it to look into the server for a rendered\noutput named printed_name and fill it in.\nAn even deeper idea is that the code did not simply run linearly, from top of\nthe script to the bottom. If that were all the code did, then it would have run\nonce at the beginning, and it would never have updated when you entered your\nname. Instead, it ran every time you typed into the textbox. This is the\n“reactive programming” paradigm, and it is what makes interactive visualization\npossible. renderText knows to rerun every time something is entered into the\nname text input, because we told it to depend on input$name. We will explore\nthe idea of reactivity in more depth in the next lecture, but for now, just\nremember that the order in which code is executed is not simply determined by\nthe order of lines in a file.\nLet’s look at a few more examples, just to get a feel for things. The app\nbelow updates a plot of random normal variables given a mean specified by the\nuser. We’ve introduced a new type of input, a numericInput, which captures\nnumbers. We’ve also added a new output, plotOutput, allowing with its\naccompanying renderer, renderPlot (remember, UI outputs are always paired with\nserver renderers).\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0), # 0 is the default\n   plotOutput(\"histogram\")\n )\n \n server <- function(input, output) {\n   output$histogram <- renderPlot({\n     data.frame(values = rnorm(100, input$mean)) %>%\n       ggplot() +\n         geom_histogram(aes(values))\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nWe can make the plot depend on several inputs. The code below allows the user\nto change the total number of data points and the variance, this time using\nslider inputs. I recommend taking a look at different inputs on the shiny\ncheatsheet, though be\naware that there are many\nextensions built by the\ncommunity.\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\")\n )\n \n server <- function(input, output) {\n   output$histogram <- renderPlot({\n     data.frame(values = rnorm(input$n, input$mean, input$sigma)) %>%\n       ggplot() +\n         geom_histogram(aes(values), bins = 100) +\n         xlim(-10, 10)\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nWe can also make the app return several outputs, not just a plot. The code\nbelow attempts to print the data along in addition to the histogram, but it\nmakes a crucial mistake (can you spot it?).\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\"),\n   dataTableOutput(\"dt\")\n )\n \n server <- function(input, output) {\n   output$histogram <- renderPlot({\n     data.frame(values = rnorm(input$n, input$mean, input$sigma)) %>%\n       ggplot() +\n         geom_histogram(aes(values), bins = 100) +\n         xlim(-10, 10)\n   })\n   \n   output$dt <- renderDataTable({\n     data.frame(values = rnorm(input$n, input$mean, input$sigma))\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nThe issue is that this code reruns rnorm for each output. So, even though\nthe interfaces suggests that the printed samples are the same as the ones in the\nhistogram, they are actually different. To resolve this, we need a way of\nstoring an intermediate computation which (1) depends on the inputs but (2)\nfeeds into several outputs. Whenever we encounter this need, we can use a\nreactive expression. It is a type of server element that depends on the input\nand can be referred to directly by outputs, which call the reactive expression\nlike a function. For example, the code below generates the random normal samples\na single time, using the samples() reactive expression.\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\"),\n   dataTableOutput(\"dt\")\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     data.frame(values = rnorm(input$n, input$mean, input$sigma))\n   })\n   \n   output$histogram <- renderPlot({\n       ggplot(samples()) +\n         geom_histogram(aes(values), bins = 100) +\n         xlim(-10, 10)\n   })\n   \n   output$dt <- renderDataTable(samples())\n }\n \n app <- shinyApp(ui, server)\n\n\n\nFinally, a good practice is to move as much non-app related code to separate\nfunctions. This makes the flow of the app more transparent. The clearer the\ndelineation between “computation required for individual app components” and\n“relationship across components,” the easier the code will be to understand and\nextend.\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ### Functions within app components\n generate_data <- function(n, mean, sigma) {\n   data.frame(values = rnorm(n, mean, sigma))\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(aes(values), bins = 100) +\n     xlim(-10, 10)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\"),\n   dataTableOutput(\"dt\")\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(samples()))\n   output$dt <- renderDataTable(samples())\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\nI like to use these names to keep everything organized, but they\nare not standard in the community.↩︎\n",
    "preview": {},
    "last_modified": "2025-01-14T17:20:21+02:00",
    "input_file": "2024-12-27-week04-01.knit.md"
  },
  {
    "path": "posts/2024-12-27-week04-02/",
    "title": "Introduction to Reactivity",
    "description": "Viewing shiny code execution as a graph.",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nRecording, Code\n\n\n\nThese notes will explore the idea of reactivity in more depth. Recall that\nreactivity refers to the fact that Shiny app code is not run from top to bottom,\nlike an ordinary R script. Instead, it runs reactively, depending on inputs that\nthe user has provided. This can make writing Shiny code a bit unintuitive at\nfirst, but there are a few higher-level concepts that can help when writing\nreactive code.\nThe most important of these concepts is that reactive code can be viewed as a\ngraph. The ui and server define an explicit dependency structure for how\ncomponents depend on one another. The input$’s within render* functions in\nthe server specify how UI inputs affect server computations. The IDs within the\n*Output elements in the ui specify which of the rendered output$’s in the\nserver should be used to populate the visible interface.\nFor example, our first “Hello” app has the following (simple) reactivity\ngraph. Note that I’ve drawn input and output nodes differently, to emphasize the\nflow of computation. I’ve also copied the code from the original app for\nreference.\n\n\n\n\n\nlibrary(shiny)\n \n ui <- fluidPage(\n   titlePanel(\"Hello!\"),\n   textInput(\"name\", \"Enter your name\"),\n   textOutput(\"printed_name\")\n )\n \n server <- function(input, output) {\n   output$printed_name <- renderText({\n     paste0(\"Welcome to shiny, \", input$name, \"!\")\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nEven though the graph is simple, note that the outputs will be recomputed\neach time that the input is changed. For more general graphs, all downstream\nnodes will be re-executed whenever an upstream source is changed (typically by a\nuser input, though it’s possible to trigger changes automatically).\nReactive expressions provide a special kind of node that live between inputs\nand outputs. They depend on inputs, and they feed into outputs, but they are\nnever made directly visible to the user. This is why we’ve drawn them as a kind\nof special intermediate node. Below, I’ve drawn the graph for our random normal\nplotter, with the reactive samples() expression.\n\n\n\n\n\nlibrary(shiny)\n library(tidyverse)\n \n ### Functions within app components\n generate_data <- function(n, mean, sigma) {\n   data.frame(values = rnorm(n, mean, sigma))\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(aes(values), bins = 100) +\n     xlim(-10, 10)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   titlePanel(\"Random Normals\"),\n   numericInput(\"mean\", \"Enter the mean\", 0),\n   sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n   sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n   plotOutput(\"histogram\"),\n   dataTableOutput(\"dt\")\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(samples()))\n   output$dt <- renderDataTable(samples())\n }\n \n app <- shinyApp(ui, server)\n\n\n\nA useful perspective is to think of reactive expressions as simplifying the\noverall reactivity graph. Specifically, by adding a reactive node, it’s possible\nto trim away many edges. For example, our initial implementation of the random\nnormal plotter (which didn’t use the reactive expression) has a much more\ncomplicated graph, since many inputs feed directly into outputs.\n\n\n\nLet’s see these principles in action for a similar, but more complex app. The\napp below can be used for power analysis. It simulates two groups of samples,\nboth from normal distributions, but with different (user specified) means. We’ve\nused a reactive expression to generate the samples, so that both the histogram\nand hypothesis test result outputs can refer to the same intermediate simulated\ndata.\n\n\nlibrary(shiny)\n library(tidyverse)\n library(broom)\n \n ### Functions within app components\n generate_data <- function(n, mean1, mean2, sigma) {\n   data.frame(\n     values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n     group = rep(c(\"A\", \"B\"), each = n)\n   )\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(\n       aes(values, fill = group), \n       bins = 100, position = \"identity\",\n       alpha = 0.8\n     ) +\n     xlim(-10, 10)\n }\n \n test_fun <- function(df) {\n   t.test(values ~ group, data = df) %>%\n     tidy() %>%\n     select(p.value, conf.low, conf.high)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   sidebarLayout(\n     sidebarPanel(\n       sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n       sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n       sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n       sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n     ),\n     mainPanel(\n       plotOutput(\"histogram\"),\n       dataTableOutput(\"test_result\")\n     )\n   )\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean1, input$mean2, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n   output$test_result <- renderDataTable(test_fun(generate_data(input$n, input$mean1, input$mean2, input$sigma)))\n }\n \n app <- shinyApp(ui, server)\n\n\n\nOther than that, the only difference is that I’ve saved output from the\nt.test using test_result. Notice the use of the broom package, which\nhelps format the test output into a data.frame.\nSo far, all of our reactive code has lived within the render* or\nreactive() sets of functions. However, there is a another kind that is often\nuseful, especially in more advanced applications: observers. An observer is a\ncomputation that is done every time certain inputs are changed, but which don’t\naffect downstream UI outputs through a render* function. For example, below,\nwe’ve added a block (under observeEvent) that prints to the console every time\neither of the means are changed. I realize it is a bit of a mystery why these\nfunctions would ever be useful, but we will see them in more realistic contexts\nnext week.\n\n\nlibrary(shiny)\n library(tidyverse)\n library(broom)\n \n ### Functions within app components\n generate_data <- function(n, mean1, mean2, sigma) {\n   data.frame(\n     values = c(rnorm(n, mean1, sigma), rnorm(n, mean2, sigma)),\n     group = rep(c(\"A\", \"B\"), each = n)\n   )\n }\n \n histogram_fun <- function(df) {\n   ggplot(df) +\n     geom_histogram(\n       aes(values, fill = group), \n       bins = 100, position = \"identity\",\n       alpha = 0.8\n     ) +\n     xlim(-10, 10)\n }\n \n test_fun <- function(df) {\n   t.test(values ~ group, data = df) %>%\n     tidy() %>%\n     select(p.value, conf.low, conf.high)\n }\n \n ### Defines the app\n ui <- fluidPage(\n   sidebarLayout(\n     sidebarPanel(\n       sliderInput(\"mean1\", \"Mean (Group 1)\", 0, min = -10.0, max = 10.0, step = 0.1),\n       sliderInput(\"mean2\", \"Mean (Group 2)\", 0, min = -10, max = 10, step = 0.1),\n       sliderInput(\"sigma\", \"Enter the standard deviation\", 1, min=.1, max=5),\n       sliderInput(\"n\", \"Enter the number of samples\", 500, min=1, max=2000),\n     ),\n     mainPanel(\n       plotOutput(\"histogram\"),\n       dataTableOutput(\"test_result\")\n     )\n   )\n )\n \n server <- function(input, output) {\n   samples <- reactive({\n     generate_data(input$n, input$mean1, input$mean2, input$sigma)\n   })\n   output$histogram <- renderPlot(histogram_fun(samples()))\n   output$test_result <- renderDataTable(test_fun(samples()))\n   observeEvent(input$mean1 | input$mean2, {\n     message(\"group 1 mean is now: \", input$mean1)\n     message(\"group 2 mean is now: \", input$mean2)\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:20:28+02:00",
    "input_file": "2024-12-27-week04-02.knit.md"
  },
  {
    "path": "posts/2024-12-27-week04-03/",
    "title": "IMDB Shiny Application",
    "description": "Using Shiny to explore a movies dataset",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nRecording, Code\n\n\n\nSo far, all of our Shiny applications have been based on toy simulated data.\nIn this set of notes, we’ll use Shiny to explore a real dataset, illustrating\nthe general development workflow in the process. Before diving into code, let’s\nconsider the role of interactivity in data analysis.\nA major difference between doing visualization on paper and on computers is\nthat visualization on computers can make use of interactivity. An interactive\nvisualization is one that changes in response to user cues. This allows a\ndisplay to update in a way that provides a visual comparison that was not\navailable in a previous view. In this way, interactive visualization allows\nusers to answer a sequence of questions.\nSelection, both of observations and of attributes, is fundamental to\ninteractive visualization. This is because it precedes other interactive\noperations: you can select a subset of observations to filter down to or\nattributes to coordinate across multiple displays (we consider both types of\ninteractivity in later lectures).\nThe code below selects movies to highlight based on Genre. We use a\nselectInput to create the dropdown menu. A reactive expression creates a new\ncolumn (selected) in the movies dataset specifiying whether the current\nmovie is selected. The reactive graph structure means that the ggplot2 figure is\nrecreated each time the selection is changed, and the selected column is used\nto shade in the points. This process of changing the visual encoding of\ngraphical marks depending on user selections is called “conditional encoding.”\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   ggplot(df) +\n     geom_point(\n       aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n     ) +\n     scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n     scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres),\n   plotOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = 1 * (Major_Genre %in% input$genres))\n   })\n   \n   output$ratings_scatter <- renderPlot({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\nWe can extend this further. Let’s allow the user to filter by year and MPAA\nrating. Notice that there are some years in the future! We also find that there\nare systematic differences in IMDB and Rotten Tomatoes ratings as a function of\nthese variables.\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n ratings <- pull(movies, MPAA_Rating) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   ggplot(df) +\n     geom_point(\n       aes(Rotten_Tomatoes_Rating, IMDB_Rating, size = selected, alpha = selected)\n     ) +\n     scale_size(limits = c(0, 1), range = c(.5, 2), guide = \"none\") +\n     scale_alpha(limits = c(0, 1), range = c(.1, 1), guide = \"none\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres, multiple = TRUE),\n   checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n   sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n   plotOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = 1 * (\n         (Major_Genre %in% input$genres) &\n         (MPAA_Rating %in% input$mpaa) &\n         (year >= input$year[1]) &\n         (year <= input$year[2])\n       ))\n   })\n   \n   output$ratings_scatter <- renderPlot({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\n\n\n\nWe’ll include a final version of this plot which additionally shows the movie\nname when points are hovered. To accomplish this, we can no longer use ggplot2\non its own – it has to be linked with a plotting library that renders web-based\nvisualizations (not just static image files). This is what the ggplotly() call\ndoes in the updated version of the app. The mouseover text is added through the\ntooltip argument.\n\n\nlibrary(shiny)\n library(tidyverse)\n library(lubridate)\n library(plotly)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre),\n     MPAA_Rating = fct_explicit_na(MPAA_Rating),\n   )\n \n genres <- pull(movies, Major_Genre) %>%\n   unique() %>%\n   na.omit()\n ratings <- pull(movies, MPAA_Rating) %>%\n   unique() %>%\n   na.omit()\n \n ### functions used in app\n scatterplot <- function(df) {\n   p <- ggplot(mapping = aes(Rotten_Tomatoes_Rating, IMDB_Rating)) +\n     geom_point(data = df %>% filter(selected),  aes(text = Title), size = 2, alpha = 1) +\n     geom_point(data = df %>% filter(!selected),  size = .5, alpha = .1)\n   ggplotly(p, tooltip = \"Title\") %>%\n     style(hoveron = \"fill\")\n }\n \n ### definition of app\n ui <- fluidPage(\n   titlePanel(\"IMDB Analysis\"),\n   selectInput(\"genres\", \"Genre\", genres),\n   checkboxGroupInput(\"mpaa\", \"MPAA Rating\", ratings, ratings),\n   sliderInput(\"year\", \"Year\", min = min(movies$year), max = max(movies$year), c(1928, 2020), sep = \"\"),\n   plotlyOutput(\"ratings_scatter\")\n )\n \n server <- function(input, output) {\n   movies_subset <- reactive({\n     movies %>%\n       mutate(selected = (\n         (Major_Genre %in% input$genres) &\n         (MPAA_Rating %in% input$mpaa) &\n         (year >= input$year[1]) &\n         (year <= input$year[2])\n       ))\n   })\n   \n   output$ratings_scatter <- renderPlotly({\n     scatterplot(movies_subset())\n   })\n }\n \n app <- shinyApp(ui, server)\n\n\n\nThese visualizations are an instance of the more general idea of using\nfiltering to reduce complexity in data. Filtering is an especially powerful\ntechnique in the interactive paradigm, where it is possible to easily reverse\n(or compare) filtering choices.\nConceptually, what we are doing falls under the name of “Dynamic Querying,”\nwhich refers more generally to updating a visualization based on user queries.\nThere are several ways to think about these dynamic queries,\nInterpretation 1: Dynamic queries create the visual analog of a database interaction. Rather than using a programming-based interface to filter elements or select attributes, we can design interactive visual equivalents.\nInterpretation 2: Dynamic queries allow rapid evaluation of conditional probabilities. The visualization above was designed to answer: What is the joint distribution of movie ratings, conditional on being a drama?\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:20:40+02:00",
    "input_file": "2024-12-27-week04-03.knit.md"
  },
  {
    "path": "posts/2024-12-27-week05-01/",
    "title": "Graphical Queries - Click Events",
    "description": "An introduction to click events in Shiny",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nCode, Recording\n\n\n\nSome of the most sophisticated interactive data visualizations are based on\nthe idea that user queries can themselves be defined visually. For example, to\nselect a date range, we could directly interact with a time series plot, rather\nthan relying on a slider input. Or, instead of a long dropdown menu of items, a\nuser could select items by clicking on bars in a bar plot. There are many\nvariations of this idea, but they all leverage graphical (rather than textual)\ndisplays to define queries. The advantage of this approach is that it increases\ninformation density – the selection inputs themselves encode data.\nTo implement this in Shiny, we first need a way of registering user\ninteractions on plots themselves. We will consider two types of plot interaction\nmechanisms: clicks and brushes. These can be specified by adding click or\nbrush events to plotOutput objects.\nThis creates a UI with a single plot on which we will be able to track user\nclicks,\n\n\nui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\")\n )\n\n\nHere, plot_click is an ID that can be used as input$plot_click in the\nserver. We could name it however we want, but we need to be consistent\nacross the UI and server (just like ordinary, non-graphical inputs).\nBefore, we just needed to place the input$id items within render and\nreactive server components, and the associated outputs would automatically\nknow to redraw each time the value of any input was changed. Clicks are treated\nslightly differently. We have to both (a) recognize when a click event has\noccurred and (b) extract relevant information about what the click was referring\nto.\nFor (a), we generally use observeEvent,\n\nobserveEvent(\n  input$plot_click,\n  ... things to do when the plot is clicked ...\n)\n\nThis piece of code will be run anytime the plot is clicked.\nFor (b), we can use the nearPoints helper function. Suppose the plot was\nmade using the data.frame x. Then\n\n\nnearPoints(x, input$click)\n\n\nwill return the samples in x that are close to the clicked location. We\nwill often use a variant of this code that doesn’t just return the closeby\nsamples – it returns all samples, along with their distance from the\nclicked location,\n\n\nnearPoints(x, input$click, allRows = TRUE, addDist = TRUE)\n\n\nWe are almost ready to build a visualization whose outputs respond to\ngraphical queries. Suppose we want a scatterplot where point sizes update\naccording to their distance from the user’s click. Everytime the plot is\nclicked, we need to update the set of distances between samples and the clicked\npoint. We then need to rerender the plot to reflect the new distances. This\nlogic is captured by the block below,\n\n\nserver <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(x)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(x, input$plot_click))\n   )\n   \n   output$plot <- renderPlot({\n     scatter(x, dist())\n   })\n }\n\n\nThe code above uses one new concept, the reactiveVal on the first line of\nthe function. It is a variable that doesn’t directly depend on any inputs,\nwhich can become a source node for downstream reactive and render nodes\nin the reactive graph. Anytime the variable’s value is changed, all\ndownstream nodes will be recomputed. A very common pattern is use an\nobserveEvent to update a reactiveVal every time a graphical query is\nperformed. Any plots that depend on this value will then be updated. For\nexample,\n\nval <- reactiveVal(initial_val) # initialize the reactive value\n\nobserveEvent(\n  ...some input event...\n  ...do some computation...\n  val(new_value) # update val to new_val\n)\n\n# runs each time the reactiveVal changes\nrenderPlot({\n  val() # get the current value of the reactive value\n})\n\nSo, revisiting the dist in the earlier code block, we see that it is\ninitialized as a vector of 1’s whose length is equal to the number of rows of\nx. Everytime the plot is clicked, we update the value of dist according to\nthe function reset_dist. Finally, the changed value of dist triggers a rerun\nof renderPlot. Let’s look at the full application in action. It makes a\nscatterplot using the cars dataset and resizes points every time the plot is\nclicked.\n\n\nlibrary(tidyverse)\n library(shiny)\n \n # wrapper to get the distances from points to clicks\n reset_dist <- function(x, click) {\n   nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n }\n \n # scatterplot plot with point size dependent on click location\n scatter <- function(x, dists) {\n   x %>%\n     mutate(dist = dists) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, size = dist)) +\n     scale_size(range = c(6, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\")\n )\n \n server <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(mtcars)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(mtcars, input$plot_click))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, dist()))\n }\n \n shinyApp(ui, server)\n\n\n\nThe reset_dist function uses nearPoints to compute the distance between\neach sample and the plot, each time the plot is clicked. The associated reactive\nvalue dist gets changed, which triggers scatterplot to run, and it is\nencoded using size in the downstream ggplot2 figure.\nWe can make the plot more interesting by outputting a table showing the\noriginal dataset. Using the same dist() call, we can sort the table by\ndistance each time the plot is clicked.\n\n\nlibrary(tidyverse)\n library(shiny)\n mtcars <- add_rownames(mtcars)\n \n reset_dist <- function(x, click) {\n   nearPoints(x, click, allRows = TRUE, addDist = TRUE)$dist_\n }\n \n scatter <- function(x, dists) {\n   x %>%\n     mutate(dist = dists) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, size = dist)) +\n     scale_size(range = c(6, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", click = \"plot_click\"),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   dist <- reactiveVal(rep(1, nrow(mtcars)))\n   observeEvent(\n     input$plot_click,\n     dist(reset_dist(mtcars, input$plot_click))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, dist()))\n   output$table <- renderDataTable({\n     mtcars %>%\n       mutate(dist = dist()) %>%\n       arrange(dist)\n   })\n }\n \n shinyApp(ui, server)\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:20:44+02:00",
    "input_file": "2024-12-27-week05-01.knit.md"
  },
  {
    "path": "posts/2024-12-27-week05-02/",
    "title": "Graphical Queries - Brush Events",
    "description": "An introduction to brush events in Shiny.",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nCode, Recording\n\n\n\nClick events are useful for referring to individual samples. However, they\nare not ideal for referring to groups of samples. In this case, a useful type of\nplot input is a brush. This is a selection that can be defined by clicking and\ndragging over a region.\nIn shiny, brush events are treated similarly to click events. For example, to\ndefine a new brush input, we can set the brush argument to plotOutput.\n\n\nui <- fluidPage(\n   plotOutput(\"plot\", brush = \"plot_brush\")\n )\n\n\nJust like the click argument, the value \"plot_brush\" is an ID that can be\nused in the server. Also like in click events, we can setup an observer to\nchange a reactive value every time a brush is drawn1. The\ngeneral pattern is similar to what we had before,\n\nserver <- function(input, output) {\n  selected <- reactiveVal(initial value)\n  observeEvent(\n    input$plot_brush,\n    ... computation using get new_value ...\n    selected(new_value)\n  )\n\n  output$plot <- renderPlot(... use scatter() reactive val...)\n}\n\nThe example below is similar to the plot_click example from the previous\nnotes. Instead of sorting points by proximity to the click, though, prints the\nsubset of rows that have been currently brushed.\n\n\nlibrary(tidyverse)\n library(shiny)\n mtcars <- add_rownames(mtcars)\n \n reset_selection <- function(x, brush) {\n   brushedPoints(x, brush, allRows = TRUE)$selected_\n }\n \n scatter <- function(x, selected_) {\n   x %>%\n     mutate(selected_ = selected_) %>%\n     ggplot() +\n     geom_point(aes(mpg, hp, alpha = as.numeric(selected_))) +\n     scale_alpha(range = c(0.1, 1))\n }\n \n ui <- fluidPage(\n   plotOutput(\"plot\", brush = \"plot_brush\"),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(mtcars)))\n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(mtcars, input$plot_brush))\n   )\n   \n   output$plot <- renderPlot(scatter(mtcars, selected()))\n   output$table <- renderDataTable(filter(mtcars, selected()))\n }\n \n shinyApp(ui, server)\n\n\n\nIt is often useful to combine multi-view composition (i.e., faceting or\ncompound figures) with dynamic queries. The basic idea is to (a) show different\naspects of a dataset using different views, and then (b) link the views using\ndynamic queries. This strategy is sometimes called dynamic linking.\nThe example below implements dynamic linking with the penguins dataset.\nBrushing over either scatterplot highlights the corresponding points in the\nadjacent plot (it also updates the data table). This is a way of understanding\nstructure beyond two dimensions. The implementation is similar to the brushing\nabove, except that the reactive value selected() is called in two renderPlot\ncontexts, leading to changes in both plots every time the brush is moved.\n\n\nlibrary(tidyverse)\n library(shiny)\n penguins <- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\")\n \n reset_selection <- function(x, brush) {\n   brushedPoints(x, brush, allRows = TRUE)$selected_\n }\n \n scatter <- function(x, selected_, var1, var2) {\n   x %>%\n     mutate(selected_ = selected_) %>%\n     ggplot(aes_string(var1, var2)) +\n     geom_point(aes(alpha = as.numeric(selected_), col = species)) +\n     scale_alpha(range = c(0.1, 1))\n }\n \n ui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"scatter1\", brush = \"plot_brush\")),\n     column(6, plotOutput(\"scatter2\", brush = \"plot_brush\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(penguins)))\n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(penguins, input$plot_brush))\n   )\n   \n   output$scatter1 <- renderPlot({\n     scatter(penguins, selected(), \"bill_length_mm\", \"bill_depth_mm\")\n   })\n   output$scatter2 <- renderPlot({\n     scatter(penguins, selected(), \"flipper_length_mm\", \"body_mass_g\")\n   })\n   \n   output$table <- renderDataTable(filter(penguins, selected()))\n }\n \n shinyApp(ui, server)\n\n\n\n\nTechnically, the code\nonly executes when the mouse lifts off the brush selection. Some\nvisualizations will be able to call the updating code every time the mouse is\nmoved with the brush selected. This creates a smoother experience.↩︎\n",
    "preview": {},
    "last_modified": "2025-01-14T17:20:48+02:00",
    "input_file": "2024-12-27-week05-02.knit.md"
  },
  {
    "path": "posts/2024-12-27-week05-03/",
    "title": "Linked Brushing",
    "description": "More examples defining brush queries using Shiny and `ggplot2`.",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nCode, Recording\n\n\n\nThese notes provide more realistic examples of linked brushing. Though the\nvisual design problems they address are more complex, they follow the same\nrecipe described earlier,\nA reactiveVal is defined to track the currently selected samples.\nAn observeEvent is used to update the reactiveVal every time a plot is\nbrushed.\nDownstream render contexts update plot and data table outputs whenever\nthe reactiveVal is changed.\n\nThe first example implements linked brushing on the movie ratings dataset\npresented earlier. Before we used a slider to select movies within a\nuser-specified time range. Our graphical alternative is to allow selections over\na histogram of movie release dates within the dataset. Specifically, we will\ncreate an interactive version of the histogram below,\n\n\nlibrary(tidyverse)\n library(lubridate)\n \n movies <- read_csv(\"https://raw.githubusercontent.com/krisrs1128/stat479_s22/main/_posts/2022-02-10-week04-03/apps/data/movies.csv\") %>%\n   mutate(\n     date = as_date(Release_Date, format = \"%b %d %Y\"),\n     year = year(date),\n     Major_Genre = fct_explicit_na(Major_Genre)\n   )\n \n movies %>% \n   count(year) %>%\n   ggplot(aes(year, n)) +\n   geom_bar(stat = \"identity\", width = 1) +\n   scale_y_continuous(expand = c(0, 0))\n\n\n\nand when a subset of years of has been brushed, we will highlight the\ncorresponding movies in the same kind of scatterplot used in the earlier,\nslider-based implementation.\n\n\nggplot(movies) +\n   geom_point(aes(Rotten_Tomatoes_Rating, IMDB_Rating))\n\n\n\nViewed more abstractly, we are going to use a brush to link the histogram\nand scatterplot views. We will be able to evaluate the change in a\nvisualization (the scatterplot) after “conditioning” on a subset defined by a\ncomplementary view (the histogram). This is analogous to the penguins dataset\nexample – only the form of the base plots has changed.\nThe main logic needed to link these views is given in the block below. The\nhistogram plotOutput in the UI is given a brush which will be used to select\nyears1. We\nuse the selected reactive value to store a list of TRUE/FALSE’s indicating\nwhich movie falls into the currently brushed time range. Each time the brushed\nrange is changed, the output$scatterplot and output$table outputs are\nregenerated, highlighting those movies that appear in the selected() list.\n\n\nui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"histogram\", brush = brushOpts(\"plot_brush\", direction = \"x\"))),\n     column(6, plotOutput(\"scatterplot\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(movies)))\n   \n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(movies, input$plot_brush))\n   )\n   \n   output$histogram <- renderPlot(histogram(movies))\n   output$scatterplot <- renderPlot(scatterplot(movies, selected()))\n   output$table <- renderDataTable(data_table(movies, selected()))\n }\n\n\nWe haven’t included the full code for histogram, scatterplot, and\ndata_table, since they in and of themselves don’t require any logic for\ninteractivity. You can try out the full code\nhere\nand tinker with the interface below.\n\nA natural extension of the previous app is to allow brushing on both the\nhistogram and the scatterplot. Brushing over the scatterplot would show the\nyears during which the selected movies were released – this can be used to find\nout if very poorly or highly rated movies are associated with specific time\nranges, for example.\nThe updated application is below. The main differences are that,\nThe scatterplot plotOutput now includes a brush.\nWe are passing in the reactive value of the selected() movies into the\nhistogram as well.\n\n\nui <- fluidPage(\n   fluidRow(\n     column(6, plotOutput(\"histogram\", brush = brushOpts(\"plot_brush\", direction = \"x\"))),\n     column(6, plotOutput(\"scatterplot\", brush = \"plot_brush\"))\n   ),\n   dataTableOutput(\"table\")\n )\n \n server <- function(input, output) {\n   selected <- reactiveVal(rep(TRUE, nrow(movies)))\n   \n   observeEvent(\n     input$plot_brush,\n     selected(reset_selection(movies, input$plot_brush))\n   )\n   \n   output$histogram <- renderPlot(histogram(movies, selected()))\n   output$scatterplot <- renderPlot(scatterplot(movies, selected()))\n   output$table <- renderDataTable(data_table(movies, selected()))\n }\n \n shinyApp(ui, server)\n\n\nFor the scatterplot, we simply reduced the transparency for the movies that\nweren’t selected. We cannot do this for the histogram, though, because the\nmovies are not directly represented in this plot, only their counts over time.\nInstead, our idea will be to draw two overlapping histograms. A static one in\nthe background will represent the year distribution before any selection. A\nchanging one in the foreground will be redrawn whenever the selected movies are\nchanged. For example, the code below overlays two geom_bar layers, with one\ncorresponding only to the first 500 movies in the dataset.\n\n\n  sub_counts <- movies[1:500, ] %>%\n     count(year)\n   \n   movies %>%\n     count(year) %>%\n     ggplot(aes(year, n)) +\n     geom_bar(stat = \"identity\", fill = \"#d3d3d3\", width = 1) +\n     geom_bar(data = sub_counts, stat = \"identity\", width = 1) +\n     scale_y_continuous(expand = c(0, 0))\n\n\n\nCombining these ideas leads to the app\nhere\nand included below. Try brushing on both the scatterplot and the histogram. The\nespecially interesting thing about this approach is that, without introducing\nany new screen elements, we’ve widened the class of questions of that can be\nanswered. In a sense, we’ve increased the information density of the display –\nwe can present more information without having to introduce any peripheral UI\ncomponents or graphical marks.\n\n\nIn our last problem, we would like to use a dataset of flight delays to\nunderstand what characteristics of the flights make some more / less likely to\nbe delayed. The basic difficulty is that there are many potentially relevant\nvariables, and they might interact in ways that are not obvious in advance.\n\n\nlibrary(nycflights13)\nhead(flights)\n\n# A tibble: 6 × 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# ℹ 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nOur solution strategy will be to dynamically link complementary histograms.\nBy brushing the histogram of delays time, we’ll be able to see the conditional\ndistributions for other variables of interest. In principle, we could do this\nfor every variable in the dataset, but for the example, we’ll focus on just the\nscheduled departure time and flight distance.\nThe UI in this case creates three separate histograms, each of which\nintroduces a brush. We will plan on brushing one histogram at a time, which is\nthen used to update overlays on each.\n\n\n  ui <- fluidPage(\n  fluidRow(\n    column(\n      6, \n      plotOutput(\"h1\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200),\n      plotOutput(\"h2\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200),\n      plotOutput(\"h3\", brush = brushOpts(\"plot_brush\", direction = \"x\"), height = 200)\n    ),\n    column(6, dataTableOutput(\"table\"))\n  ),\n)\n\n\nThe logic for drawing the overlays is encapsulated by the functions below.\nThe bar_plot function draws two bar plots over one another, one referring to a\nglobal counts object of unchanging histogram bar heights. The second refers to\nthe bar heights for the continually updated overlays. Notice that we use\n.data[[v]] to use variable names encoded in strings. The plot_overlay\nfunction provides the histogram bar heights for variable v after brushing over\nthe flights in selected_.\n\n\nbar_plot <- function(sub_flights, v) {\n   ggplot(counts[[v]], aes(.data[[v]], n)) +\n     geom_bar(fill = \"#d3d3d3\", stat = \"identity\") +\n     geom_bar(data = sub_flights, stat = \"identity\")\n }\n \n plot_overlay <- function(selected_, v) {\n   flights %>%\n     filter(selected_) %>%\n     count(.data[[v]]) %>%\n     bar_plot(v)\n }\n\n\nCode for the full application is linked\nhere.\nThanks to shiny’s reactiveVal and brushedPoints definitions, implementing\ninteractivity only requires about 20 lines (starting from ui <- ... to the\nend). The rest of the code is used to draw new static plots depending on the\ncurrent selection.\n\n\nNote that we restrict brush motion to the \\(x\\)-direction. This is because\nthe \\(x\\) direction alone encodes year information, which we want to select.↩︎\n",
    "preview": "posts/2024-12-27-week05-03/2024-12-27-week05-03_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:20:54+02:00",
    "input_file": "2024-12-27-week05-03.knit.md",
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2024-12-27-week05-04/",
    "title": "Linking using Crosstalk",
    "description": "Linking in web-based visualizations.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\n\n\nlibrary(ggplot2)\nlibrary(plotly)\nlibrary(leaflet)\nlibrary(DT)\nlibrary(crosstalk)\n\n\nFor most graphical queries, the click and brush inputs implemented in Shiny\nwill be sufficient. However, a basic limitation of shiny’s plotOutput is that\nit has to create views by generating static image files – it only creates the\nillusion of interactivity by rapidly changing the underlying files. In some\ncases, there will be so many points on the display each update will be slow and\nthe fluidity of interaction will suffer.\nOne approach around this problem is to use a library that directly supports\nweb-based plots. These plots can modify elements in place, without having to\nredraw and save the entire figure on each interaction. The crosstalk package\ngives one approach to linked views in this setting. We’ll only give an overview\nof it here, but the purpose of sharing it is so we have at least one example\nthat we can refer to in case the Shiny approach becomes untenable.\nWe’ll study a problem about the dropoff in Chicago subway ridership after the\nstart of the COVID-19 lockdowns. We have data on the weekday and weekend transit\nridership at each subway station, along with the locations of the stations. We\nare curious about the extent of the change in ridership, along with features\nthat might be responsible for some stations being differentially affected. The\nblock below reads in this raw data,\n\n\ndownload.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/stations.rds?raw=true\", \"stations.rds\")\n download.file(\"https://github.com/emilyriederer/demo-crosstalk/blob/master/data/trips_apr.rds?raw=true\", \"trips_apr.rds\")\n stations <- readRDS(\"stations.rds\")\n trips <- readRDS(\"trips_apr.rds\")\n\n\nThe crosstalk package implements a SharedData object. This is used to track\nselections across all the plots that refer to it. We can think of it as\ncrosstalk’s analog of our earlier brushedPoints function. These objects are\ndefined by calling SharedData$new() on the data.frame which will be used\nacross views. The key argument provides a unique identifier that is used to\nmatch corresponding samples across all displays (notice that it uses ~ formula\nnotation).\n\n\ntrips_ct <- SharedData$new(trips, key = ~station_id, group = \"stations\")\n\n\nLet’s see how this object can be used for linked brushing. We’ll first\ngenerate static ggplot2 objects giving (1) a view of weekday ridership in 2019\nvs. 2020 and (2) a view of what proportion of 2019 ridership at the station took\nplace on weekends.\n\n\np1 <- ggplot(trips_ct) +\n   geom_point(aes(year_2019_wday, year_2020_wday)) +\n   geom_abline(slope = 1, col = \"#0c0c0c\")\n \n p2 <- ggplot(trips_ct) +\n   geom_point(aes(prop_wend_2019, reorder(station_name, prop_wend_2019)), stat = \"identity\")\n\n\nGiven this ggplot2 base, we can build web-based plotly objects using the\nggplotly command. The layout and highlight functions are specifying that\nwe want user interactions to define brushes, not zoom events. The final bscols\nfunction allows us to place the views side by side. By brushing the two plots,\nwe can see that those stations with the largest dropoff in riderships were those\nthat were mostly used during the weekdays. This makes sense, considering many of\nthe office workers in Chicago started working from home in 2020.\n\n\np1 <- ggplotly(p1, tooltip = \"station_name\") %>%\n   layout(dragmode = \"select\") %>%\n   highlight(on = \"plotly_selected\")\n \n p2 <- ggplotly(p2) %>%\n   layout(dragmode = \"select\", direction = \"v\") %>%\n   highlight(on = \"plotly_selected\")\n \n bscols(p1, p2)\n\n\nThis crosstalk approach works with more than plotly-derived plots. In the\nblock below, we also generate a map (using the leaflet package) and a data\ntable (using the DT package). The views are all synchronized because they\nrefer to the same station_id key in SharedData objects. This visualization\nconfirms our intuition that those stations with the largest drop-off in\nridership are those that are downtown.\n\n\nstations_ct <- SharedData$new(stations, key = ~station_id, group = \"stations\")\ndt <- datatable(trips_ct)\nlf <- leaflet(stations_ct) %>% \n  addTiles() %>% \n  addMarkers()\n\nbscols(p1, p2, lf, dt, widths = rep(6, 4))\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:21:03+02:00",
    "input_file": "2024-12-27-week05-04.knit.md"
  },
  {
    "path": "posts/2024-12-27-week06-01/",
    "title": "tsibble Objects",
    "description": "A data structure for managing time series data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(feasts)\nlibrary(tsibbledata)\n\n\nTsibbles are data structures that are designed specifically for storing time series data. They are useful because they create a unified interface to various time series visualization and modeling tasks. This removes the friction of having to transform back and forth between data.frames, lists, and matrices, depending on the particular task of interest.\nThe key difference between a tsibble and an ordinary data.frame is that it\nrequires a temporal key variable, specifying the frequency with which\nobservations are collected. For example, the code below generates a tsibble with\nyearly observations.\n\n\ntsibble(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110),\n  index = Year\n)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\nWe can also create a tsibble from an ordinary data.frame by calling the\nas_tsibble function. The only subtlety is that we have to specify an index.\n\n\nx <- data.frame(\n  Year = 2015:2019,\n  Observation = c(123, 39, 78, 52, 110)\n)\n\nas_tsibble(x, index = Year)\n\n# A tsibble: 5 x 2 [1Y]\n   Year Observation\n  <int>       <dbl>\n1  2015         123\n2  2016          39\n3  2017          78\n4  2018          52\n5  2019         110\n\nThe index is useful because it creates a data consistency check. If a few\ndays are missing from a daily dataset, the index makes it easy to detect and\nfill in these gaps. Notice that when we print a tsibble object, it prints the\nindex and guessed sampling frequency on the top right corner.\n\n\ndays <- seq(as_date(\"2021-01-01\"), as_date(\"2021-01-31\"), by = \"day\")\ndays <- days[-5] # Skip January 5\n\nx <- tsibble(day = days, value = rnorm(30), index = day)\nfill_gaps(x)\n\n# A tsibble: 31 x 2 [1D]\n   day          value\n   <date>       <dbl>\n 1 2021-01-01  1.25  \n 2 2021-01-02 -0.473 \n 3 2021-01-03  0.791 \n 4 2021-01-04 -0.764 \n 5 2021-01-05 NA     \n 6 2021-01-06  0.240 \n 7 2021-01-07  1.28  \n 8 2021-01-08  0.0963\n 9 2021-01-09 -0.714 \n10 2021-01-10  0.642 \n# ℹ 21 more rows\n\nTsibbles can store more than one time series at a time. In this case, we have\nto specify key columns that distinguish between the separate time series. For\nexample, in the olympics running times dataset,\n\n\nolympic_running\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Length, Sex [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\nthe keys are running distance and sex. If we were creating a tsibble from a\ndata.frame containing these multiple time series, we would need to specify the\nkeys. This protects against accidentally having duplicate observations at given\ntimes.\n\n\nolympic_df <- as.data.frame(olympic_running)\nas_tsibble(olympic_df, index = Year, key = c(\"Sex\", \"Length\")) # what happens if we remove key?\n\n# A tsibble: 312 x 4 [4Y]\n# Key:       Sex, Length [14]\n    Year Length Sex    Time\n   <int>  <int> <chr> <dbl>\n 1  1896    100 men    12  \n 2  1900    100 men    11  \n 3  1904    100 men    11  \n 4  1908    100 men    10.8\n 5  1912    100 men    10.8\n 6  1916    100 men    NA  \n 7  1920    100 men    10.8\n 8  1924    100 men    10.6\n 9  1928    100 men    10.8\n10  1932    100 men    10.3\n# ℹ 302 more rows\n\nThe usual data tidying functions from dplyr are implemented for tsibbles.\nFiltering rows, selecting columns, deriving variables using mutate, and\nsummarizing groups using group_by and summarise all work as expected. One\ndistinction to be careful about is that the results will be grouped by their\nindex.\nFor example, this computes the total cost of Australian pharmaceuticals per\nmonth for a particular type of script. We simply filter to the script type and\ntake the sum of costs.\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  summarise(TotalC = sum(Cost))\n\n# A tsibble: 204 x 2 [1M]\n      Month  TotalC\n      <mth>   <dbl>\n 1 1991 Jul 3526591\n 2 1991 Aug 3180891\n 3 1991 Sep 3252221\n 4 1991 Oct 3611003\n 5 1991 Nov 3565869\n 6 1991 Dec 4306371\n 7 1992 Jan 5088335\n 8 1992 Feb 2814520\n 9 1992 Mar 2985811\n10 1992 Apr 3204780\n# ℹ 194 more rows\n\nIf we had wanted the total cost by year, we would have to convert to an ordinary\ndata.frame with a year variable. We cannot use a tsibble here because we would\nhave multiple measurements per year, and this would violate tsibble’s policy of\nhaving no duplicates.\n\n\nPBS %>%\n  filter(ATC2 == \"A10\") %>%\n  mutate(Year = year(Month)) %>%\n  as_tibble() %>%\n  group_by(Year) %>%\n  summarise(TotalC = sum(Cost))\n\n# A tibble: 18 × 2\n    Year     TotalC\n   <dbl>      <dbl>\n 1  1991  21442946 \n 2  1992  45686946.\n 3  1993  55532688.\n 4  1994  60816080.\n 5  1995  67326599.\n 6  1996  77397927.\n 7  1997  85131672.\n 8  1998  93310626.\n 9  1999 105959043.\n10  2000 122496586.\n11  2001 136467442.\n12  2002 149066136.\n13  2003 156464261.\n14  2004 183798935.\n15  2005 199655595 \n16  2006 220354676 \n17  2007 265718966.\n18  2008 135036513 \n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:21:07+02:00",
    "input_file": "2024-12-27-week06-01.knit.md"
  },
  {
    "path": "posts/2024-12-27-week06-02/",
    "title": "Time Series Patterns",
    "description": "Vocabulary for describing visual structure in time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(tsibble)\nlibrary(tsibbledata)\nlibrary(fpp2)\ntheme_set(theme_minimal())\n\n\nThere are a few structures that are worth keeping an eye out for whenever you\nplot a single time series. We’ll review the main vocabulary in these notes.\nVocabulary\nTrend: A long-run increase or decrease in a time series.\nSeasonal: A pattern that recurs over a fixed and known period.\nCyclic: A rising and falling pattern that does not occur over a fixed or known period.\n\nA few series with different combinations of these patterns are shown below.\n\n\nggplot(as_tsibble(qauselec)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Australian Quarterly Electricity Production\")\n\n\nggplot(as_tsibble(hsales)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Housing Sales\")\n\n\nggplot(as_tsibble(ustreas)) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"US Treasury Bill Contracts\")\n\n\nggplot(as_tsibble(diff(goog))) +\n  geom_line(aes(x = index, y = value)) +\n  labs(title = \"Google Stock Prices\")\n\n\n\nA series can display combinations of these patterns at once. Further, the\nsame data can exhibit different patterns depending on the scale at which it is\nviewed. For example, though a dataset might seem seasonal at short time scales,\na long-term trend might appear after zooming out. This is visible in the\nelectricity production plot above.\nFinally, it’s worth keeping in mind that real-world structure can be much more complicated than any of these patterns. For example, the plot below shows the number of passengers on flights from Melbourne to Sydney between 1987 and 1992. You can see a period when no flights were made and a trial in 1992 where economy seats were switched to business seats.\n\n\nmelbourne_sydney <- ansett %>%\n  filter(Airports == \"MEL-SYD\") # challenge: facet by Airports, instead of filtering\n\nggplot(melbourne_sydney) +\n  geom_line(aes(x = Week, y = Passengers, col = Class))\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week06-02/2024-12-27-week06-02_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:21:11+02:00",
    "input_file": "2024-12-27-week06-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week06-03/",
    "title": "Seasonal Plots",
    "description": "Approaches for visualizing seasonality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\nIf our data have seasonal structure, it’s natural to compare individual\nperiods against one another. In contrast to plotting the data in one long time\nseries, seasonal plots reduce the amount of distance our eyes have to travel in\norder to compare two periods in a seasonal pattern. This also reduces the burden\non our memory.\nIn R, we can use the gg_season function to overlay all the seasons onto one\nanother. The plot below shows antidiabetes drug sales over time. This view makes\nit clear that there is a spike in sales every January.\n\n\ncols <- scales::viridis_pal()(10)\ngg_season(as_tsibble(a10), pal = cols)\n\n\n\nIf the time series exhibit seasonal structure at multiple scales, then\nwe can view them all using the period argument.\n\n\ngg_season(vic_elec, Demand, period = \"day\", pal = cols)\n\n\ngg_season(vic_elec, Demand, period = \"week\", pal = cols)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week06-03/2024-12-27-week06-03_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:21:22+02:00",
    "input_file": "2024-12-27-week06-03.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week06-04/",
    "title": "Cross and Auto-Correlation",
    "description": "Summaries of relationships between and within time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(tsibbledata)\nlibrary(feasts)\ntheme_set(theme_minimal())\n\n\nThere is often interest in seeing how two time series relate to one another.\nA scatterplot can be useful in gauging this relationship. For example, the plot\nbelow suggests that there is a relationship between electricity demand and\ntemperature, but it’s hard to make out exactly the nature of the relationship.\n\n\nvic_2014 <- vic_elec %>%\n  filter(year(Time) == 2014)\n\nvic_2014 %>%\n  pivot_longer(Demand:Temperature) %>%\n  ggplot(aes(x = Time, y = value)) +\n  geom_line() +\n  facet_wrap(~ name, scales = \"free_y\")\n\n\n\nA scatterplot clarifies that, while electricity demand generally goes up in the\ncooler months, the very highest demand happens during high heat days.\n\n\nggplot(vic_2014, aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7)\n\n\n\nNote that the timepoints are far from independent. Points tend to drift\ngradually across the scatterplot, rather than jumping to completely different\nregions in short time intervals. This is just the 2D consequence of the time\nseries varying smoothly.\n\n\nlagged <- vic_2014[c(2:nrow(vic_2014), 2), ] %>%\n  setNames(str_c(\"lagged_\", colnames(vic_2014)))\n\nggplot(bind_cols(vic_2014, lagged), aes(x = Temperature, y = Demand)) +\n  geom_point(alpha = 0.6, size = 0.7) +\n  geom_segment(\n    aes(xend = lagged_Temperature, yend = lagged_Demand),\n    size = .4, alpha = 0.5\n  )\n\n\n\nTo formally measure the linear relationship between two time series, we can use the cross-correlation,\n\\[\\begin{align}\n\\frac{\\sum_{t}\\left(x_{t} - \\hat{\\mu}_{X}\\right)\\left(y_{t} - \\hat{\\mu}_{Y}\\right)}{\\hat{\\sigma}_{X}\\hat{\\sigma}_{Y}}\n\\end{align}\\]\nwhich for the data above is,\n\n\ncor(vic_2014$Temperature, vic_2014$Demand)\n\n[1] 0.2797854\n\nCross-correlation can be extended to autocorrelation — the correlation\nbetween a time series and a lagged version of itself. This measure is useful for\nquantifying the strength of seasonality within a time series. A daily time\nseries with strong weekly seasonality will have high autocorrelation at lag 7,\nfor example. The example below shows the lag-plots for Australian beer\nproduction after 2000. The plot makes clear that there is high autocorrelation\nat lags 4 and 8, suggesting high quarterly seasonality.\n\n\nrecent_production <- aus_production %>%\n  filter(year(Quarter) > 2000)\n\ngg_lag(recent_production, Beer, geom = \"point\")\n\n\n\nIndeed, we can confirm this by looking at the original data.\n\n\nggplot(recent_production) +\n  geom_line(aes(x = time(Quarter), y = Beer))\n\n\n\nThese lag plots take up a bit of space. A more compact summary is to compute\nthe autocorrelation function (ACF). Peaks and valleys in an ACF suggest\nseasonality at the frequency indicated by the lag value.\n\n\nacf_data <- ACF(recent_production, Beer)\nautoplot(acf_data)\n\n\n\nGradually decreasing slopes in the ACF suggest trends. This is because if\nthere is a trend, the current value tends to be very correlated with the recent\npast. It’s possible to have both seasonality within a trend, in which case the\nACF function has bumps where the seasonal peaks align.\n\n\nggplot(as_tsibble(a10), aes(x = time(index), y = value)) +\n  geom_line()\n\n\nacf_data <- ACF(as_tsibble(a10), lag_max = 100)\nautoplot(acf_data)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week06-04/2024-12-27-week06-04_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:21:31+02:00",
    "input_file": "2024-12-27-week06-04.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week06-05/",
    "title": "Collections of Time Series",
    "description": "Navigating across related time series.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(broom)\nlibrary(tidyverse)\nlibrary(feasts)\nlibrary(fpp2)\nlibrary(ggrepel)\nlibrary(tsibble)\nlibrary(tsibbledata)\ntheme_set(theme_minimal())\n\n\nWe have seen ways of visualizing a single time series (seasonal plots, ACF) and small numbers of time series (Cross Correlation). In practice, it’s also common to encounter large collections of time series. These datasets tend to require more sophisticated analysis techniques, but we will review one useful approach, based on extracted features.\nThe high-level idea is to represent each time series by a vector of summary\nstatistics, like the maximum value, the slope, and so on. These vector summaries\ncan then be used to create an overview of variation seen across all time series.\nFor example, just looking at the first few regions in the Australian tourism\ndataset, we can see that there might be useful features related to the overall\nlevel (Coral Coast is larger than Barkly), recent trends (increased business in\nNorth West), and seasonality (South West is especially seasonal).\n\n\ntourism <- as_tsibble(tourism, index = Quarter) %>%\n  mutate(key = str_c(Region, Purpose, sep=\"-\")) %>%\n  update_tsibble(key = c(\"Region\", \"State\", \"Purpose\", \"key\"))\n\nregions <- tourism %>%\n  distinct(Region) %>%\n  pull(Region)\n\nggplot(tourism %>% filter(Region %in% regions[1:9])) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free\") +\n  theme(legend.position = \"bottom\")\n\n\n\nComputing these kinds of summary statistics by hand would be tedious.\nFortunately, the feasts package makes it easy to extract a variety of statistics\nfor tsibble objects.\n\n\ntourism_features <- tourism %>%\n  features(Trips, feature_set(pkgs = \"feasts\"))\n\ntourism_features\n\n# A tibble: 304 × 52\n   Region    State Purpose key   trend_strength seasonal_strength_year\n   <chr>     <chr> <chr>   <chr>          <dbl>                  <dbl>\n 1 Adelaide  Sout… Busine… Adel…          0.464                  0.407\n 2 Adelaide  Sout… Holiday Adel…          0.554                  0.619\n 3 Adelaide  Sout… Other   Adel…          0.746                  0.202\n 4 Adelaide  Sout… Visiti… Adel…          0.435                  0.452\n 5 Adelaide… Sout… Busine… Adel…          0.464                  0.179\n 6 Adelaide… Sout… Holiday Adel…          0.528                  0.296\n 7 Adelaide… Sout… Other   Adel…          0.593                  0.404\n 8 Adelaide… Sout… Visiti… Adel…          0.488                  0.254\n 9 Alice Sp… Nort… Busine… Alic…          0.534                  0.251\n10 Alice Sp… Nort… Holiday Alic…          0.381                  0.832\n# ℹ 294 more rows\n# ℹ 46 more variables: seasonal_peak_year <dbl>,\n#   seasonal_trough_year <dbl>, spikiness <dbl>, linearity <dbl>,\n#   curvature <dbl>, stl_e_acf1 <dbl>, stl_e_acf10 <dbl>, acf1 <dbl>,\n#   acf10 <dbl>, diff1_acf1 <dbl>, diff1_acf10 <dbl>,\n#   diff2_acf1 <dbl>, diff2_acf10 <dbl>, season_acf1 <dbl>,\n#   pacf5 <dbl>, diff1_pacf5 <dbl>, diff2_pacf5 <dbl>, …\n\nOnce you have a data.frame summarizing these time series, you can run any\nclustering or dimensionality reduction procedure on the summary. For example,\nthis is 2D representation from PCA. We will get into much more depth about\ndimensionality reduction later in this course — for now, just think of this as\nan abstract map relating all the time series.\n\n\npcs <- tourism_features %>%\n  select(-State, -Region, -Purpose, -key) %>%\n  prcomp(scale = TRUE) %>%\n  augment(tourism_features)\n\noutliers <- pcs %>% \n  filter(.fittedPC1 ^ 2 + .fittedPC2 ^ 2 > 120)\n\n\nThis PCA makes it very clear that the different travel purposes have different\ntime series, likely due to the heavy seasonality of holiday travel (Melbourne\nseems to be an interesting exception).\n\n\nggplot(pcs, aes(x = .fittedPC1, y = .fittedPC2)) +\n  geom_point(aes(col = Purpose)) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = Region),\n    size = 2.5 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"PC1\", y = \"PC2\") +\n  coord_fixed() +\n  theme(legend.position = \"bottom\")\n\n\n\nWe can look at the series that are outlying in the PCA. The reading has some\nstories for why these should be considered outliers. They seem to be series with\nsubstantial increasing trends or which have exceptionally high overall counts.\n\n\noutlier_series <- tourism %>%\n  filter(key %in% outliers$key)\n\nggplot(outlier_series) +\n  geom_line(aes(x = date(Quarter), y = Trips, col = Purpose)) +\n  scale_color_brewer(palette = \"Set2\") +\n  facet_wrap(~Region, scale = \"free_y\") +\n  theme(legend.position = \"bottom\")\n\n\n\nThis featurization approach is especially powerful when combined with\ncoordinated views. It is possible to link the points in the PCA plot with the\ntime series display, so that selecting points in the PCA shows the corresponding\ntime series.\n\n\n\n",
    "preview": "posts/2024-12-27-week06-05/2024-12-27-week06-05_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:22:07+02:00",
    "input_file": "2024-12-27-week06-05.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week07-02/",
    "title": "Vector Data",
    "description": "Manipulating and visualizing spatial vector data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(ceramic)\nlibrary(knitr)\nlibrary(sf)\nlibrary(spData)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())\n\n\nAs mentioned previously, vector data are used to store geometric spatial\ndata. Specifically, there are 7 types of geometric information that are commonly\nused, as given in the figure below.\n\n\ninclude_graphics(\"https://krisrs1128.github.io/stat479/posts/2021-03-02-week7-2/sf-classes.png\")\n\n\n\nWe can construct these geometric objects from scratch. For example, starting\nfrom the defining coordinates, we can use st_point to create a point object,\n\n\n# make a point\np <- st_point(c(5, 2))\nplot(p)\n\n\n\nst_linestring to create a linestring,\n\n\n# make a line\nlinestring_matrix <- rbind(c(1, 5), c(4, 4), c(4, 1), c(2, 2), c(3, 2))\np <- st_linestring(linestring_matrix)\nplot(p)\n\n\n\nand st_polygon to create a polygon.\n\n\n# make a polygon\npolygon_list <- list(rbind(c(1, 5), c(2, 2), c(4, 1), c(4, 4), c(1, 5)))\np <- st_polygon(polygon_list)\nplot(p)\n\n\n\nDifferent geometries can be combined into a geometry collection, using sfc.\n\n\npoint1 <- st_point(c(5, 2))\npoint2 <- st_point(c(1, 3))\npoints_sfc <- st_sfc(point1, point2)\nplot(points_sfc)\n\n\n\nReal-world vector datasets are more than just these geometries — they also\nassociate each geometry with some additional information about each feature. We\ncan add this information to the geometries above by associating each element\nwith a row of a data.frame. This merging is accomplished by st_sf, using\ngeometry to associate a raw st_geom each row of a data.frame.\n\n\nlnd_point <- st_point(c(0.1, 51.5))                \nlnd_geom <- st_sfc(lnd_point, crs = 4326)         \nlnd_attrib = data.frame(                        \n  name = \"London\",\n  temperature = 25,\n  date = as.Date(\"2017-06-21\")\n  )\n\nlnd_sf = st_sf(lnd_attrib, geometry = lnd_geom)\n\n\nVisualization\nVector data can be directly plotted using base R. For example, suppose we\nwant to plot the boundaries of India, within it’s local context. We can use the\nworld dataset, provided by the spData package. Each row of the world\nobject contains both the boundary of a country (in the geom column) and\ninformation about its location and population characteristics.\n\n\ndata(world)\nhead(world)\n\nSimple feature collection with 6 features and 10 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -180 ymin: -18.28799 xmax: 180 ymax: 83.23324\nGeodetic CRS:  WGS 84\n# A tibble: 6 × 11\n  iso_a2 name_long      continent   region_un subregion type  area_km2\n  <chr>  <chr>          <chr>       <chr>     <chr>     <chr>    <dbl>\n1 FJ     Fiji           Oceania     Oceania   Melanesia Sove…   1.93e4\n2 TZ     Tanzania       Africa      Africa    Eastern … Sove…   9.33e5\n3 EH     Western Sahara Africa      Africa    Northern… Inde…   9.63e4\n4 CA     Canada         North Amer… Americas  Northern… Sove…   1.00e7\n5 US     United States  North Amer… Americas  Northern… Coun…   9.51e6\n6 KZ     Kazakhstan     Asia        Asia      Central … Sove…   2.73e6\n# ℹ 4 more variables: pop <dbl>, lifeExp <dbl>, gdpPercap <dbl>,\n#   geom <MULTIPOLYGON [°]>\n\nThis makes the plot, using dplyr to filter down to just the row containing the\nIndia geometry.\n\n\nindia_geom <- world %>%\n  filter(name_long == \"India\") %>%\n  st_geometry()\n\nplot(india_geom)\n\n\n\nUsing base R, we can also layer on several vector objects, using add = TRUE.\n\n\nworld_asia <- world %>%\n  filter(continent == \"Asia\")\n\nplot(india_geom, expandBB = c(0, 0.2, 0.1, 1), col = \"gray\", lwd = 3)\nplot(st_union(world_asia), add = TRUE)\n\n\n\nWe can also use tm_polygons in tmap. To change the coordinates of the\nviewing box, we can set the bbox (bounding box) argument.\n\n\nbbox <- c(60, 5, 110, 40)\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"white\") +\n  tm_shape(india_geom) +\n  tm_polygons()\n\n\n\nWe can also encode data that’s contained in the vector dataset.\n\n\ntm_shape(world_asia, bbox = bbox) +\n  tm_polygons(col = \"lifeExp\") +\n  tm_polygons()\n\n\n\nEven in this more complex setup, where we work with background images and\nvector data rather than standard data.frames, we can still apply the kinds of\nvisual encoding ideas that we are familiar with. For example, we can still color\ncode or facet by fields in the vector dataset. To illustrate, we revisit the bus\nroute data from the last lecture and distinguish between buses operated by the\ncities of Madison vs. Monona. Before plotting, we fetch the underlying data.\n\n\nSys.setenv(MAPBOX_API_KEY=\"pk.eyJ1Ijoia3Jpc3JzMTEyOCIsImEiOiJjbDYzdjJzczQya3JzM2Jtb2E0NWU1a3B3In0.Mk4-pmKi_klg3EKfTw-JbQ\")\nbasemap <- cc_location(loc= c(-89.401230, 43.073051), buffer = 15e3)\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"#bc7ab3\", size = 1)\n\n\n\nNote that operator is the field containing information about which city is\noperating the buses. We can color code the routes by this attribute.\n\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"operator\", size = 1) +\n  tm_layout(legend.bg.color = \"white\")\n\n\n\nAlternatively, we can facet.\n\n\ntm_shape(basemap) +\n  tm_rgb() +\n  tm_shape(bus) +\n  tm_lines(col = \"operator\", size = 1) +\n  tm_facets(\"operator\")\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week07-02/2024-12-27-week07-02_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:32:57+02:00",
    "input_file": "2024-12-27-week07-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week07-03/",
    "title": "Raster Data",
    "description": "Storing spatially gridded information in rasters.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(raster)\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tmap)\ntheme_set(theme_bw())\n\n\nThe raster data format is used to store spatial data that lie along regular\ngrids. The values along the grid are stored as entries in the matrix. The raster\nobject contains metadata that associates each entry in the matrix with a\ngeographic coordinate.\nSince the data they must lie along a regular grid, rasters are most often\nused for continuously measured data, like elevation, temperature, population\ndensity, or landcover class.\nWe can create a raster using the rast command. The code block below loads\nan elevation map measured by the space shuttle.\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\n zion <- rast(f)\n\n\nTyping the name of the object shows the metadata associated with it (but not\nthe actual grid values). We can see that the grid has 457 rows and 465 columns.\nWe also see its spatial extent: The minimum and maximum longitude are both close\nto -113 and the latitudes are between 37.1 and 37.5. A quick google map search shows that this is located in Zion\nnational park.\n\n\nzion\n\nclass       : SpatRaster \ndimensions  : 457, 465, 1  (nrow, ncol, nlyr)\nresolution  : 0.0008333333, 0.0008333333  (x, y)\nextent      : -113.2396, -112.8521, 37.13208, 37.51292  (xmin, xmax, ymin, ymax)\ncoord. ref. : lon/lat WGS 84 (EPSG:4326) \nsource      : srtm.tif \nname        : srtm \nmin value   : 1024 \nmax value   : 2892 \n\n\n\nplot(zion)\n\n\n\nIn contrast, the raster command lets us create raster objects from scratch.\nFor example, the code below makes a raster with increasing values in a 6 x 6\ngrid. Notice that we had to give a fake spatial extent.\n\n\ntest <- raster(\n   nrows = 6, ncols = 6, res = 0.5, \n   xmn = -1.5, xmx = 1.5, ymn = -1.5, ymx = 1.5,\n   vals = 1:36\n )\n \n plot(test)\n\n\n\nReal-world rasters typically have more than one layer of data. For example, you might measure both elevation and slope along the same spatial grid, which would lead to a 2 layer raster. Or, for satellite images, you might measure light at multiple wavelengths (usual RGB, plus infrared or thermal for example).\nMulti-layer raster data can be read in using rast. You can refer to particular layers in a multi-layer raster by indexing.\n\n\nf <- system.file(\"raster/landsat.tif\", package = \"spDataLarge\")\n satellite <- rast(f)\n \n satellite # all 4 channels\n\nclass       : SpatRaster \ndimensions  : 1428, 1128, 4  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2, landsat_3, landsat_4 \nmin values  :      7550,      6404,      5678,      5252 \nmax values  :     19071,     22051,     25780,     31961 \n\nsatellite[[1:2]] # only first two channels\n\nclass       : SpatRaster \ndimensions  : 1428, 1128, 2  (nrow, ncol, nlyr)\nresolution  : 30, 30  (x, y)\nextent      : 301905, 335745, 4111245, 4154085  (xmin, xmax, ymin, ymax)\ncoord. ref. : WGS 84 / UTM zone 12N (EPSG:32612) \nsource      : landsat.tif \nnames       : landsat_1, landsat_2 \nmin values  :      7550,      6404 \nmax values  :     19071,     22051 \n\nBase R’s plot function supports plotting one layer of a raster at a time. To plot more than one layer in a multichannel image (like ordinary RGB images) you can use the plotRGB function.\n\n\nplotRGB(satellite, stretch = \"lin\")\n\n\n\nSometimes, it’s useful to overlay several visual marks on top of a raster\nimage.\n\n\nsatellite <- project(satellite, \"EPSG:4326\")\n point <- data.frame(geom = st_sfc(st_point(c(-113, 37.3)))) %>%\n   st_as_sf()\n \n tm_shape(satellite) +\n   tm_raster() +\n   tm_shape(point) +\n   tm_dots(col = \"blue\", size = 5)\n\n\n\nIf we want to visualize just a single layer, we can use tm_rgb with all\ncolor channels set to the layer of interest. Note that, here, I’ve rescaled the\nmaximum value of each pixel to 255, since this is the default maximum value for\na color image.\n\n\ntm_shape(satellite / max(satellite) * 255) +\n  tm_rgb(r = 1, g = 1, b = 1)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week07-03/2024-12-27-week07-03_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-01-14T17:33:16+02:00",
    "input_file": "2024-12-27-week07-03.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week07-04/",
    "title": "Coordinate Reference Systems",
    "description": "The projection problem, and how to check your CRS.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(tmap)\nlibrary(sf)\nlibrary(spData)\ntheme_set(theme_minimal())\n\n\nAn important subtlety of geographic data visualization is that all our maps\nare in 2D, but earth is 3D. The process of associated points on the earth with\n2D coordinates is called « projection. » All projections introduce some level of\ndistortion, and there is no universal, ideal projection.\nHere are a few examples of famous global projections. There are also many\nprojections designed to give optimal representations locally within a particular\ngeographic area.\nThis means that there is no universal standard for how to represent\ncoordinates on the earth, and it’s common to find different projections in\npractice. For example, the block below shows how North America gets projected\naccording to two different CRSs.\n\n\n# some test projections, don't worry about this syntax\n miller <- \"+proj=mill +lat_0=0 +lon_0=0 +x_0=0 +y_0=0 +ellps=WGS84 +datum=WGS84 +units=m +no_defs\"\n lambert <- \"+proj=lcc +lat_1=20 +lat_2=60 +lat_0=40 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +datum=NAD83 +units=m +no_defs\"\n \n north_america <- world %>%\n   filter(continent == \"North America\")\n \n tm_shape(st_transform(north_america, miller)) +\n   tm_polygons()\n\n\ntm_shape(st_transform(north_america, lambert)) +\n   tm_polygons()\n\n\n\nBoth vector and raster spatial data will be associated with CRS’s. In either\nsf or raster objects, they can be accessed using the crs function.\nA common source of bugs is to use two different projections for the same\nanalysis. In this class, we will always use the EPSG:4326 projection, which is\nwhat is used in most online maps. But in your own projects, you should always\ncheck that the projections are consistent across data sources. If you find an\ninconsistency, it will be important to « reproject » the data into the same CRS.\n\n\n\n",
    "preview": "posts/2024-12-27-week07-04/2024-12-27-week07-04_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:33:19+02:00",
    "input_file": "2024-12-27-week07-04.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week07-05/",
    "title": "Geospatial Interaction",
    "description": "Idioms for interacting with geographic data.",
    "author": [],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nCode, Recording\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(leaflet)\n\n\nMaps can be information dense, so it’s often useful to make them interactive.\nThese notes review some basic strategies for interactive spatial data\nvisualization.\nleaflet is an easy-to-use R package that’s often sufficient for routine visualization. It offers several types of marks (marks, circles, polygons) and allows them to\nencode fields in a dataset. Note that its interface is more like base R than\nggplot2 — we specify each attribute in one plot command. For example, in the\ncode block below, addTiles fetches the background map. addCircles overlays the\nnew vector features on top of the map. It’s worth noting that the vector\nfeatures were created automatically – there was no need to create or read in any\ntype of sf object.\n\n\ncities <- read_csv(\"https://uwmadison.box.com/shared/static/j98anvdoasfb1h651qxzrow2ua45oap1.csv\")\n leaflet(cities) %>%\n   addTiles() %>%\n   addCircles(\n     lng = ~Long,\n     lat = ~Lat,\n     radius = ~sqrt(Pop) * 30\n   )\n\n\n\n\nLeaflet maps can be embedded into Shiny\napps using leafletOutput and\nrenderLeaflet. For example, the Superzip Explorer is a visualization\ndesigned for showing income and education levels across ZIP codes in the US. In\nthe\nserver,\nthe map is initialized using the leaflet command (without even adding any data\nlayers).\n\n\n# Create the map\n output$map <- renderLeaflet({\n   leaflet() %>%\n     addTiles() %>%\n     setView(lng = -93.85, lat = 37.45, zoom = 4)\n })\n\n\n\nThe most interesting aspect of the explorer is that it lets us zoom into\nregions and study properties of ZIP codes within the current view.\nleaflet automatically creates an input$map_bounds input which is triggered\nanytime we pan or zoom the map. It returns a subset of the full dataset within\nthe current view.\n\n\nzipsInBounds <- reactive({\n   if (is.null(input$map_bounds)) return(zipdata[FALSE,]) # return empty data\n   bounds <- input$map_bounds\n   latRng <- range(bounds$north, bounds$south)\n   lngRng <- range(bounds$east, bounds$west)\n \n   # filters to current view\n   subset(zipdata,\n     latitude >= latRng[1] & latitude <= latRng[2] &\n       longitude >= lngRng[1] & longitude <= lngRng[2])\n })\n\n\nWhenever this reactive is run, the histogram (output$histCentile) and\nscatterplot (output$scatterCollegeIncome) on the side of the app are\nupdated.\nNotice that an observer was created to monitor any interactions with the map.\nWithin this observe block, a leafletProxy call is used. This function makes it\npossible to modify a leaflet map without redrawing the entire map. It helps\nsupport efficient rendering – we’re able to change the colors of the circles\nwithout redrawing the entire leaflet view.\n\n\nleafletProxy(\"map\", data = zipdata) %>%\n   clearShapes() %>%\n   addCircles(~longitude, ~latitude, radius=radius, layerId=~zipcode,\n     stroke=FALSE, fillOpacity=0.4, fillColor=pal(colorData)) %>%\n   addLegend(\"bottomleft\", pal=pal, values=colorData, title=colorBy,\n     layerId=\"colorLegend\")\n\n\nWe can often dynamically query spatial data. Querying a map can highlight\nproperties of samples within a geographic region. For example, here is a map of\nin which each US county has been associated with a (random) pair of data\nfeatures. Clicking on counties (or hovering with the shift key pressed) updates\nthe bars on the right. Each bar shows the average from one of the data fields,\nacross all selected counties.\n\nThis linking is accomplished using event listeners. For example, the map\nincludes the call .on(\"mouseover\", update_selection), and update_selection\nchanges the fill of the currently hovered county,\n\nsvg.selectAll(\"path\")\n  .attr(\"fill\", (d) => selected.indexOf(d.id) == -1 ? \"#f7f7f7\" : \"#4a4a4a\");\n\nThe full implementation can be read\nhere. Note that\ninteractivity here is done just like in any other D3 visualization. We can\ntreat the map as just another collection of SVG paths, and all our\ninteraction events behave in the same way.\nWe can also imagine selecting geographic regions by interacting with linked\nviews. This is used in Nadieh Bremer’s Urbanization in East Asia\nvisualization, for example, where we\ncan see all the urban areas within a country by hovering its associated bar.\n\nHere is a somewhat more complex version of the earlier random data example\nwhere acounties are associated with (random) time series. Redrawing the lower\nand upper bounds on the time series changes which counties are highlighted.\n\nThough it’s not exactly interaction, another common strategy for\nspatiotemporal data is animation. The major trends often become apparent by\nvisualizing the flow of visual marks on the screen. For example, how can we\nvisualize where football players go on a field before they score a goal? One\napproach is to animate the trajectories leading up to the goal. Here is one\nbeautiful visualization (in D3!) by Karim Douieb that shows the most common\npaths and the speed at which the players run.\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:33:29+02:00",
    "input_file": "2024-12-27-week07-05.knit.md"
  },
  {
    "path": "posts/2024-12-27-week08-01/",
    "title": "Introduction to Networks and Trees",
    "description": "Typical tasks and example network datasets.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading 1 (Chapter 9), Reading 2, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nNetworks and trees can be used to represent information in a variety of\ncontexts. Abstractly, networks and trees are types of graphs, which are\ndefined by (a) a set \\(V\\) of vertices and (b) a set \\(E\\) of edges between pairs of\nvertices.\nIt is helpful to have a few specific examples in mind,\nThe Internet: \\(V = \\{\\text{All Webpages}\\}, \\left(v, v^{\\prime}\\right) \\in E\\)\nif there is a hyperlink between pages \\(v\\) and \\(v^{\\prime}\\).\nEvolutionary Tree: \\(V = \\{\\text{All past and present species}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if one of the species \\(v\\) or \\(v^{\\prime}\\) is a\ndescendant of the other.\nDisease Transmission: \\(V = \\{\\text{Community Members}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if the two community members have come in close\ncontact.\nDirectory Tree: \\(V = \\{\\text{All directories in a computer}\\}, \\left(v,\nv^{\\prime}\\right) \\in E\\) if one directory is contained in the other.\n\n\n\n\nFigure 1: A visualization of the internet, from the Opte Project.\n\n\n\n\n\n\nFigure 2: An evolutionary tree, from the Interactive Tree of Life.\n\n\n\n\n\n\nFigure 3: A COVID-19 transmission network, from Clustering and superspreading potential of SARS-CoV-2 infections in Hong Kong.\n\n\n\n\n\n\nFigure 4: Directories in a file system can be organized into a tree, with parent and child directories.\n\n\n\nEither vertices or edges might have attributes. For example, in the directory\ntree, we might know the sizes of the files (vertex attribute), and in the\ndisease transmission network we might know the duration of contact between\nindividuals (edge attribute).\nAn edge may be either undirected or directed. In a directed edge, one vertex\nleads to the other, while in an undirected edge, there is no sense of ordering.\nIn R, the tidygraph package can be used to manipulate graph data. It’s\ntbl_graph class stores node and edge attributes in a single data structure.\nand ggraph extends the usual ggplot2 syntax to graphs.\n\n\nE <- data.frame(\n  source = c(1, 2, 3, 4, 5),\n  target = c(3, 3, 4, 5, 6)\n)\n\nG <- tbl_graph(edges = E)\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Node Data: 6 × 0 (active)\n#\n# Edge Data: 5 × 2\n   from    to\n  <int> <int>\n1     1     3\n2     2     3\n3     3     4\n# ℹ 2 more rows\n\nThis tbl_graph can be plotted using the code below. There are different geoms\navailable for nodes and edges – for example, what happens if you replace\ngeom_edge_link() with geom_edge_arc()?\n\n\nggraph(G, layout = 'kk') + \n  geom_edge_link() +\n  geom_node_point()\n\n\n\nWe can mutate node and edge attributes using dplyr-like syntax. Before\nmutating edges, it’s necessary to call activate(edges).\n\n\nG <- G %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) %>%\n  activate(edges) %>%\n  mutate(width = runif(n()))\nG\n\n# A tbl_graph: 6 nodes and 5 edges\n#\n# A rooted tree\n#\n# Edge Data: 5 × 3 (active)\n   from    to  width\n  <int> <int>  <dbl>\n1     1     3 0.969 \n2     2     3 0.462 \n3     3     4 0.0642\n4     4     5 0.264 \n5     5     6 0.250 \n#\n# Node Data: 6 × 2\n     id group\n  <int> <lgl>\n1     1 TRUE \n2     2 TRUE \n3     3 TRUE \n# ℹ 3 more rows\n\nNow we can visualize these derived attributes using an aesthetic mapping within\nthe geom_edge_link and geom_node_point geoms.\n\n\nggraph(G, layout = \"kk\") +\n  geom_edge_link(aes(width = width)) +\n  geom_node_label(aes(label = id))\n\n\n\nFigure 5: The same network as above, but with edge size encoding the weight attribute.\n\n\n\nExample Tasks\nWhat types of data that are amenable to representation by networks or trees?\nWhat visual comparisons do networks and trees facilitate?\nOur initial examples suggest that trees and networks can be used to represent\neither physical interactions or conceptual relationships. Typical tasks include,\nSearching for groupings\nFollowing paths\nIsolating key nodes\n\nBy “searching for groupings,” we mean finding clusters of nodes that are\nhighly interconnected, but which have few links outside the cluster. This kind\nof modular structure might lend itself to deeper investigation within each of\nthe clusters.\nClusters in a network of political blogs might suggest an echo chamber effect.\nGene clusters in a differential expression study might suggest pathways\nneeded for the production of an important protein.\nClusters in a recipe network could be used identify different culinary\ntechniques or cuisines.\n\n\n\n\nFigure 6: A representation of 1200 blogs before the 2004 election, from The political blogosphere and the 2004 US election: divided they blog.\n\n\n\nBy “following paths,” we mean tracing the paths out from a particular node,\nto see which other nodes it is close to.\nFollowing paths in a citation network might reveal the chain of publications\nthat led to an important discovery.\nFollowing paths in a recommendation network might suggest other users who\nmight be interested in watching a certain movie.\n\n\n\n\nFigure 7: A recommendation network, linking individuals and the movies that they viewed.\n\n\n\n“Isolating key nodes” is a more fuzzy concept, usually referring to the task\nof finding nodes that are exceptional in some way. For example, it’s often\ninteresting to find nodes with many more connections than others, or which link\notherwise isolated clusters.\nA node with many edges in a disease transmission network is a superspreader.\nA node that links two clusters in a citation network might be especially\ninterdisciplinary.\nA node with large size in a directory tree might be a good target for\nreducing disk usage.\n\n\n\n\nFigure 8: The scientific journal, Social Networks, links several publication communities, as found by Betweenness Centrality as an Indicator of the Interdisciplinarity of Scientific Journals.\n\n\n\nIf you find these questions interesting, you might enjoy the catalog of\nexamples on the website VisualComplexity.\n\n\n\n",
    "preview": "posts/2024-12-27-week08-01/2024-12-27-week08-01_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2025-01-14T17:33:34+02:00",
    "input_file": "2024-12-27-week08-01.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week08-02/",
    "title": "Node - Link Diagrams",
    "description": "The most common network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(ggraph)\nlibrary(gridExtra)\nlibrary(networkD3)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nA node-link diagram is a visual encoding strategy for network data, where\nnodes are drawn as points and links between nodes are drawn as lines between\nthem. The dataset below is a friendship network derived from a survey of high\nschoolers in 1957 and 1958, available in the tidygraph package.\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school) +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\n\n\nFor trees, the vertical or radial position can further encode the depth of a\nnode in the tree. The data below represent the directory structure from a widely\nused web package called flare.\n\n\nG_flare <- tbl_graph(flare$vertices, flare$edges)\np1 <- ggraph(G_flare, 'tree') + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\np2 <- ggraph(G_flare, 'tree', circular = TRUE) + \n  geom_edge_link() +\n  geom_node_label(aes(label = shortName), size = 3)\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\nFigure 1: The same node-link diagram, with either height or radius encoding depth in the tree.\n\n\n\nIn either trees or networks, attributes of nodes and edges can be encoded\nusing size (node radius or edge width) or color.\nThe node-link representation is especially effective for the task of\nfollowing paths. It’s an intuitive visualization for examining the local\nneighborhood of one node or describing the shortest path between two nodes.\nIn node-link diagrams, spatial position is subtle. It does not directly\nencode any attribute in the dataset, but layout algorithms (i.e., algorithms\nthat try to determine the spatial positions of nodes in a node-link diagram) try\nto ensure that nodes that are close to one another in the shortest-path-sense\nalso appear close to one another on the page.\n\n\np1 <- ggraph(G_school, layout = \"kk\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\np2 <- ggraph(G_school, layout = \"fr\") +\n  geom_edge_link(aes(col = year), width = 0.1) +\n  geom_node_point()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\nFigure 2: A comparison of two layout algorithms for the same network.\n\n\n\nOne common layout algorithm uses force-directed placement. The edges here are\ninterpreted as physical springs, and the node positions are iteratively updated\naccording to the forces induced by the springs.\nHere is an interactive force-directed layout of the network from above,\ngenerated using the networkD3 package.\n\n\nschool_edges <- G_school %>%\n  activate(edges) %>%\n  as.data.frame()\nsimpleNetwork(school_edges)\n\n\n\nThe key drawback of node-link diagrams is that they do not scale well to\nnetworks with a large number of nodes or with a large number of edges per node.\nThe nodes and edges begin to overlap too much, and the result looks like a\n« hairball. »\nIn this situation, it is possible to use additional structure in the data to\nsalvage the node-link display. For example, in a large tree, a rectangular or\nBubbleTree layout can be used.\n\n\n\nFigure 3: Example rectangular and BubbleTree layouts, for very large trees, as shown in Visualization Analysis and Design.\n\n\n\nIf a large network has a modular structure, then it is possible to first lay out the separate clusters far apart from one another, before running force directed placement.\n\n\n\nFigure 4: A hierarchical force directed layout algorithm, as shown in Visualization Analysis and Design.\n\n\n\nIf many edges go through a few shared paths, it may be possible to bundle them.\n\n\n\nFigure 5: In edge bundling, similar paths are placed close to one another.\n\n\n\nBundled connections can be visualized using the geom_conn_bundle geometry in\nggraph. Before using this layout, it is necessary to have a hierarchy over all\nthe nodes, since shared ancestry among connected nodes is how the proximity of\npaths is determined.\n\n\nfrom <- match(flare$imports$from, flare$vertices$name)\nto <- match(flare$imports$to, flare$vertices$name)\nggraph(G_flare, layout = 'dendrogram', circular = TRUE) + \n  geom_conn_bundle(data = get_con(from = from, to = to), alpha = 0.1) + \n  geom_node_label(aes(label = shortName), size = 2) +\n  coord_fixed()\n\n\n\nFigure 6: An example of hierarchical edge bundling in R.\n\n\n\nTo summarize, node-link diagrams are very good for characterizing local\nstructure, but struggle with large networks.\n\n\n\n",
    "preview": "posts/2024-12-27-week08-02/2024-12-27-week08-02_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:33:51+02:00",
    "input_file": "2024-12-27-week08-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week08-03/",
    "title": "Adjacency Matrix Views",
    "description": "A scalable network visualization strategy.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(gridExtra)\nlibrary(igraph)\nlibrary(ggraph)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nThe adjacency matrix of an undirected graph is the matrix with a 1 in entry\n\\(ij\\) if nodes \\(i\\) and \\(j\\) are linked by an edge and 0 otherwise. It has one row\nand one column for every node in the graph. Visually, these 1’s and 0’s can be\nencoded as a black and white squares.\nThe example below shows the node-link and adajcency matrix plots associated\nwith the toy example from the first lecture.\n\n\nE <- matrix(c(1, 3, 2, 3, 3, 4, 4, 5, 5, 6),\n  byrow = TRUE, ncol = 2) %>%\n  as_tbl_graph() %>%\n  mutate(\n    id = row_number(),\n    group = id < 4\n  ) \n\np1 <- ggraph(E, layout = 'kk') + \n    geom_edge_fan() +\n    geom_node_label(aes(label = id))\n\np2 <- ggraph(E, \"matrix\") +\n    geom_edge_tile(mirror = TRUE, show.legend = TRUE) +\n    geom_node_text(aes(label = id), x = -.5, nudge_y = 0.5) +\n    geom_node_text(aes(label = id), y = 0.5, nudge_x = -0.5) +\n    scale_y_reverse(expand = c(0, 0, 0, 1.5)) + # make sure the labels aren't hidden\n    scale_x_discrete(expand = c(0, 1.5, 0, 0)) +\n    coord_fixed() # make sure squares, not rectangles\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\nIn a directed graph, the \\(ij^{th}\\) entry is set to 1 if node \\(i\\) leads\ndirectly to node \\(j\\). Unlike the undirected graph, it is not necessarily\nsymmetric.\nThe example below shows the adjacency matrix associated with the high-school\nstudent friendship network from last lecture. This is a directed graph, because\nthe students were surveyed about their closest friends, and this was not always\nsymmetric.\n\n\nG_school <- as_tbl_graph(highschool) %>%\n  activate(edges) %>%\n  mutate(year = factor(year))\n\nggraph(G_school, \"matrix\") +\n  geom_edge_tile() +\n  coord_fixed() +\n  facet_wrap(~ year)\n\n\n\nNode properties can be encoded along the rows and columns of the matrix. Edge\nproperties can be encoded by the color or opacity of cells in the matrix. The\nexample below simulates clustered data and draws the cluster label for each node\nalong rows / columns.\n\n\nG <- sample_islands(4, 40, 0.4, 15) %>%\n  as_tbl_graph() %>%\n  mutate(\n    group = rep(seq_len(4), each = 40),\n    group = as.factor(group)\n  )\n\nggraph(G, \"matrix\") +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\nThe example below takes a protein network and colors each edge according to the\nweight of experimental evidence behind that interaction.\n\n\nproteins <- read_tsv(\"https://uwmadison.box.com/shared/static/t97v5315isog0wr3idwf5z067h4j9o7m.tsv\") %>%\n  as_tbl_graph(from = node1, to = node2)\n\nggraph(proteins, \"matrix\") +\n  geom_edge_tile(aes(fill = combined_score), mirror = TRUE) +\n  coord_fixed() +\n  geom_node_text(aes(label = name), size = 3, x = -2.5, nudge_y = 0.5, hjust = 0) +\n  geom_node_text(aes(label = name), size = 3, angle = 90, y = 0.5, nudge_x = -0.5, hjust = 0) +\n  scale_y_reverse(expand = c(0, 0, 0, 2.7)) + # make sure the labels aren't hidden\n  scale_x_discrete(expand = c(0, 3, 0, 0)) +\n  scale_edge_fill_distiller() +\n  labs(edge_fill = \"Edge Confidence\")\n\n\n\nThe key advantage of visualization using adjacency matrices is that they can\nscale to large and dense networks. It’s possible to perceive structure even when\nthe squares are quite small, and there is no risk of edges overlapping with one\nanother.\nAdjacency matrices can be as useful as node-link diagrams for the task of\nfinding group structure. The example below shows how cliques (fully connected\nsubsets), bicliques (sets of nodes that are fully connected between one\nanother), and clusters (densely, but not completely, connected subsets) are\nclearly visible in both node-link and adjacency matrix visualizations.\n\n\n\nFigure 1: Cliques, bicliques, and clusters are visually salient using either node-link or adjacency matrix representations of a matrix.\n\n\n\nThe key disadvantage of adjacency matrix visualization is that it’s\nchallenging to make sense of the local topology around a node. Finding the\n« friends of friends » of a node requires effort.\n\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\nAnother issue is that different orderings of rows and columns can have a\ndramatic effect on the structure that’s visible1. For example,\nhere is the same clustered network from 5., but with a random reordering of rows\nand columns. This is also beautifully illustrated at this\npage (change the “order” dropdown\nmenu).\n\n\nggraph(G, \"matrix\", sort.by = sample(1:160)) +\n  geom_node_point(aes(col = group), x = -1) +\n  geom_node_point(aes(col = group), y = 1) +\n  geom_edge_tile(mirror = TRUE) +\n  scale_y_reverse() +\n  scale_color_brewer(palette = \"Set2\") +\n  coord_fixed()\n\n\n\nThese visualizations also tend to be less intuitive for audiences that\nhaven’t viewed them before.\nIn a sense, adjacency matrix and node-link visualizations are complementary\nto one another. Some\napproaches\ntry to use the adjacency matrix to give a global view of a network, and dynamic\nqueries on the adjacency matrix can reveal the\nassociated, local node-link diagrams.\n\nThough viewed differently, this\nadditional degree of freedom can facilitate richer comparisons.↩︎\n",
    "preview": "posts/2024-12-27-week08-03/2024-12-27-week08-03_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:34:02+02:00",
    "input_file": "2024-12-27-week08-03.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week08-04/",
    "title": "Enclosure",
    "description": "Visualization of hierarchical structure using containment.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading (Chapter 9), Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(tidygraph)\nlibrary(gridExtra)\ntheme_set(theme_graph())\n\n\nIf nodes can be conceptually organized into a hierarchy, then it’s possible\nto use enclosure (i.e., the containment of some visual marks within others) to\nencode those relationships.\n\n\ngraph <- tbl_graph(flare$vertices, flare$edges)\n\np1 <- ggraph(graph, \"tree\") +\n  geom_edge_link() +\n  geom_node_point(aes(size = size)) +\n  scale_size(range = c(0.1, 5))\np2 <- ggraph(graph, \"circlepack\", weight = size) +\n  geom_node_circle(aes(fill = depth)) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\ngrid.arrange(p1, p2, ncol = 2)\n\n\n\nFigure 1: A tree and the equivalent representation using containment. The outer circle corresponds to the root node in the tree, and paths down the tree are associated with sequences of nested circles.\n\n\n\nHierarchy most obviously occurs in trees, but it can also be present in\nnetworks with clustering structure at several levels. (see point 6 below).\nEnclosure is used in treemaps. In this visualization, each node is allocated\nan area, and all its children are drawn within that area (and so on,\nrecursively, down to the leaves).\n\n\nggraph(graph, \"treemap\", weight = size) +\n  geom_node_tile(aes(fill = depth, size = depth), size = 0.25) +\n  scale_fill_distiller(direction = 1) +\n  coord_fixed()\n\n\n\nFigure 2: A treemap representation associated with the tree from above.\n\n\n\nThis is a particularly useful visualization when it’s important to visualize\na continuous attribute associated with each node. For example, a large node\nmight correspond to a large part of a budget or a large directory in a\nfilesystem. Here is an example\nvisualization\nof Obama’s budget proposal in 2013.\nA caveat: treemaps are not so useful for making topological comparisons, like\nthe distance between two nodes in the tree.\nIn situations where network nodes can be organized hierarchically,\ncontainment marks can directly represent the these relationships. For example,\nin the network below, the red and yellow clusters are contained in a green\nsupercluster. The combination of node-link diagram and containment sets makes\nthis structure clear.\n\n\n\nFigure 3: An example of how hierarchy across groups of nodes can be encoded within a network.\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week08-04/2024-12-27-week08-04_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:34:07+02:00",
    "input_file": "2024-12-27-week08-04.knit.md",
    "preview_width": 2304,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week09-01/",
    "title": "K-means",
    "description": "An introduction to clustering and how to manage its output.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dslabs\")\nlibrary(\"ggplot2\")\nlibrary(\"knitr\")\nlibrary(\"tidymodels\")\nlibrary(\"tidyr\")\nlibrary(\"dplyr\")\ntheme_set(theme_minimal())\n\n\nThe goal of clustering is to discover distinct groups within a dataset. In an\nideal clustering, samples are very different between groups, but relatively\nsimilar within groups. At the end of a clustering routine, \\(K\\) clusters have\nbeen identified, and each sample is assigned to one of these \\(K\\) clusters. \\(K\\)\nmust be chosen by the user.\nClustering gives a compressed representation of the dataset. Therefore,\nclustering is useful for getting a quick overview of the high-level structure in\na dataset.\nFor example, clustering can be used in the following applications,\nUser segmentation: A marketing study might try to summarize different user\nbehaviors by clustering into a few key “segments.” Each segment might motivate\na different type of marketing campaign.\nGene expression profiling: A genomics study might try to identify genes whose\nexpression levels are similar across different experimental conditions. These\ngene clusters might be responsible for a shared biological function.\n\n\\(K\\)-means is a particular algorithm for finding clusters. First, it randomly\ninitializes \\(K\\) cluster centroids. Then, it alternates the following two steps\nuntil convergence,\nAssign points to their nearest cluster centroid.\nUpdate the \\(K\\) centroids to be the averages of points within their cluster.\n\nHere is an animation from the tidymodels page on \\(K\\)-means,\n\n\n\nNote that, since we have to take an average for each coordinate, we require\nthat our data be quantitative, not categorical1.\nWe illustrate this idea using the movielens dataset from the reading. This\ndataset has ratings (0.5 to 5) given by 671 users across 9066 movies. We can\nthink of this as a matrix of movies vs. users, with ratings within the entries.\nFor simplicity, we filter down to only the 50 most frequently rated movies. We\nwill assume that if a user never rated a movie, they would have given that movie\na zero2. We’ve skipped a\nfew steps used in the reading (subtracting movie / user averages and filtering\nto only active users), but the overall results are comparable.\n\n\ndata(\"movielens\")\nfrequently_rated <- movielens %>%\n  group_by(movieId) %>%\n  summarize(n=n()) %>%\n  top_n(50, n) %>%\n  pull(movieId)\n\nmovie_mat <- movielens %>% \n  filter(movieId %in% frequently_rated) %>%\n  select(title, userId, rating) %>%\n  pivot_wider(title, names_from = userId, values_from = rating, values_fill = 0)\n\nmovie_mat[1:10, 1:20]\n\n# A tibble: 10 × 20\n   title     `2`   `3`   `4`   `5`   `6`   `7`   `8`   `9`  `10`  `11`\n   <chr>   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 Seven …     4   0       0     0     0     0   5       3     0     0\n 2 Usual …     4   0       0     0     0     0   5       0     5     5\n 3 Braveh…     4   4       0     0     0     5   4       0     0     0\n 4 Apollo…     5   0       0     4     0     0   0       0     0     0\n 5 Pulp F…     4   4.5     5     0     0     0   4       0     0     5\n 6 Forres…     3   5       5     4     0     3   4       0     0     0\n 7 Lion K…     3   0       5     4     0     3   0       0     0     0\n 8 Mask, …     3   0       4     4     0     3   0       0     0     0\n 9 Speed       3   2.5     0     4     0     3   0       0     0     0\n10 Fugiti…     3   0       0     0     0     0   4.5     0     0     0\n# ℹ 9 more variables: `12` <dbl>, `13` <dbl>, `14` <dbl>, `15` <dbl>,\n#   `16` <dbl>, `17` <dbl>, `18` <dbl>, `19` <dbl>, `20` <dbl>\n\nNext, we run kmeans on this dataset. I’ve used the dplyr pipe notation to\nrun kmeans on the data above with “title” removed. augment is a function\nfrom the tidymodels package that adds the cluster labels identified by kmeans\nto the rows in the original dataset.\n\n\nkclust <- movie_mat %>%\n  select(-title) %>%\n  kmeans(centers = 10)\n\nmovie_mat <- augment(kclust, movie_mat) # creates column \".cluster\" with cluster label\nkclust <- tidy(kclust)\n\nmovie_mat %>%\n  select(title, .cluster) %>%\n  arrange(.cluster)\n\n# A tibble: 50 × 2\n   title                                              .cluster\n   <chr>                                              <fct>   \n 1 Lord of the Rings: The Return of the King, The     1       \n 2 Lord of the Rings: The Two Towers, The             1       \n 3 Lord of the Rings: The Fellowship of the Ring, The 1       \n 4 Saving Private Ryan                                2       \n 5 Sixth Sense, The                                   2       \n 6 Fight Club                                         2       \n 7 Shrek                                              2       \n 8 Matrix, The                                        2       \n 9 Good Will Hunting                                  2       \n10 Gladiator                                          2       \n# ℹ 40 more rows\n\nThere are two pieces of derived data generated by this routine,\nThe cluster assignments\nThe cluster centroids\nand both can be the subjects of visualization.\n\nIn our movie example, the cluster centroids are imaginary pseudo-movies that\nare representative of their cluster. They are represented by the scores they\nwould have received by each of the users in the dataset. This is visualized\nbelow. In a more realistic application, we would also want to display some\ninformation about each user; e.g., maybe some movies are more popular among\ncertain age groups or in certain regions.\n\n\nkclust_long <- kclust %>%\n  pivot_longer(`2`:`671`, names_to = \"userId\", values_to = \"rating\")\n\nggplot(kclust_long) +\n  geom_bar(\n    aes(x = reorder(userId, rating), y = rating),\n    stat = \"identity\"\n  ) +\n  facet_grid(cluster ~ .) +\n  labs(x = \"Users (sorted)\", y = \"Rating\") +\n  theme(\n    axis.text.x = element_blank(),\n    axis.text.y = element_text(size = 5),\n    strip.text.y = element_text(angle = 0)\n  )\n\n\n\nFigure 1: We can visualize each cluster by seeing the average ratings each user gave to the movies in that cluster (this is the definition of the centroid). An alternative visualization strategy would be to show a heatmap – we’ll discuss this soon in the superheat lecture.\n\n\n\nIt’s often of interest to relate the cluster assignments to complementary\ndata, to see whether the clustering reflects any previously known differences\nbetween the observations, which weren’t directly used in the clustering\nalgorithm.\nBe cautious: Outliers, nonspherical shapes, and variations in density can\nthrow off \\(K\\)-means.\n\n\n\nFigure 2: The difficulty that variations in density poses to k-means, from Cluster Analysis using K-Means Explained.\n\n\n\nThe goals of clustering are highly problem dependent, and different goals\nmight call for alternative algorithms. For example, consider the ways clustering\nmight be used to understand disease transmission. One problem might be to\ncluster the DNA sequences of the pathogenic agent, to recover its evolutionary\nhistory. This could be done using hierarchical clustering (next lecture). A\nsecond problem might be to determine whether patient outcomes might be driven by\none of a few environmental factors, in which case a \\(K\\)-means clustering across\nthe typical environmental factors would be reasonable. A third use would be to\nperform contact tracing, based on a network clustering algorithm. The point is\nthat no clustering algorithm is uniformly better than any other in all\nsituations, and the choice of which one to use should be guided by the problem\nrequirements.\n\nTo work with data of different\ntypes, it’s possible to use a variant called \\(K\\)-medoids, but this is beyond the\nscope of this class.↩︎\nThis is an approximation – there are probably many movies that the\nusers would have enjoyed had they had a chance to watch them.↩︎\n",
    "preview": "posts/2024-12-27-week09-01/2024-12-27-week09-01_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2025-01-14T17:34:16+02:00",
    "input_file": "2024-12-27-week09-01.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week01-01/",
    "title": "Introduction to ggplot2",
    "description": "A discussion of ggplot2 terminology, and an example of iteratively refining a\nsimple scatterplot.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\nggplot2 is an R implementation of the Grammar\nof Graphics. The idea is to define the basic “words” from which visualizations\nare built, and then let users compose them in original ways. This is in contrast\nto systems with prespecified chart types, where the user is forced to pick from\na limited dropdown menu of plots. Just like in ordinary language, the creative\ncombination of simple building blocks can support a very wide range of\nexpression.\nThese are libraries we’ll use in this lecture.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggrepel)\nlibrary(scales)\n\n\nComponents of a Graph\nWe’re going to create this plot in these notes.\n\n\n\nEvery ggplot2 plot is made from three components,\nData: This is the data.frame that we want to visualize.\nGeometry: These are the types of visual marks that appear on the plot.\nAesthetic Mapping: This links the data with the visual marks.\nThe Data\nLet’s load up the data. Each row is an observation, and each column is an\nattribute that describes the observation. This is important because each mark\nthat you see on a ggplot – a line, a point, a tile, … – had to start out as\na row within an R data.frame. The visual properties of the mark (e.g., color)\nare determined by the values along columns. These type of data are often\nreferred to as tidy data, and we’ll have a full week discussing this topic.\nHere’s an example of the data above in tidy format,\n\n\ndata(murders)\nhead(murders)\n\n       state abb region population total\n1    Alabama  AL  South    4779736   135\n2     Alaska  AK   West     710231    19\n3    Arizona  AZ   West    6392017   232\n4   Arkansas  AR  South    2915918    93\n5 California  CA   West   37253956  1257\n6   Colorado  CO   West    5029196    65\n\nThis is one example of how the same information might be stored in a non-tidy\nway, making visualization much harder.\n\n\nnon_tidy <- data.frame(t(murders))\ncolnames(non_tidy) <- non_tidy[1, ]\nnon_tidy <- non_tidy[-1, ]\nnon_tidy[, 1:6]\n\n            Alabama   Alaska  Arizona Arkansas California Colorado\nabb              AL       AK       AZ       AR         CA       CO\nregion        South     West     West    South       West     West\npopulation  4779736   710231  6392017  2915918   37253956  5029196\ntotal           135       19      232       93       1257       65\n\nOften, one of the hardest parts in making a ggplot2 plot is not coming up with\nthe right ggplot2 commands, but reshaping the data so that it’s in a tidy\nformat.\nGeometry\nThe words in the grammar of graphics are the geometry layers. We can associate\neach row of a data frame with points, lines, tiles, etc., just by referring to\nthe appropriate geom in ggplot2. A typical plot will compose a chain of layers\non top of a dataset,\n\nggplot(data) + [layer 1] + [layer 2] + …\n\nFor example, by deconstructing the plot above, we would expect to have point and\ntext layers. For now, let’s just tell the plot to put all the geom’s at the\norigin.\n\n\nggplot(murders) +\n  geom_point(x = 0, y = 0) +\n  geom_text(x = 0, y = 0, label = \"test\")\n\n\n\nYou can see all the types of geoms in the cheat\nsheet.\nWe’ll be experimenting with a few of these in a later\nlecture.\nAesthetic mappings\nAesthetic mappings make the connection between the data and the geometry. It’s\nthe piece that translates abstract data fields into visual properties. Analyzing\nthe original graph, we recognize these specific mappings,\nState Population → \\(x\\)-axis coordinate\nNumber of murders → \\(y\\)-axis coordinate\nGeographical region → color\nTo establish these mappings, we need to use the aes function. Notice that\ncolumn names don’t have to be quoted – ggplot2 knows to refer back to the\nmurders data frame in ggplot(murders).\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region))\n\n\n\nThe original plot used a log-scale. To transform the x and y axes, we can use\nscales.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\nOnce nuance is that scales aren’t limited to \\(x\\) and \\(y\\) transformations. They\ncan be applied to modify any relationship between a data field and its\nappearance on the page. For example, this changes the mapping between the region\nfield and circle color.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  scale_x_log10() +\n  scale_y_log10() +\n  scale_color_manual(values = c(\"#6a4078\", \"#aa1518\", \"#9ecaf8\", \"#50838c\")) # exercise: find better colors using https://imagecolorpicker.com/\n\n\n\nA problem with this graph is that it doesn’t tell us which state each point\ncorresponds to. For that, we’ll need text labels. We can encode the coordinates\nfor these marks again using aes, but this time within a geom_text layer.\n\n\nggplot(murders) +\n  geom_point(aes(x = population, y = total, col = region)) +\n  geom_text(\n    aes(x = population, y = total, label = abb),\n    nudge_x = 0.08 # what would happen if I remove this?\n  ) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\nNote that each type of layer uses different visual properties to encode the data\n– the argument label is only available for the geom_text layer. You can see\nwhich aesthetic mappings are required for each type of geom by checking that\ngeom’s documentation page, under the Aesthetics heading.\nIt’s usually a good thing to make your code as concise as possible. For ggplot2,\nwe can achieve this by sharing elements across aes calls (e.g., not having to\ntype population and total twice). This can be done by defining a “global”\naesthetic, putting it inside the initial ggplot call.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_point(aes(col = region)) +\n  geom_text(aes(label = abb), nudge_x = 0.08) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\nFinishing touches\nHow can we improve the readability of this plot? You might already have ideas,\nPrevent labels from overlapping. It’s impossible to read some of the state names.\nAdd a line showing the national rate. This serves as a point of reference,\nallowing us to see whether an individual state is above or below the national\nmurder rate.\nGive meaningful axis / legend labels and a title.\nMove the legend to the top of the figure. Right now, we’re wasting a lot of\nvisual real estate in the right hand side, just to let people know what each\ncolor means.\nUse a better color theme.\nFor 1., the ggrepel package find better state name positions, drawing links\nwhen necessary.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) + # I moved it up so that the geom_point's appear on top of the lines\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\nFor 2., let’s first compute the national murder rate,\n\n\nr <- murders %>% \n  summarize(rate = sum(total) /  sum(population)) %>%\n  pull(rate)\nr\n\n[1] 3.034555e-05\n\nNow, we can use this as the slope in a geom_abline layer, which encodes a\nslope and intercept as a line on a graph.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10() +\n  scale_y_log10()\n\n\n\nFor 3., we can add a labs layer to write labels and a theme to reposition\nthe legend. I used unit_format from the scales package to change the\nscientific notation in the \\(x\\)-axis labels to something more readable.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) + # used to convert scientific notation to readable labels\n  scale_y_log10() +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme(legend.position = \"top\")\n\n\n\nFor 5., I find the gray background with reference lines a bit distracting. We\ncan simplify the appearance using theme_bw. I also like the colorbrewer\npalette, which can be used by calling a different color scale.\n\n\nggplot(murders, aes(x = population, y = total)) +\n  geom_abline(intercept = log10(r), linewidth = 0.4, col = \"#b3b3b3\") +\n  geom_text_repel(aes(label = abb), segment.size = 0.2) +\n  geom_point(aes(col = region)) +\n  scale_x_log10(labels = unit_format(unit = \"million\", scale = 1e-6)) +\n  scale_y_log10() +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"Population (log scale)\",\n    y = \"Total number of murders (log scale)\",\n    color = \"Region\",\n    title = \"US Gun Murders in 2010\"\n  ) +\n  theme_bw() +\n  theme(\n    legend.position = \"top\",\n    panel.grid.minor = element_blank()\n  )\n\n\n\nSome bonus exercises, which will train you to look at your graphics more\ncarefully, as well as build your familiarity with ggplot2.\nTry reducing the size of the text labels. Hint: use the size argument in\ngeom_text_repel.\nIncrease the size of the circles in the legend. Hint: Use override.aes\nwithin a guide.\nRe-order the order of regions in the legend. Hint: Reset the factor levels in\nthe region field of the murders data.frame.\nOnly show labels for a subset of states that are far from the national rate.\nHint: Filter the murders data.frame, and use a data field specific to the\ngeom_text_repel layer.\n\n\n\n",
    "preview": "posts/2024-12-27-week01-01/2024-12-27-week01-01_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:37:09+02:00",
    "input_file": "2024-12-27-week01-01.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week01-02/",
    "title": "A Vocabulary of Marks",
    "description": "Examples of encodings and sequential refinement of a plot.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\n\n\n\nReading, Recording, Rmarkdown\nThe choice of encodings can have a strong effect on (1) the types of\ncomparisons that a visualization suggests and (2) the chance that readers leave with complete and accurate conclucions.\nWith this in mind, it’s worthwhile\nto develop a rich vocabulary of potential visual encodings.\nSo, let’s look at a few different types of encodings available in ggplot2.\nBefore we get started, let’s load up the libraries that will be used in these\nnotes. ggplot2 is our plotting library. readr is used to read data files\nfrom a web link, and dplyr is useful for some of the data manipulations below\n(we dive into it deeply in Week 2).\n\n\nlibrary(readr)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(scales)\ntheme_set(theme_bw()) # create a simpler default theme\n\n\nPoint Marks\nLet’s read in the gapminder dataset, which describes the changes in standard\nof living around the world over the last few decades. The %>% “pipe” operator\ntakes the output of the previous command as input to the next one – it is\nuseful for chains of commands where the intermediate results are not needed. The\nmutate command makes sure that the country group variable is treated as a\ncategorical, and not numeric, variable.\n\n\ngapminder <- read_csv(\"https://uwmadison.box.com/shared/static/dyz0qohqvgake2ghm4ngupbltkzpqb7t.csv\", col_types = cols()) %>%\n  mutate(cluster = as.factor(cluster))  # specify that cluster is nominal\ngap2000 <- filter(gapminder, year == 2000) # keep only year 2000\n\n\nPoint marks can encode data fields using their \\(x\\) and \\(y\\) positions, color,\nsize, and shape. Below, each mark is a country, and we’re using shape and the\n\\(y\\) position to distinguish between country clusters.\n\n\nggplot(gap2000) +\n  geom_point(aes(x = fertility, y = cluster, shape = cluster))\n\n\n\nSince the first two arguments in aes are always the x and y positions, we\ncan omit it from our command. The code below produces the exact same plot (try\nit!).\n\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster, shape = cluster))\n\n\nWe can specify different types of shapes using the shape parameter outside of\nthe aes encoding.\n\n\nggplot(gap2000) +\n  geom_point(aes(fertility, cluster), shape = 15)\n\n\n\nBar Marks\nBar marks let us associate a continuous field with a nominal one.\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\")\n\n\n\nThe plot above is messy – it would not be appropriate for a publication or\npresentation. The grid lines associated with each bar are distracting. Further,\nthe axis labels are all running over one another. For the first issue, we can\ncustomize the theme of the plot. Note that we don’t have to memorize the names\nof these arguments, since they should autocomplete when pressing tab (we just\nneed to memorize the first few letters).\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(panel.grid.major.x = element_blank())\n\n\n\nFor the second issue, one approach is to turn the labels on their side, again by\ncustomizing the theme.\n\n\nggplot(gap2000) +\n  geom_bar(aes(country, pop), stat = \"identity\") +\n  theme(\n    axis.text.x = element_text(angle = 90),\n    panel.grid.major.x = element_blank()\n  )\n\n\n\nAn approach I like better is to turn the bars on their side. This way, readers\ndon’t have to tilt their heads to read the country names.\n\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(panel.grid.major.y = element_blank()) # note change from x to y\n\n\n\nI’m also going to remove the small tick marks associated with every name, again\nbecause it seems distracting.\n\n\nggplot(gap2000) +\n  geom_bar(aes(pop, country), stat = \"identity\") +\n  theme(\n    panel.grid.major.y = element_blank(),\n    axis.ticks = element_blank() # remove tick marks\n  )\n\n\n\nTo make comparisons between countries with similar populations easier, we can\norder them by population (alphabetical ordering is not that meaningful). To\ncompare clusters, we can color in the bars.\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nWe’ve been spending a lot of time on this plot. This is because I want to\nemphasize that a visualization is not just something we can get just by memorizing some\nmagic (programming) incantation. Instead, it is something worth critically\nengaging with and refining, in a similar way that we would refine an essay or\nspeech.\nPhilosophy aside, there are still a few points that need to be improved in this figure,\nThe axis titles are not meaningful.\nThere is a strange gap between the left hand edge of the plot and the start of the bars.\nI would also prefer if the bars were exactly touching one another, without the small vertical gap.\nThe scientific notation for population size is unnecessarily technical.\nThe color scheme is a bit boring.\nI’ll address each of these in a separate code block, with comments on the\nparts that are different. First, improving the axis titles,\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + # add better titles\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nNow we remove the gap. I learned this trick by googling\nit\n– there is no shame in doing this! A wise friend of mine once shared, “I am\nnot a programming expert, just an expert at StackOverflow.”\n\n\nggplot(gap2000) +\n   geom_bar(aes(pop, reorder(country, pop), fill = cluster), stat = \"identity\") +\n   scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) + # remove space to the axis\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\") + \n   theme(\n     axis.text.y = element_text(size = 6),\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nNow, removing the gaps between bars.\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\" # increase width of bars\n   ) +\n   scale_x_continuous(expand = c(0, 0, 0.1, 0.1)) +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nNow, we remove scientific notation,\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\"\n   ) +\n   scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. ::omma() is also useful.\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nFinally, we customize the colors. I often like to look up neat colors on\ncolor.adobe.com,\niwanthue or\ncolorhexa, but there are dozens of similar\ncolorpicker sites out there.\n\n\nggplot(gap2000) +\n   geom_bar(\n     aes(pop, reorder(country, pop), fill = cluster),\n     width = 1, stat = \"identity\"\n   ) +\n   scale_x_continuous(label = label_number(scale_cut = cut_short_scale()), expand = c(0, 0, 0.1, 0.1)) + # remove scientific notation. comma() is also useful.\n   scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n   labs(x = \"Population\", y = \"Country\", fill = \"Country Group\", color = \"Country Group\") +\n   theme(\n     axis.ticks = element_blank(),\n     panel.grid.major.y = element_blank()\n   )\n\n\n\nThis seems like a lot of work for just a lowly bar plot! But I think it’s\namazing customizable the figure is – we can give it our own sense of style.\nWith a bit of practice, these sorts of modifications will become second nature,\nand it won’t be necessary to keep track of all the intermediate code. And\nreally, even though we spent some time on this plot, there are still many things\nthat could be interesting to experiment with, like font styles, background\nappearance, maybe even splitting the countries into two panels.\nIn the plot above, each bar is anchored at 0. Instead, we could have each\nbar encode two continuous values, a left and right. To illustrate, let’s\ncompare the minimum and maximimum life expectancies within each country cluster.\nWe’ll need to create a new data.frame with just the summary information. For\nthis, we group_by each cluster, so that a summarise call finds the minimum\nand maximum life expectancies restricted to each cluster. We’ll discuss the\ngroup_by + summarise pattern in detail next week.\n\n\n# find summary statistics\nlife_ranges <- gap2000 %>%\n  group_by(cluster) %>%\n  summarise(\n    min_life = min(life_expect),\n    max_life = max(life_expect)\n  )\n\n# look at a few rows\nhead(life_ranges)\n\n# A tibble: 6 × 3\n  cluster min_life max_life\n  <fct>      <dbl>    <dbl>\n1 0           42.1     63.6\n2 1           70.5     80.6\n3 2           43.4     53.4\n4 3           58.1     79.8\n5 4           66.7     82  \n6 5           57.0     79.7\n\nggplot(life_ranges) +\n  geom_segment(\n    aes(min_life, reorder(cluster, max_life), xend = max_life, yend = cluster, col = cluster),\n    size = 5,\n  ) +\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\")) +\n  labs(x = \"Minimum and Maximum Expected Span\", col = \"Country Group\", y = \"Country Group\") +\n  xlim(0, 85) # otherwise would only range from 42 to 82\n\n\n\nLine Marks\nLine marks are useful for comparing changes. Our eyes naturally focus on\nrates of change when we see lines. Below, we’ll plot the fertility over time,\ncolored in by country cluster. The group argument is useful for ensuring each\ncountry gets its own line; if we removed it, ggplot2 would become confused by\nthe fact that the same x (year) values are associated with multiple y’s\n(fertility rates).\n\n\nggplot(gapminder) +\n  geom_line(\n    aes(year, fertility, col = cluster, group = country),\n      alpha = 0.7, size = 0.9\n  ) +\n  scale_x_continuous(expand = c(0, 0)) +  # same trick of removing gap\n  scale_color_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\nArea Marks\nArea marks have a flavor of both bar and line marks. The filled area supports\nabsolute comparisons, while the changes in shape suggest derivatives.\n\n\npopulation_sums <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(total_pop = sum(pop))\nhead(population_sums)\n\n# A tibble: 6 × 3\n# Groups:   year [1]\n   year cluster total_pop\n  <dbl> <fct>       <dbl>\n1  1955 0       495927174\n2  1955 1       360609771\n3  1955 2        60559800\n4  1955 3       355392405\n5  1955 4       854125031\n6  1955 5        56064015\n\nggplot(population_sums) +\n  geom_area(aes(year, total_pop, fill = cluster)) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale()))  + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\nJust like in bar marks, we don’t necessarily need to anchor the \\(y\\)-axis at 0.\nFor example, here the bottom and top of each area mark is given by the 30% and\n70% quantiles of population within each country cluster.\n\n\npopulation_ranges <- gapminder %>%\n  group_by(year, cluster) %>%\n  summarise(min_pop = quantile(pop, 0.3), max_pop = quantile(pop, 0.7))\nhead(population_ranges)\n\n# A tibble: 6 × 4\n# Groups:   year [1]\n   year cluster   min_pop   max_pop\n  <dbl> <fct>       <dbl>     <dbl>\n1  1955 0       40880121. 83941368.\n2  1955 1        4532940  25990229.\n3  1955 2        6600426. 17377594.\n4  1955 3        2221139   8671500 \n5  1955 4        9014491  61905422 \n6  1955 5        3007625  12316126.\n\nggplot(population_ranges) +\n  geom_ribbon(\n    aes(x = year, ymin = min_pop, ymax = max_pop, fill = cluster),\n    alpha = 0.8\n  ) +\n  scale_y_continuous(expand = c(0, 0, 0.1, 0.1), label = label_number(scale_cut = cut_short_scale())) + # remove scientific notation. scales::comma() is also useful.\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_manual(values = c(\"#80BFA2\", \"#7EB6D9\", \"#3E428C\", \"#D98BB6\", \"#BF2E21\", \"#F23A29\"))\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week01-02/2024-12-27-week01-02_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-01-14T17:37:12+02:00",
    "input_file": "2024-12-27-week01-02.knit.md",
    "preview_width": 1248,
    "preview_height": 960
  },
  {
    "path": "posts/2024-12-27-week02-01/",
    "title": "Tidy Data",
    "description": "The definition of tidy data, and why it's often helpful for visualization._",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\nA dataset is called tidy if rows correspond to distinct observations and columns correspond to distinct variables.\n\nFor visualization, it is important that data be in tidy format. This is because (a) each visual mark will be associated with a row of the dataset and (b) properties of the visual marks will determined by values within the columns. A plot that is easy to create when the data are in tidy format might be very hard to create otherwise.\nThe tidy data might seem like an idea so natural that it’s not worth teaching (let alone formalizing). However, exceptions are encountered frequently, and it’s important that you be able to spot them. Further, there are now many utilities for “tidying” data, and they are worth becoming familiar with.\nHere is an example of a tidy dataset.\n\n\ntable1\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nIt is easy to visualize the tidy dataset.\n\n\nggplot(table1, aes(x = year, y = cases, col = country)) +\n  geom_point() +\n  geom_line()\n\n\n\nBelow are three non-tidy versions of the same dataset. They are\nrepresentative of more general classes of problems that may arise,\nA variable might be implicitly stored within column names, rather than\nexplicitly stored in its own column. Here, the years are stored as column\nnames. It’s not really possible to create the plot above using the data in this\nformat.\n\n\n\ntable4a # cases\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b # population\n\n# A tibble: 3 × 3\n  country         `1999`     `2000`\n  <chr>            <dbl>      <dbl>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\nThe same observation may appear in multiple rows, where each instance of the\nrow is associated with a different variable. Here, the observations are the\ncountry by year combinations.\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nA single column actually stores multiple variables. Here, rate is being\nused to store both the population and case count variables.\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\nThe trouble is that this variable has to be stored as a character; otherwise, we\nlose access to the original population and case variable. But, this makes the\nplot useless.\n\n\nggplot(table3, aes(x = year, y = rate)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\nThe next few lectures provide tools for addressing these three problems.\nA few caveats are in order. It’s easy to become a tidy-data purist, and lose\nsight of the bigger data-analytic picture. To prevent that, first, remember that\nwhat is or is not tidy may be context dependent. Maybe you want to treat each\nweek as an observation, rather than each day. Second, know that there are\nsometimes computational reasons to prefer non-tidy data. For example, “long”\ndata often require more memory, since column names that were originally stored\nonce now have to be copied onto each row. Certain statistical models are also\nsometimes best framed as matrix operations on non-tidy datasets.\n\n\n\n",
    "preview": "posts/2024-12-27-week02-01/2024-12-27-week02-01_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:37:16+02:00",
    "input_file": "2024-12-27-week02-01.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week02-02/",
    "title": "Pivoting",
    "description": "Tools for reshaping data into tidy format.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\ntheme_set(theme_bw())\n\n\nPivoting refers to the process of changing the interpretation of each row in a data frame. It is useful for addressing problems 1 - 2 in the previous lecture, which we repeat here for completeness.\nA variable might be implicitly stored within column names, rather than explicitly stored in its own column.\nThe same observation may appear in multiple rows, where each instance of the row is associated with a different variable.\n\nTo address (a), we can use the pivot_longer function in tidyr. It takes an implicitly stored variable and explicitly stores it in a column defined by the names_to argument. In\nThe example below shows pivot_longer being used to tidy one of the non-tidy tuberculosis datasets. Note that the data has doubled in length, because there are now two rows per country (one per year).\nFor reference, these are the original data.\n\n\ntable4a\n\n# A tibble: 3 × 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\nThis step lengthens the data,\n\n\ntable4a_longer <- table4a %>% \n  pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = \"cases\")\n\ntable4a_longer\n\n# A tibble: 6 × 3\n  country     year   cases\n  <chr>       <chr>  <dbl>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\n\n\ndim(table4a)\n\n[1] 3 3\n\ndim(table4a_longer)\n\n[1] 6 3\n\nWe can pivot both the population and the cases table, then combine them using\na join operation. A join operation matches rows across two tables according to\ntheir shared columns.\n\n\n# helper function, to avoid copying and pasting code\npivot_fun <- function(x, value_column = \"cases\") {\n  x %>%\n    pivot_longer(c(`1999`, `2000`), names_to = \"year\", values_to = value_column)\n}\n\ntable4 <- left_join(\n  pivot_fun(table4a), # look for all country x year combinations in left table\n  pivot_fun(table4b, \"population\") # and find matching rows in right table\n)\ntable4\n\n# A tibble: 6 × 4\n  country     year   cases population\n  <chr>       <chr>  <dbl>      <dbl>\n1 Afghanistan 1999     745   19987071\n2 Afghanistan 2000    2666   20595360\n3 Brazil      1999   37737  172006362\n4 Brazil      2000   80488  174504898\n5 China       1999  212258 1272915272\n6 China       2000  213766 1280428583\n\nThis lets us make the year vs. rate plot that we had tried to put together in\nthe last lecture. It’s much easier to recognize trends when comparing the rates,\nthan when looking at the raw case counts.\n\n\nggplot(table4, aes(x = year, y = cases / population, col = country)) +\n  geom_point() +\n  geom_line(aes(group = country))\n\n\n\nTo address (b), we can use the pivot_wider function. It spreads the column\nin the values_from argument across new columns specified by the names_from\nargument.\nThe example below shows pivot_wider being used to tidy one of the other non-tidy datasets. Note when there are more than two levels in the names_from column, this will always be wider than the starting data frame, which is why this operation is called pivot_wider.\nFor reference, here is table2 before pivoting.\n\n\ntable2\n\n# A tibble: 12 × 4\n   country      year type            count\n   <chr>       <dbl> <chr>           <dbl>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nNow, we spread the cases and population variables into their own columns.\n\n\ntable2 %>%\n    pivot_wider(names_from = type, values_from = count)\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <dbl>      <dbl>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\n\n",
    "preview": "posts/2024-12-27-week02-02/2024-12-27-week02-02_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2025-01-14T17:37:21+02:00",
    "input_file": "2024-12-27-week02-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week02-03/",
    "title": "Deriving Variables",
    "description": "Using `separate`, `mutate`, and `summarise` to derive new variables for\ndownstream visualization.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\n\n\nIt’s easiest to define visual encodings when the variables we want to encode are contained in their own columns. After sketching out a visualization of interest, we may find that these variables are not explicitly represented among the columns of the raw dataset. In this case, we may need to derive them based on what is available. The dplyr and tidyr packages provide functions for deriving new variables, which we review in these notes.\nSometimes a single column is used to implicitly store several variables. To make the data tidy, separate can be used to split that single column into several columns, each of which corresponds to exactly one variable.\nThe block below separates our earlier table3, which stored rate as a\nfraction in a character column. The original table was,\n\n\ntable3\n\n# A tibble: 6 × 3\n  country      year rate             \n  <chr>       <dbl> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\nand the separated version is,\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) # try without convert, and compare the data types of the columns\n\n# A tibble: 6 × 4\n  country      year  cases population\n  <chr>       <dbl>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nNote that this function has an inverse, called unite, which can merge\nseveral columns into one. This is sometimes useful, but not as often as\nseparate, since it isn’t needed to tidy a dataset.\nSeparating a single column into several is a special case of a more general\noperation, mutate, which defines new columns as functions of existing ones. We\nhave used this is in previous lectures, but now we can philosophically justify\nit: the variables we want to encode need to be defined in advance.\nFor example, we may want to create a column rate that includes cases over\npopulation,\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year)\n\n# A tibble: 6 × 5\n  country      year  cases population    rate\n  <chr>       <dbl>  <int>      <int>   <dbl>\n1 Afghanistan  1999    745   19987071   0.373\n2 Afghanistan  2000   2666   20595360   1.33 \n3 Brazil       1999  37737  172006362  18.9  \n4 Brazil       2000  80488  174504898  40.2  \n5 China        1999 212258 1272915272 106.   \n6 China        2000 213766 1280428583 107.   \n\nSometimes, the variables of interest are functions of several rows. For\nexample, perhaps we want to visualize averages of a variable across age groups.\nIn this case, we can derive a summary across groups of rows using the\ngroup_by-followed-by-summarise pattern.\nFor example, perhaps we want the average rate over both years.\n\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE) %>%\n  mutate(rate = cases / year) %>%\n  group_by(country) %>%\n  summarise(avg_rate = mean(rate))\n\n# A tibble: 3 × 2\n  country     avg_rate\n  <chr>          <dbl>\n1 Afghanistan    0.853\n2 Brazil        29.6  \n3 China        107.   \n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:37:24+02:00",
    "input_file": "2024-12-27-week02-03.knit.md"
  },
  {
    "path": "posts/2024-12-27-week02-04/",
    "title": "Tidy Data Example",
    "description": "An extended example of tidying a real-world dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(skimr)\n\n\nLet’s work through the example of tidying a WHO dataset. This was discussed\nin the reading and is good practice in pivoting and deriving new variables.\nThe raw data, along with a summary from the skimr package, is shown below\n(notice the small multiples!).\n\n\nwho\n\n# A tibble: 7,240 × 60\n   country     iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>       <chr> <chr> <dbl>       <dbl>        <dbl>        <dbl>\n 1 Afghanistan AF    AFG    1980          NA           NA           NA\n 2 Afghanistan AF    AFG    1981          NA           NA           NA\n 3 Afghanistan AF    AFG    1982          NA           NA           NA\n 4 Afghanistan AF    AFG    1983          NA           NA           NA\n 5 Afghanistan AF    AFG    1984          NA           NA           NA\n 6 Afghanistan AF    AFG    1985          NA           NA           NA\n 7 Afghanistan AF    AFG    1986          NA           NA           NA\n 8 Afghanistan AF    AFG    1987          NA           NA           NA\n 9 Afghanistan AF    AFG    1988          NA           NA           NA\n10 Afghanistan AF    AFG    1989          NA           NA           NA\n# ℹ 7,230 more rows\n# ℹ 53 more variables: new_sp_m3544 <dbl>, new_sp_m4554 <dbl>,\n#   new_sp_m5564 <dbl>, new_sp_m65 <dbl>, new_sp_f014 <dbl>,\n#   new_sp_f1524 <dbl>, new_sp_f2534 <dbl>, new_sp_f3544 <dbl>,\n#   new_sp_f4554 <dbl>, new_sp_f5564 <dbl>, new_sp_f65 <dbl>,\n#   new_sn_m014 <dbl>, new_sn_m1524 <dbl>, new_sn_m2534 <dbl>,\n#   new_sn_m3544 <dbl>, new_sn_m4554 <dbl>, new_sn_m5564 <dbl>, …\n\n\n\nskim(who)\n\nTable 1: Data summary\nName\nwho\nNumber of rows\n7240\nNumber of columns\n60\n_______________________\n\nColumn type frequency:\n\ncharacter\n3\nnumeric\n57\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncountry\n0\n1\n4\n52\n0\n219\n0\niso2\n34\n1\n2\n2\n0\n218\n0\niso3\n0\n1\n3\n3\n0\n219\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nyear\n0\n1.00\n1996.56\n9.83\n1980\n1988.00\n1997.0\n2005.00\n2013\n▇▇▇▇▇\nnew_sp_m014\n4067\n0.44\n83.71\n316.14\n0\n0.00\n5.0\n37.00\n5001\n▇▁▁▁▁\nnew_sp_m1524\n4031\n0.44\n1015.66\n4885.38\n0\n9.00\n90.0\n502.00\n78278\n▇▁▁▁▁\nnew_sp_m2534\n4034\n0.44\n1403.80\n5718.39\n0\n14.00\n150.0\n715.50\n84003\n▇▁▁▁▁\nnew_sp_m3544\n4021\n0.44\n1315.88\n6003.26\n0\n13.00\n130.0\n583.50\n90830\n▇▁▁▁▁\nnew_sp_m4554\n4017\n0.45\n1103.86\n5441.06\n0\n12.00\n102.0\n440.00\n82921\n▇▁▁▁▁\nnew_sp_m5564\n4022\n0.44\n800.70\n4418.31\n0\n8.00\n63.0\n279.00\n63814\n▇▁▁▁▁\nnew_sp_m65\n4031\n0.44\n682.82\n4089.14\n0\n8.00\n53.0\n232.00\n70376\n▇▁▁▁▁\nnew_sp_f014\n4066\n0.44\n114.33\n504.63\n0\n1.00\n7.0\n50.75\n8576\n▇▁▁▁▁\nnew_sp_f1524\n4046\n0.44\n826.11\n3552.02\n0\n7.00\n66.0\n421.00\n53975\n▇▁▁▁▁\nnew_sp_f2534\n4040\n0.44\n917.30\n3580.15\n0\n9.00\n84.0\n476.25\n49887\n▇▁▁▁▁\nnew_sp_f3544\n4041\n0.44\n640.43\n2542.51\n0\n6.00\n57.0\n308.00\n34698\n▇▁▁▁▁\nnew_sp_f4554\n4036\n0.44\n445.78\n1799.23\n0\n4.00\n38.0\n211.00\n23977\n▇▁▁▁▁\nnew_sp_f5564\n4045\n0.44\n313.87\n1381.25\n0\n3.00\n25.0\n146.50\n18203\n▇▁▁▁▁\nnew_sp_f65\n4043\n0.44\n283.93\n1267.94\n0\n4.00\n30.0\n129.00\n21339\n▇▁▁▁▁\nnew_sn_m014\n6195\n0.14\n308.75\n1727.25\n0\n1.00\n9.0\n61.00\n22355\n▇▁▁▁▁\nnew_sn_m1524\n6210\n0.14\n513.02\n3643.27\n0\n2.00\n15.5\n102.00\n60246\n▇▁▁▁▁\nnew_sn_m2534\n6218\n0.14\n653.69\n3430.03\n0\n2.00\n23.0\n135.50\n50282\n▇▁▁▁▁\nnew_sn_m3544\n6215\n0.14\n837.87\n8524.53\n0\n2.00\n19.0\n132.00\n250051\n▇▁▁▁▁\nnew_sn_m4554\n6213\n0.14\n520.79\n3301.70\n0\n2.00\n19.0\n127.50\n57181\n▇▁▁▁▁\nnew_sn_m5564\n6219\n0.14\n448.62\n3488.68\n0\n2.00\n16.0\n101.00\n64972\n▇▁▁▁▁\nnew_sn_m65\n6220\n0.14\n460.36\n3991.90\n0\n2.00\n20.5\n111.75\n74282\n▇▁▁▁▁\nnew_sn_f014\n6200\n0.14\n291.95\n1647.30\n0\n1.00\n8.0\n58.00\n21406\n▇▁▁▁▁\nnew_sn_f1524\n6218\n0.14\n407.90\n2379.13\n0\n1.00\n12.0\n89.00\n35518\n▇▁▁▁▁\nnew_sn_f2534\n6224\n0.14\n466.26\n2272.86\n0\n2.00\n18.0\n103.25\n28753\n▇▁▁▁▁\nnew_sn_f3544\n6220\n0.14\n506.59\n5013.53\n0\n1.00\n11.0\n82.25\n148811\n▇▁▁▁▁\nnew_sn_f4554\n6222\n0.14\n271.16\n1511.72\n0\n1.00\n10.0\n76.75\n23869\n▇▁▁▁▁\nnew_sn_f5564\n6223\n0.14\n213.39\n1468.62\n0\n1.00\n8.0\n56.00\n26085\n▇▁▁▁▁\nnew_sn_f65\n6221\n0.14\n230.75\n1597.70\n0\n1.00\n13.0\n74.00\n29630\n▇▁▁▁▁\nnew_ep_m014\n6202\n0.14\n128.61\n460.14\n0\n0.00\n6.0\n55.00\n7869\n▇▁▁▁▁\nnew_ep_m1524\n6214\n0.14\n158.30\n537.74\n0\n1.00\n11.0\n88.00\n8558\n▇▁▁▁▁\nnew_ep_m2534\n6220\n0.14\n201.23\n764.05\n0\n1.00\n13.0\n124.00\n11843\n▇▁▁▁▁\nnew_ep_m3544\n6216\n0.14\n272.72\n3381.41\n0\n1.00\n10.5\n91.25\n105825\n▇▁▁▁▁\nnew_ep_m4554\n6220\n0.14\n108.11\n380.61\n0\n1.00\n8.5\n63.25\n5875\n▇▁▁▁▁\nnew_ep_m5564\n6225\n0.14\n72.17\n234.55\n0\n1.00\n7.0\n46.00\n3957\n▇▁▁▁▁\nnew_ep_m65\n6222\n0.14\n78.94\n227.34\n0\n1.00\n10.0\n55.00\n3061\n▇▁▁▁▁\nnew_ep_f014\n6208\n0.14\n112.89\n446.55\n0\n0.00\n5.0\n50.00\n6960\n▇▁▁▁▁\nnew_ep_f1524\n6219\n0.14\n149.17\n543.89\n0\n1.00\n9.0\n78.00\n7866\n▇▁▁▁▁\nnew_ep_f2534\n6219\n0.14\n189.52\n761.79\n0\n1.00\n12.0\n95.00\n10759\n▇▁▁▁▁\nnew_ep_f3544\n6219\n0.14\n241.70\n3218.50\n0\n1.00\n9.0\n77.00\n101015\n▇▁▁▁▁\nnew_ep_f4554\n6223\n0.14\n93.77\n339.33\n0\n1.00\n8.0\n56.00\n6759\n▇▁▁▁▁\nnew_ep_f5564\n6223\n0.14\n63.04\n212.95\n0\n1.00\n6.0\n42.00\n4684\n▇▁▁▁▁\nnew_ep_f65\n6226\n0.14\n72.31\n202.72\n0\n0.00\n10.0\n51.00\n2548\n▇▁▁▁▁\nnewrel_m014\n7050\n0.03\n538.18\n2082.18\n0\n5.00\n32.5\n210.00\n18617\n▇▁▁▁▁\nnewrel_m1524\n7058\n0.03\n1489.51\n6848.18\n0\n17.50\n171.0\n684.25\n84785\n▇▁▁▁▁\nnewrel_m2534\n7057\n0.03\n2139.72\n7539.87\n0\n25.00\n217.0\n1091.00\n76917\n▇▁▁▁▁\nnewrel_m3544\n7056\n0.03\n2036.40\n7847.94\n0\n24.75\n208.0\n851.25\n84565\n▇▁▁▁▁\nnewrel_m4554\n7056\n0.03\n1835.07\n8324.28\n0\n19.00\n175.0\n688.50\n100297\n▇▁▁▁▁\nnewrel_m5564\n7055\n0.03\n1525.30\n8760.27\n0\n13.00\n136.0\n536.00\n112558\n▇▁▁▁▁\nnewrel_m65\n7058\n0.03\n1426.00\n9431.99\n0\n17.00\n117.0\n453.50\n124476\n▇▁▁▁▁\nnewrel_f014\n7050\n0.03\n532.84\n2117.78\n0\n5.00\n32.5\n226.00\n18054\n▇▁▁▁▁\nnewrel_f1524\n7056\n0.03\n1161.85\n4606.76\n0\n10.75\n123.0\n587.75\n49491\n▇▁▁▁▁\nnewrel_f2534\n7058\n0.03\n1472.80\n5259.59\n0\n18.00\n161.0\n762.50\n44985\n▇▁▁▁▁\nnewrel_f3544\n7057\n0.03\n1125.01\n4210.58\n0\n12.50\n125.0\n544.50\n38804\n▇▁▁▁▁\nnewrel_f4554\n7057\n0.03\n877.27\n3556.18\n0\n10.00\n92.0\n400.50\n37138\n▇▁▁▁▁\nnewrel_f5564\n7057\n0.03\n686.41\n3379.33\n0\n8.00\n69.0\n269.00\n40892\n▇▁▁▁▁\nnewrel_f65\n7055\n0.03\n683.76\n3618.47\n0\n9.00\n69.0\n339.00\n47438\n▇▁▁▁▁\n\nAccording to the data dictionary, the columns have the following meanings,\nThe first three letters -> are we counting new or old cases of TB?\nNext two letters -> Type of tab.\nSixth letter -> Sex of patients\nRemaining numbers -> Age group. E.g., 3544 should be interpreted as 35 - 44\nyears old.\nOur first step is to pivot_longer. There is quite a bit of information\nimplicitly stored in the column names, and we want to make those variables\nexplicitly available for visual encoding.\n\n\nwho_longer <- who %>% \n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,  # notice we can refer to groups of columns without naming each one\n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  )\n\nwho_longer\n\n# A tibble: 76,046 × 6\n   country     iso2  iso3   year key          cases\n   <chr>       <chr> <chr> <dbl> <chr>        <dbl>\n 1 Afghanistan AF    AFG    1997 new_sp_m014      0\n 2 Afghanistan AF    AFG    1997 new_sp_m1524    10\n 3 Afghanistan AF    AFG    1997 new_sp_m2534     6\n 4 Afghanistan AF    AFG    1997 new_sp_m3544     3\n 5 Afghanistan AF    AFG    1997 new_sp_m4554     5\n 6 Afghanistan AF    AFG    1997 new_sp_m5564     2\n 7 Afghanistan AF    AFG    1997 new_sp_m65       0\n 8 Afghanistan AF    AFG    1997 new_sp_f014      5\n 9 Afghanistan AF    AFG    1997 new_sp_f1524    38\n10 Afghanistan AF    AFG    1997 new_sp_f2534    36\n# ℹ 76,036 more rows\n\nThe new column key contains several variables at once. We can separate it\ninto gender and age group.\n\n\nwho_separate <- who_longer %>% \n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\nwho_separate\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <dbl>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\nWhile we have performed each step one at a time, it’s possible to chain them\ninto a single block of code. This is good practice, because it avoids having to\ndefine intermediate variables that are only ever used once. This is also\ntypically more concise.\n\n\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65, \n    names_to = \"key\", \n    values_to = \"cases\", \n    values_drop_na = TRUE # if a cell is empty, we do not keep it in the tidy version\n  ) %>%\n  mutate(key = str_replace(key, \"newrel\", \"new_rel\")) %>%\n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>%\n  separate(sexage, c(\"sex\", \"age\"), sep = 1)\n\n# A tibble: 76,046 × 9\n   country     iso2  iso3   year new   type  sex   age   cases\n   <chr>       <chr> <chr> <dbl> <chr> <chr> <chr> <chr> <dbl>\n 1 Afghanistan AF    AFG    1997 new   sp    m     014       0\n 2 Afghanistan AF    AFG    1997 new   sp    m     1524     10\n 3 Afghanistan AF    AFG    1997 new   sp    m     2534      6\n 4 Afghanistan AF    AFG    1997 new   sp    m     3544      3\n 5 Afghanistan AF    AFG    1997 new   sp    m     4554      5\n 6 Afghanistan AF    AFG    1997 new   sp    m     5564      2\n 7 Afghanistan AF    AFG    1997 new   sp    m     65        0\n 8 Afghanistan AF    AFG    1997 new   sp    f     014       5\n 9 Afghanistan AF    AFG    1997 new   sp    f     1524     38\n10 Afghanistan AF    AFG    1997 new   sp    f     2534     36\n# ℹ 76,036 more rows\n\nA recommendation for visualization in javascript. We have only discussed tidying in R. While there is work to implement tidy-style transformations in javascript, the R tidyverse provides a more mature suite of tools. If you are making an interactive visualization in javascript, I recommend first tidying data in R so that each row corresponds to a visual mark and each column to a visual property. You can always save the result as either a json or csv, which can serve as the source data for your javascript visualization.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-14T17:37:32+02:00",
    "input_file": "2024-12-27-week02-04.knit.md"
  },
  {
    "path": "posts/",
    "title": "Ridge Plots",
    "description": "An extended example of faceting with data summaries.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nRidge plots are a special case of small multiples that are particularly suited\nto displaying multiple parallel time series or densities.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggridges)\ntheme_set(theme_bw())\n\n\nAn example ridge plot for the gapminder dataset is shown below. The\neffectiveness of this plot comes from the fact that multiple densities can be\ndisplayed in close proximity to one another, making it possible to facet across\nmany variables in a space-efficient way.\n\n\ndata(gapminder)\ngapminder <- gapminder %>%\n  mutate(\n    dollars_per_day = (gdp / population) / 365,\n    group = case_when(\n      region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \"Northern America\",  \"Australia and New Zealand\") ~ \"West\",\n      region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n      region %in% c(\"Caribbean\", \"Central America\",  \"South America\") ~ \"Latin America\",\n      continent == \"Africa\" &  region != \"Northern Africa\" ~ \"Sub-Saharan\", \n      TRUE ~ \"Others\"\n    ),\n    group = factor(group, levels = c(\"Others\", \"Latin America\",  \"East Asia\", \"Sub-Saharan\", \"West\")),\n    west = ifelse(group == \"West\", \"West\", \"Developing\")\n  )\n\n\n\n\npast_year <- 1970\ngapminder_subset <- gapminder %>%\n  filter(year == past_year, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +  \n  geom_density_ridges() +\n  scale_x_log10()\n\n\n\nYou might wonder, why not plot the raw data? We do this using a geom_point\nbelow. The result isn’t as satisfying, because, while the shape of the ridges\npopped out immediately in the original plot, we have to invest a bit more effort\nto understand the regions of higher density in the raw data plot. Also, when\nthere are many samples, the entire range might appeared covered in marks when in\nfact there are some intervals with higher density than others.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_point(shape = \"|\", size = 5) +\n  scale_x_log10()\n\n\n\nA nice compromise is to include both the raw positions and the smoothed\ndensities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_density_ridges(\n    jittered_points = TRUE, \n    position = position_points_jitter(height = 0),\n    point_shape = '|', point_size = 3\n  ) +\n  scale_x_log10()\n\n\n\nTo exercise our knowledge of both faceting and ridge plots, let’s study a\nparticular question in depth: How did the gap between rich and poor countries\nchange between 1970 and 2010?\nA first reasonable plot is to facet year and development status. While the rich\ncountries have become slightly richer, there is a larger shift in the incomes\nfor the poorer countries.\n\n\npast_year <- 1970\npresent_year <- 2010\nyears <- c(past_year, present_year)\n\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day)) +\n  geom_histogram(binwidth = 0.25) +\n  scale_x_log10() +\n  facet_grid(year ~ west)\n\n\n\nWe can express this message more compactly by overlaying densities, but the\nattempt below is misleading, because it gives the same total area to both groups\nof countries. This doesn’t make sense, considering there are more than 4 times\nas many developing vs. western countries (87 vs. 21).\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\nWe can adjust the areas of the curves by directly referencing the ..count\nvariable, which is implicitly computed by ggplot2 while it’s computing these\ndensities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = ..count.., fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\nCan we attribute the changes to specific regions? Let’s apply our knowledge of\nridge plots.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = group, fill = as.factor(year))) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_log10()\n\n\n\nAlternatively, we can overlay densities from the different regions. Both plots\nsuggest that much of the increase in incomes can be attributed to countries in\nLatin America and East Asia.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = group, y = ..count..)) +\n  geom_density(alpha = 0.7, bw = .12, position = \"stack\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_log10(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week03-02_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:37:52+02:00",
    "input_file": "2024-12-27-week03-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week03-02/",
    "title": "Ridge Plots",
    "description": "An extended example of faceting with data summaries.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-14",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nRidge plots are a special case of small multiples that are particularly suited\nto displaying multiple parallel time series or densities.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\nlibrary(ggridges)\ntheme_set(theme_bw())\n\n\nAn example ridge plot for the gapminder dataset is shown below. The\neffectiveness of this plot comes from the fact that multiple densities can be\ndisplayed in close proximity to one another, making it possible to facet across\nmany variables in a space-efficient way.\n\n\ndata(gapminder)\ngapminder <- gapminder %>%\n  mutate(\n    dollars_per_day = (gdp / population) / 365,\n    group = case_when(\n      region %in% c(\"Western Europe\", \"Northern Europe\",\"Southern Europe\",  \"Northern America\",  \"Australia and New Zealand\") ~ \"West\",\n      region %in% c(\"Eastern Asia\", \"South-Eastern Asia\") ~ \"East Asia\",\n      region %in% c(\"Caribbean\", \"Central America\",  \"South America\") ~ \"Latin America\",\n      continent == \"Africa\" &  region != \"Northern Africa\" ~ \"Sub-Saharan\", \n      TRUE ~ \"Others\"\n    ),\n    group = factor(group, levels = c(\"Others\", \"Latin America\",  \"East Asia\", \"Sub-Saharan\", \"West\")),\n    west = ifelse(group == \"West\", \"West\", \"Developing\")\n  )\n\n\n\n\npast_year <- 1970\ngapminder_subset <- gapminder %>%\n  filter(year == past_year, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +  \n  geom_density_ridges() +\n  scale_x_log10()\n\n\n\nYou might wonder, why not plot the raw data? We do this using a geom_point\nbelow. The result isn’t as satisfying, because, while the shape of the ridges\npopped out immediately in the original plot, we have to invest a bit more effort\nto understand the regions of higher density in the raw data plot. Also, when\nthere are many samples, the entire range might appeared covered in marks when in\nfact there are some intervals with higher density than others.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_point(shape = \"|\", size = 5) +\n  scale_x_log10()\n\n\n\nA nice compromise is to include both the raw positions and the smoothed\ndensities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, group)) +\n  geom_density_ridges(\n    jittered_points = TRUE, \n    position = position_points_jitter(height = 0),\n    point_shape = '|', point_size = 3\n  ) +\n  scale_x_log10()\n\n\n\nTo exercise our knowledge of both faceting and ridge plots, let’s study a\nparticular question in depth: How did the gap between rich and poor countries\nchange between 1970 and 2010?\nA first reasonable plot is to facet year and development status. While the rich\ncountries have become slightly richer, there is a larger shift in the incomes\nfor the poorer countries.\n\n\npast_year <- 1970\npresent_year <- 2010\nyears <- c(past_year, present_year)\n\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, !is.na(dollars_per_day))\n\nggplot(gapminder_subset, aes(dollars_per_day)) +\n  geom_histogram(binwidth = 0.25) +\n  scale_x_log10() +\n  facet_grid(year ~ west)\n\n\n\nWe can express this message more compactly by overlaying densities, but the\nattempt below is misleading, because it gives the same total area to both groups\nof countries. This doesn’t make sense, considering there are more than 4 times\nas many developing vs. western countries (87 vs. 21).\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\nWe can adjust the areas of the curves by directly referencing the ..count\nvariable, which is implicitly computed by ggplot2 while it’s computing these\ndensities.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = ..count.., fill = west)) +\n  geom_density(alpha = 0.8, bw = 0.12) +\n  scale_x_log10() +\n  scale_fill_brewer(palette = \"Set2\") +\n  facet_grid(year ~ .)\n\n\n\nCan we attribute the changes to specific regions? Let’s apply our knowledge of\nridge plots.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, y = group, fill = as.factor(year))) +\n  geom_density_ridges(alpha = 0.7) +\n  scale_fill_brewer(palette = \"Set1\") +\n  scale_x_log10()\n\n\n\nAlternatively, we can overlay densities from the different regions. Both plots\nsuggest that much of the increase in incomes can be attributed to countries in\nLatin America and East Asia.\n\n\nggplot(gapminder_subset, aes(dollars_per_day, fill = group, y = ..count..)) +\n  geom_density(alpha = 0.7, bw = .12, position = \"stack\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_x_log10(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0, 0.15, 0)) +\n  facet_grid(year ~ .)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week03-02/2024-12-27-week03-02_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:37:56+02:00",
    "input_file": "2024-12-27-week03-02.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week09-02/",
    "title": "Hierarchical Clustering",
    "description": "Clustering data at multiple scales using trees.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggraph)\nlibrary(knitr)\nlibrary(tidygraph)\ntheme_set(theme_graph())\n\n\nIn reality, data are rarely separated into a clear number of homogeneous\nclusters. More often, even once a cluster formed, it’s possible to identify a\nfew subclusters. For example, if you initially clustered movies into “drama” and\n“scifi”, you might be able to further refine the scifi cluster into “time\ntravel” and “aliens.”\n\\(K\\)-means only allows clustering at a single level of magnification. To\ninstead simultaneously cluster across scales, you can use an approach called\nhierarchical clustering. As a first observation, note that a tree can be used to\nimplicitly store many clusterings at once. You can get a standard clustering by\ncutting the tree at some level.\n\n\n\nFigure 1: We can recover clusters at different levels of granularity, by cutting a hierarchical clustering tree.\n\n\n\nThese hierarchical clustering trees can be thought of abstract versions of\nthe taxonomic trees. Instead of relating species, they relate observations in a\ndataset.\n\n\nElaborating on this analogy, the leaves of a hierarchical clustering tree are\nthe original observations. The more recently two nodes share a common\nancestor, the more similar those observations are.\nThe specific algorithm proceeds as follows,\nInitialize: Associate each point with a cluster \\(C_i := \\{x_i\\}\\).\nIterate until only one cluster: Look at all pairs of clusters. Merge the pair\n\\(C_k, C_{k^{\\prime}}\\) which are the most similar.\n\n\n\n\nFigure 2: At initialization, the hierarchical clustering routine has a cluster for each observation.\n\n\n\n\n\n\nFigure 3: Next, the two closest observations are merged into one cluster. This is the first merge point on the tree.\n\n\n\n\n\n\nFigure 4: We continue this at the next iteration, though this time we have compute the pairwise distance between all clusters, not observations (technically, all the observations were their own cluster at the first step, and in both cases, we compare the pairwise distances between clusters).\n\n\n\n\n\n\nFigure 5: We can continue this process…\n\n\n\n\n\n\nFigure 6: … and eventually we will construct the entire tree.\n\n\n\n\n\n\nIn R, this can be accomplished by using the hclust function. First, we\ncompute the distances between all pairs of observations (this provides the\nsimilarities used in the algorithm). Then, we apply hclust to the matrix of\npairwise distances.\nWe apply this to a movie ratings dataset. Movies are considered similar if\nthey tend to receive similar ratings across all audience members. The result is\nvisualized below.\n\n\nmovies_mat <- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\")\n\nD <- movies_mat %>%\n  column_to_rownames(var = \"title\") %>%\n  dist()\n\nhclust_result <- hclust(D)\nplot(hclust_result, cex = 0.5)\n\n\n\nWe can customize our tree visualization using the ggraph package. We can\nconvert the hclust object into a ggraph, using the same as_tbl_graph function\nfrom the network and trees lectures.\n\n\nhclust_graph <- as_tbl_graph(hclust_result, height = height)\nhclust_graph <- hclust_graph %>%\n  mutate(height = ifelse(height == 0, 27, height)) # shorten the final edge\nhclust_graph\n\n# A tbl_graph: 99 nodes and 98 edges\n#\n# A rooted tree\n#\n# Node Data: 99 × 4 (active)\n   height leaf  label                                members\n    <dbl> <lgl> <chr>                                  <int>\n 1   27   TRUE  \"Schindler's List\"                         1\n 2   27   TRUE  \"Forrest Gump\"                             1\n 3   27   TRUE  \"Shawshank Redemption, The\"                1\n 4   27   TRUE  \"Pulp Fiction\"                             1\n 5   27   TRUE  \"Silence of the Lambs, The\"                1\n 6   58.7 FALSE \"\"                                         2\n 7   63.9 FALSE \"\"                                         3\n 8   64.9 FALSE \"\"                                         4\n 9   67.9 FALSE \"\"                                         5\n10   27   TRUE  \"Star Wars: Episode IV - A New Hope\"       1\n# ℹ 89 more rows\n#\n# Edge Data: 98 × 2\n   from    to\n  <int> <int>\n1     6     4\n2     6     5\n3     7     3\n# ℹ 95 more rows\n\n\n\nggraph(hclust_graph, \"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label), size = 4) +\n  coord_fixed()\n\n\n\nWe can cut the tree to recover a standard clustering. This is where the\ngrammar-of-graphics approach from ggraph becomes useful – we can encode the\ncluster membership of a movie using color, for example.\n\n\ncluster_df <- cutree(hclust_result, k = 10) %>% # try changing K and regenerating the graph below\n  tibble(label = names(.), cluster = as.factor(.))\ncluster_df\n\n# A tibble: 50 × 3\n       . label                cluster\n   <int> <chr>                <fct>  \n 1     1 Seven (a.k.a. Se7en) 1      \n 2     1 Usual Suspects, The  1      \n 3     2 Braveheart           2      \n 4     2 Apollo 13            2      \n 5     3 Pulp Fiction         3      \n 6     4 Forrest Gump         4      \n 7     2 Lion King, The       2      \n 8     2 Mask, The            2      \n 9     2 Speed                2      \n10     2 Fugitive, The        2      \n# ℹ 40 more rows\n\n\n\n# colors chosen using https://medialab.github.io/iwanthue/\ncols <- c(\"#51b48c\", \"#cf3d6e\", \"#7ab743\", \"#7b62cb\", \"#c49644\", \"#c364b9\", \"#6a803a\", \"#688dcd\", \"#c95a38\", \"#c26b7e\")\nhclust_graph %>%\n  left_join(cluster_df) %>%\n  ggraph(\"dendrogram\", height = height, circular = TRUE) +\n  geom_edge_elbow() +\n  geom_node_text(aes(label = label, col = cluster), size = 4) +\n  coord_fixed() +\n  scale_color_manual(values = cols) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week09-02/figure/hclust_cutting.png",
    "last_modified": "2025-01-13T17:23:00+02:00",
    "input_file": {},
    "preview_width": 762,
    "preview_height": 377
  },
  {
    "path": "posts/2024-12-27-week09-03/",
    "title": "Heatmaps",
    "description": "Visualizing table values, ordered by clustering results.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"readr\")\nlibrary(\"superheat\")\nlibrary(\"tibble\")\ntheme_set(theme_minimal())\n\n\nThe direct outputs of a standard clustering algorithim are (a) cluster\nassignments for each sample, (b) the centroids associated with each cluster. A\nhierarchical clustering algorithm enriches this output with a tree, which\nprovide (a) and (b) at multiple levels of resolution.\nThese outputs can be used to improve visualizations. For example, they can be\nused to define small multiples, faceting across clusters. One especially common\nidea is to reorder the rows of a heatmap using the results of a clustering, and\nthis is the subject of these notes.\nIn a heatmap, each mark (usually a small tile) corresponds to an entry of a\nmatrix. The \\(x\\)-coordinate of the mark encodes the index of the observation,\nwhile the \\(y\\)-coordinate encodes the index of the feature. The color of each\ntile represents the value of that entry. For example, here are the first few\nrows of the movies data, along with the corresponding heatmap, made using the\nsuperheat package.\n\n\nmovies_mat <- read_csv(\"https://uwmadison.box.com/shared/static/wj1ln9xtigaoubbxow86y2gqmqcsu2jk.csv\") %>%\n  column_to_rownames(var = \"title\")\n\n\n\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(movies_mat, left.label.text.size = 4, heat.pal = cols, heat.lim = c(0, 5))\n\n\n\nJust like in adjacency matrix visualizations, the effectiveness of a heatmap\ncan depend dramatically on the way in which rows and columns are ordered. To\nprovide a more coherent view, we cluster both rows and columns, placing rows /\ncolumns belonging to the same cluster next to one another.\n\n\nmovies_clust <- movies_mat %>%\n  kmeans(centers = 10)\n\nusers_clust <- movies_mat %>%\n  t() %>%\n  kmeans(centers = 10)\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\nsuperheat also makes it easy to visualize plot statistics adjacent ot the\nadjacent to the main heatmap. These statistics can be plotted as points, lines,\nor bars. Points are useful when we want to highlight the raw value, lines are\neffective for showing change, and bars give a sense of the area below a set of\nobservations. In this example, we use an added panel on the right hand side\n(yr) to encode the total number of ratings given to that movie. The\nyr.obs.cols allows us to change the color of each point in the adjacent plot.\nIn this example, we change color depending on which cluster the movie was found\nto belong to.\n\n\ncluster_cols <- c('#8dd3c7','#ccebc5','#bebada','#fb8072','#80b1d3','#fdb462','#b3de69','#fccde5','#d9d9d9','#bc80bd')\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  order.rows = order(movies_clust$cluster),\n  order.cols = order(users_clust$cluster),\n  heat.pal = cols,\n  heat.lim = c(0, 5),\n  yr = rowSums(movies_mat > 0),\n  yr.axis.name = \"Number of Ratings\",\n  yr.obs.col = cluster_cols[movies_clust$cluster],\n  yr.plot.type = \"bar\"\n)\n\n\n\nIt also makes sense to order the rows / columns using hierarchical\nclustering. This approach is especially useful when the samples fall along a\ncontinuous gradient, rather than belonging to clearly delineated groups. The\npretty.order.rows and pretty.order.cols arguments use hierarchical\nclustering to reorder the heatmap.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\nThe hierarchical clustering trees estimated by pretty.order.rows and\npretty.order.cols can be also visualized.\n\n\nsuperheat(\n  movies_mat, \n  left.label.text.size = 4, \n  pretty.order.cols = TRUE,  \n  pretty.order.rows = TRUE, \n  row.dendrogram = TRUE,\n  col.dendrogram = TRUE,\n  heat.pal = cols,\n  heat.lim = c(0, 5)\n)\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week09-03/2024-12-27-week09-03_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-13T17:23:10+02:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1728
  },
  {
    "path": "posts/2024-12-27-week09-04/",
    "title": "Silhouette Statistics",
    "description": "Diagnostics for the quality of a clustering.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"cluster\")\nlibrary(\"stringr\")\nlibrary(\"dplyr\")\nlibrary(\"tidymodels\")\nlibrary(\"readr\")\nlibrary(\"ggplot2\")\ntheme_set(theme_bw())\nset.seed(123)\n\n\nClustering algorithms usually require the number of clusters \\(K\\) as an\nargument. How should it be chosen?\nThere are many possible criteria, but one common approach is to compute the\nsilhouette statistic. It is a statistic that can be computed for each\nobservation in a dataset, measuring how strongly it is tied to its assigned\ncluster. If a whole cluster has large silhouette statistics, then that cluster\nis well-defined and clearly isolated other clusters.\nThe plots below illustrate the computation of silhouette statistics for a\nclustering of the penguins dataset that used \\(K = 3\\). To set up, we first need\nto cluster the penguins dataset. The idea is the same as in the \\(K\\)-means notes,\nbut we encapsulate the code in a function, so that we can easily extract data\nfor different values of \\(K\\).\n\n\npenguins <- read_csv(\"https://uwmadison.box.com/shared/static/ijh7iipc9ect1jf0z8qa2n3j7dgem1gh.csv\") %>%\n  na.omit() %>%\n  mutate(id = row_number())\n\ncluster_penguins <- function(penguins, K) {\n  x <- penguins %>%\n    select(matches(\"length|depth|mass\")) %>%\n    scale()\n    \n  kmeans(x, center = K) %>%\n    augment(penguins) %>% # creates column \".cluster\" with cluster label\n    mutate(silhouette = silhouette(as.integer(.cluster), dist(x))[, \"sil_width\"])\n}\n\n\nDenote the silhouette statistic of observation \\(i\\) by \\(s_{i}\\). We will\ncompute \\(s_i\\) for the observation with the black highlight below1.\n\n\ncur_id <- 2\npenguins3 <- cluster_penguins(penguins, K = 3)\nobs_i <- penguins3 %>%\n  filter(id == cur_id)\n\n\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\nFigure 1: The observation on which we will compute the silhouette statistic.\n\n\n\nThe first step in the calculation of the silhouette statistic is to measure\nthe pairwise distances between the observation \\(i\\) and all observations in the\nsame cluster. These distances are the lengths of the small lines below. Call\naverage of these lengths \\(a_{i}\\).\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster == obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm),\n    size = 0.6, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", a[i])))\n\n\n\nFigure 2: The average distance between the target observation and all others in the same cluster.\n\n\n\nNext, we compute pairwise distances to all observations in clusters 2 and 3.\nThe average of these pairwise distances are called \\(b_{i2}\\) and \\(b_{i3}\\). Choose\nthe smaller of \\(b_{i2}\\) and \\(b_{i3}\\), and call it \\(b_{i}\\). In a sense, this is\nthe “next best” cluster to put observation \\(i\\). For a general \\(K\\), you would\ncompute \\(b_{ik}\\) for all \\(k\\) (other than observation \\(i\\)’s cluster) and take the\nminimum across all of them. In this case, the orange segments are on average\nsmaller than the blue segments, so \\(b_i\\) is defined as the average length of the\norange segments.\n\n\nggplot(penguins3, aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster)) +\n  geom_segment(\n    data = penguins3 %>% filter(.cluster != obs_i$.cluster), \n    aes(xend = obs_i$bill_length_mm, yend = obs_i$bill_depth_mm, col = .cluster),\n    size = 0.5, alpha = 0.3\n  ) +\n  geom_point(data = obs_i, size = 5, col = \"black\") + \n  geom_point() +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1)) +\n  labs(title = expression(paste(\"Distances used for \", b[i][1], \" and \", b[i][2])))\n\n\n\nFigure 3: The average distance between the target observation and all others in different clusters.\n\n\n\nThe silhouette statistic for observation \\(i\\) is derived from the relative\nlengths of the orange vs. green segments. Formally, the silhouette statistic for\nobservation \\(i\\) is \\(s_{i}:= \\frac{b_{i} - a_{i}}{\\max\\left({a_{i},\nb_{i}}\\right)}\\). This number is close to 1 if the orange segments are much\nlonger than the green segments, close to 0 if the segments are about the same\nsize, and close to -1 if the the orange segments are much shorter than the green\nsegments2.\nThe median of these \\(s_{i}\\) for all observations within cluster \\(k\\) is a\nmeasure of how well-defined cluster \\(k\\) is overall. The higher this number, the\nmore well-defined the cluster.\nDenote the median of the silhouette statistics within cluster \\(k\\) by\n\\(SS_{k}\\). A measure how good a choice of \\(K\\) is can be determined by the median\nof these medians: \\(\\text{Quality}(K) := \\text{median}_{k = 1 \\dots, K} SS_{k}\\).\nIn particular, this can be used to define (a) a good cut point in a\nhierarchical clustering or (b) a point at which a cluster should no longer be\nsplit into subgroups.\nIn R, we can use the silhouette function from the cluster package to\ncompute the silhouette statistic. The syntax is silhouette(cluster_labels, pairwise_distances) where cluster_labels is a vector of (integer) cluster\nID’s for each observation and pairwise_distances gives the lengths of the\nsegments between all pairs of observations. An example of this function’s usage\nis given in the function at the start of the illustration.\nThis is what the silhouette statistic looks like in the penguins dataset\nwhen we choose 3 clusters. The larger points have lower silhouette statistics.\nThis points between clusters 2 and 3 have large silhouette statistics because\nthose two clusters blend into one another.\n\n\nggplot(penguins3) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\nFigure 4: The silhouette statistics on the Palmers Penguins dataset, when using \\(K\\)-means with \\(K = 3\\).\n\n\n\nWe can also visualize the histogram of silhouette statistics within each\ncluster. Since the silhouette statistics for cluster 2 are generally lower than\nthose for the other two clusters (in particular, its median is lower), we can\nconclude that it is less well-defined.\n\n\nggplot(penguins3) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\nFigure 5: The per-cluster histograms of silhouette statistics summarize how well-defined each cluster is.\n\n\n\nIf we choose even more clusters, then there are more points lying along the\nboundaries of poorly defined clusters. Their associated silhouette statistics\nend up becoming larger. From the histogram, we can also see a deterioration in\nthe median silhouette scores across all clusters.\n\n\npenguins4 <- cluster_penguins(penguins, K = 4)\nggplot(penguins4) +\n  geom_point(aes(x = bill_length_mm, y = bill_depth_mm, col = .cluster, size = silhouette)) +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_size(range = c(4, 1))\n\n\n\nFigure 6: We can repeat the same exercise, but with \\(K = 4\\) clusters instead.\n\n\n\n\n\nggplot(penguins4) +\n  geom_histogram(aes(x = silhouette), binwidth = 0.05) +\n  facet_grid(~ .cluster)\n\n\n\n\nYou can change\ncur_id to try different observations.↩︎\nThis last case likely indicates a misclustering.↩︎\n",
    "preview": "posts/2024-12-27-week09-04/2024-12-27-week09-04_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-01-13T17:23:15+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week09-05/",
    "title": "Cluster Stability",
    "description": "How reliable are the results of a clustering?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"MASS\")\nlibrary(\"Matrix\")\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"pdist\")\nlibrary(\"superheat\")\nlibrary(\"tidyr\")\ntheme_set(theme_minimal())\nset.seed(1234)\n\n\nOne of the fundamental principles in statistics is that, no matter how the\nexperiment / study was conducted, if we ran it again, we would get different\nresults. More formally, sampling variability creates uncertainty in our\ninferences.\nHow should we think about sampling variability in the context of clustering?\nThis is a tricky problem, because you can permute the labels of the clusters\nwithout changing the meaning of the clustering. However, it is possible to\nmeasure and visualize the stability of a point’s cluster assignment.\nTo make this less abstract, consider an example. A study has found a\ncollection of genes that are differentially expressed between patients with two\ndifferent subtypes of a disease. There is an interest in clustering genes that\nhave similar expression profiles across all patients — these genes probably\nbelong to similar biological processes.\nOnce you run the clustering, how sure can you be that, if the study would run\nagain, you would recover a similar clustering? Are there some genes that you are\nsure belong to a particular cluster? Are there some that lie between two\nclusters?\nTo illustrate, consider the simulated dataset below. Imagine that the rows\nare patients, the column are genes, and the colors are the expression levels of\ngenes within patients. There are 5 clusters of genes here (columns 1 - 20 are\ncluster 1, 21 - 41 are cluster 2, …). The first two clusters are only weakly\nvisible, while the last three stand out strongly.\n\n\nn_per <- 20\np <- n_per * 5\nSigma1 <- diag(2) %x% matrix(rep(0.3, n_per ** 2), nrow = n_per)\nSigma2 <- diag(3) %x% matrix(rep(0.6, n_per ** 2), nrow = n_per)\nSigma <- bdiag(Sigma1, Sigma2)\ndiag(Sigma) <- 1\nmu <- rep(0, 100)\nx <- mvrnorm(25, mu, Sigma)\n\ncols <- c('#f6eff7','#bdc9e1','#67a9cf','#1c9099','#016c59')\nsuperheat(\n  x, \n  pretty.order.rows = TRUE, \n  bottom.label = \"none\", \n  heat.pal = cols,\n  left.label.text.size = 3,\n  legend = FALSE\n)\n\n\n\nFigure 1: A simulated clustering of genes (columns) across rows (patients).\n\n\n\nThe main idea for how to compute cluster stability is to bootstrap (i.e.,\nrandomly resample) the patients and see whether the cluster assignments for each\ngene change. More precisely, we use the following strategy,\nUsing all the patients, \\(X\\), estimate the cluster centroids \\(c_{1}, \\dots,\nc_{K}\\).\nFor \\(B\\) bootstrap iterations, perform the following.\nSample the patients with replacement, generating a bootstrap resampled version\nof the dataset \\(X_{b}^{\\ast}\\).\nPermute the original cluster centroids to reflect the order of patients in\n\\(X_{b}^{\\ast}\\). Call the permuted centroids \\(c_{1b}^{\\ast}, \\dots,\nc_{Kb}^{\\ast}\\).\nAssign genes in \\(X_{b}^{\\ast}\\) to the cluster \\(k\\) of the closest\n\\(c_{bk}^{\\ast}\\).\n\nWe quantify our certainty that gene \\(j\\) belongs to cluster \\(k\\) by counting the\nnumber of times that gene \\(j\\) was assigned to cluster \\(k\\).\nThe picture below describes the bootstrapping process for a gene. The two\nrows correspond to the original and bootstrapped representations a specific\ngene, respectively. Each bar gives the expression level of the gene for one\nindividual. Due to the random sampling in the bootstrapped dataset, some\nindividuals become overrepresented and some are removed. If we also permute the\ncentroids in the same way, we get a new distance between genes and their\ncentroids. Since the patients who are included changes, the distances between\neach gene and each centroid changes, so the genes might be assigned to different\nclusters.\n\n\n\n\n\nK <- 5\nB <- 1000\ncluster_profiles <- kmeans(t(x), centers = K)$centers\ncluster_probs <- matrix(nrow = ncol(x), ncol = B)\n\nfor (b in seq_len(B)) {\n  b_ix <- sample(nrow(x), replace = TRUE)\n  dists <- as.matrix(pdist(t(x[b_ix, ]), cluster_profiles[, b_ix]))\n  cluster_probs[, b] <- apply(dists, 1, which.min)\n}\n\ncluster_probs <- as_tibble(cluster_probs) %>%\n  mutate(gene = row_number()) %>%\n  pivot_longer(-gene, names_to = \"b\", values_to = \"cluster\")\n\n\nThe table below shows the result of this procedure. In each bootstrap\niteration, gene 1 was assigned to cluster 4, so we can rely on that assignment.\nOn the other hand, gene 3 is assigned to cluster 4 75% of the time, but\noccasionally appears in clusters 1, 2, and 5.\n\n\ncluster_probs <- cluster_probs %>%\n  mutate(cluster = as.factor(cluster)) %>%\n  group_by(gene, cluster) %>%\n  summarise(prob = n() / B)\n\ncluster_probs\n\n# A tibble: 277 × 3\n# Groups:   gene [100]\n    gene cluster  prob\n   <int> <fct>   <dbl>\n 1     1 1       0.334\n 2     1 2       0.006\n 3     1 3       0.001\n 4     1 4       0.659\n 5     2 1       0.981\n 6     2 3       0.002\n 7     2 4       0.017\n 8     3 1       0.754\n 9     3 2       0.008\n10     3 3       0.112\n# ℹ 267 more rows\n\nThese fractions for all genes are summarized by the plot below. Each row is a\ngene. The length of each color gives the number of times that gene was assigned\nto that cluster. The genes from rows 41 - 100 are all clearly distinguished,\nwhich is in line with what we saw visually in the heatmap above. The first two\nclusters are somewhat recovered, but since they were often assigned to\nalternative clusters, we can conclude that they were harder to demarcate out\nthan the others.\n\n\nggplot(cluster_probs) +\n  geom_bar(aes(y = as.factor(gene), x = prob, col = cluster, fill = cluster), stat = \"identity\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  labs(y = \"Gene\", x = \"Proportion\") +\n  theme(\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_text(size = 7),\n    legend.position = \"bottom\"\n  )\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week09-05/2024-12-27-week09-05_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-13T17:23:17+02:00",
    "input_file": {},
    "preview_width": 576,
    "preview_height": 960
  },
  {
    "path": "posts/2024-12-27-week10-1/",
    "title": "Introduction to Dimensionality Reduction",
    "description": "Examples of high-dimensional data.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nHigh-dimensional data are data where many features are collected for each\nobservation. These tend to be wide datasets with many columns. The name comes\nfrom the fact that each row of the dataset can be viewed as a vector in a\nhigh-dimensional space (one dimension for each feature). These data are common\nin modern applications,\nEach cell in a genomics dataset might have measurements for hundreds of molecules.\nEach survey respondent might provide answers to dozens of questions.\nEach image might have several thousand pixels.\nEach document might have counts across several thousand relevant words.\nFor low-dimensional data, we could visually encode all the features in our\ndata directly, either using properties of marks or through faceting. In\nhigh-dimensional data, this is no longer possible.\nHowever, though there are many features associated with each observation, it\nmay still be possible to organize samples across a smaller number of meaningful,\nderived features.\nFor example, consider the Metropolitan Museum of Art dataset, which contains\nimages of many artworks. Abstractly, each artwork is a high-dimensional object,\ncontaining pixel intensities across many pixels. But it is reasonable to derive\na feature based on the average brightness.\n\n\n\nFigure 1: An arrangement of artworks according to their average pixel brightness, as given in the reading.\n\n\n\nIn general, manual feature construction can be difficult. Algorithmic\napproaches try streamline the process of generating these maps by optimizing\nsome more generic criterion. Different algorithms use different criteria, which\nwe will review in the next couple of lectures.\n\n\n\nFigure 2: The dimensionality reduction algorithm in this animation converts a large number of raw features into a position on a one-dimensional axis defined by average pixel brightness. In general, we might reduce to dimensions other than 1D, and we will often want to define features tailored to the dataset at hand.\n\n\n\nInformally, the goal of dimensionality reduction techniques is to produce a\nlow-dimensional “atlas” relating members of a collection of complex objects.\nSamples that are similar to one another in the high-dimensional space should be\nplaced near one another in the low-dimensional view. For example, we might want\nto make an atlas of artworks, with similar styles and historical periods being\nplaced near to one another.\n\n\n\n",
    "preview": "https://github.com/krisrs1128/stat436_s24/blob/main/exercises/figure/dimred-animation.gif?raw=true",
    "last_modified": "2025-01-13T20:25:11+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-2/",
    "title": "Principal Components Analysis I",
    "description": "Linear dimensionality reduction using PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(tidymodels)\nlibrary(readr)\n\n\n\n\nIn our last notes, we saw how we could organize a collection of images\nbased on average pixel brightness. We can think of average pixel\nbrightness as a derived feature that can be used to build a\nlow-dimensional map.\n\n\n\n\nWe can partially automate the process of deriving new features. Though,\nin general, finding the best way to combine raw features into derived\nones is a complicated problem, we can simplify things by restricting\nattention to,\n\n\n\nFeatures that are linear combinations of the raw input columns.\n\n\nFeatures that are orthogonal to one another.\n\n\nFeatures that have high variance.\n\n\n\nRestricting to linear combinations allows for an analytical\nsolution. We will relax this requirement when discussing UMAP.\n\n\n\n\nOrthogonality means that the derived features will be\nuncorrelated with one another. This is a nice property, because it would\nbe wasteful if features were redundant.\n\n\n\n\nHigh variance is desirable because it means we preserve more of\nthe essential structure of the underlying data. For example, if you look\nat this 2D representation of a 3D object, it’s hard to tell what it is,\n\n\n\n\n\n\nFigure 1: What is this object?\n\n\n\n\nBut when viewing an alternative reduction which has higher variance…\n\n\n\n\n\nFigure 2: Not so complicated now. Credit for this example goes to\nProfessor Julie Josse, at Ecole Polytechnique.\n\n\n\n\n\nPrincipal Components Analysis (PCA) is the optimal dimensionality\nreduction under these three restrictions, in the sense that it finds\nderived features with the highest variance. Formally, PCA finds a matrix\n\\(\\Phi \\in\n\\mathbb{R}^{D\n\\times K}\\) and a set of vector \\(z_{i} \\in\n\\mathbb{R}^{K}\\) such that \\(x_{i}\n\\approx \\Phi z_{i}\\) for all \\(i\\). The\ncolumns of \\(\\Phi\\) are called principal\ncomponents, and they specify the structure of the derived linear\nfeatures. The vector \\(z_{i}\\) is called the score of \\(x_{i}\\)\nwith respect to these components. The top component explains the most\nvariance, the second captures the next most, and so on.\n\n\n\n\nFor example, if one of the columns of \\(\\Phi\\) was equal to \\(\\left(\\frac{1}{D},\n\\dots, \\frac{1}{D}\\right)\\), then that feature computes\nthe average of all coordinates (e.g., to get average brightness), and\nthe corresponding \\(z_{i}\\) would be a measure of the\naverage brightness of sample \\(i\\).\n\n\n\n\nGeometrically, the columns of \\(\\Phi\\) span a plane that\napproximates the data. The \\(z_{i}\\) provide coordinates of\npoints projected onto this plane.\n\n\n\n\n\n\nFigure 3: PCA finds a low-dimensional linear subspace that closely\napproximates the high-dimensional data.\n\n\n\n\nIn R, PCA can be conveniently implemented using the tidymodels package.\nWe will see a base R implementation in the next lecture. The\ndataset\nbelow contains properties of a variety of cocktails, from the Boston\nBartender’s guide. The first two columns are qualitative descriptors,\nwhile the rest give numerical ingredient information.\n\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\ncocktails_df[, 1:6]\n\n# A tibble: 937 × 6\n   name       category light_rum lemon_juice lime_juice sweet_vermouth\n   <chr>      <chr>        <dbl>       <dbl>      <dbl>          <dbl>\n 1 Gauguin    Cocktai…      2           1          1               0  \n 2 Fort Laud… Cocktai…      1.5         0          0.25            0.5\n 3 Cuban Coc… Cocktai…      2           0          0.5             0  \n 4 Cool Carl… Cocktai…      0           0          0               0  \n 5 John Coll… Whiskies      0           1          0               0  \n 6 Cherry Rum Cocktai…      1.25        0          0               0  \n 7 Casa Blan… Cocktai…      2           0          1.5             0  \n 8 Caribbean… Cocktai…      0.5         0          0               0  \n 9 Amber Amo… Cordial…      0           0.25       0               0  \n10 The Joe L… Whiskies      0           0.5        0               0  \n# ℹ 927 more rows\n\n\nThe pca_rec object below defines a tidymodels recipe for\nperforming PCA. Computation of the lower-dimensional representation is\ndeferred until prep() is called. This delineation between\nworkflow definition and execution helps clarify the overall workflow,\nand it is typical of the tidymodels package.\n\n\n\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\n\n\nThe step_normalize call is used to center and scale all the\ncolumns. This is needed because otherwise columns with larger variance\nwill have more weight in the final dimensionality reduction, but this is\nnot conceptually meaningful. For example, if one of the columns in a\ndataset were measuring length in kilometers, then we could artificially\nincrease its influence in a PCA by expressing the same value in meters.\nTo achieve invariance to this change in units, it would be important to\nnormalize first.\n\n\n\n\nWe can tidy each element of the workflow object. Since PCA\nwas the second step in the workflow, the PCA components can be obtained\nby calling tidy with the argument “2.” The scores of each sample with\nrespect to these components can be extracted using juice.\nThe amount of variance explained by each dimension is also given by\ntidy, but with the argument type = “variance”.\nWe’ll see how to visualize and interpret these results in the next\nlecture.\n\n\n\n\ntidy(pca_prep, 2)\n\n# A tibble: 1,600 × 4\n   terms             value component id       \n   <chr>             <dbl> <chr>     <chr>    \n 1 light_rum        0.163  PC1       pca_KSuLo\n 2 lemon_juice     -0.0140 PC1       pca_KSuLo\n 3 lime_juice       0.224  PC1       pca_KSuLo\n 4 sweet_vermouth  -0.0661 PC1       pca_KSuLo\n 5 orange_juice     0.0308 PC1       pca_KSuLo\n 6 powdered_sugar  -0.476  PC1       pca_KSuLo\n 7 dark_rum         0.124  PC1       pca_KSuLo\n 8 cranberry_juice  0.0954 PC1       pca_KSuLo\n 9 pineapple_juice  0.119  PC1       pca_KSuLo\n10 bourbon_whiskey  0.0963 PC1       pca_KSuLo\n# ℹ 1,590 more rows\n\n\n\njuice(pca_prep)\n\n# A tibble: 937 × 7\n   name                 category    PC1     PC2     PC3     PC4    PC5\n   <fct>                <fct>     <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 Gauguin              Cocktai…  1.38  -1.15    1.34   -1.12    1.52 \n 2 Fort Lauderdale      Cocktai…  0.684  0.548   0.0308 -0.370   1.41 \n 3 Cuban Cocktail No. 1 Cocktai…  0.285 -0.967   0.454  -0.931   2.02 \n 4 Cool Carlos          Cocktai…  2.19  -0.935  -1.21    2.47    1.80 \n 5 John Collins         Whiskies  1.28  -1.07    0.403  -1.09   -2.21 \n 6 Cherry Rum           Cocktai… -0.757 -0.460   0.909   0.0154 -0.748\n 7 Casa Blanca          Cocktai…  1.53  -0.392   3.29   -3.39    3.87 \n 8 Caribbean Champagne  Cocktai…  0.324  0.137  -0.134  -0.147   0.303\n 9 Amber Amour          Cordial…  1.31  -0.234  -1.55    0.839  -1.19 \n10 The Joe Lewis        Whiskies  0.138 -0.0401 -0.0365 -0.100  -0.531\n# ℹ 927 more rows\n\n\n\ntidy(pca_prep, 2, type = \"variance\")\n\n# A tibble: 160 × 4\n   terms    value component id       \n   <chr>    <dbl>     <int> <chr>    \n 1 variance  2.00         1 pca_KSuLo\n 2 variance  1.71         2 pca_KSuLo\n 3 variance  1.50         3 pca_KSuLo\n 4 variance  1.48         4 pca_KSuLo\n 5 variance  1.37         5 pca_KSuLo\n 6 variance  1.32         6 pca_KSuLo\n 7 variance  1.30         7 pca_KSuLo\n 8 variance  1.20         8 pca_KSuLo\n 9 variance  1.19         9 pca_KSuLo\n10 variance  1.18        10 pca_KSuLo\n# ℹ 150 more rows\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-13T20:25:17+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week10-3/",
    "title": "Principal Components Analysis II",
    "description": "Visualizing and interpreting PCA.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nHow should we visualize the results from PCA? There are three artifacts\nproduced by the procedure worth considering — components, scores, and variances.\nThe components describe derived features, the scores lay samples out on a map,\nand the variances summarize how much information was preserved by each\ndimension.\n\n\n# produced by code in previous notes\ncomponents <- read_csv(\"https://uwmadison.box.com/shared/static/dituepd0751qqsims22v2liukuk0v4bf.csv\") %>%\n  filter(component %in% str_c(\"PC\", 1:5))\nscores <- read_csv(\"https://uwmadison.box.com/shared/static/qisbw1an4lo8naifoxyu4dqv4bfsotcu.csv\")\nvariances <- read_csv(\"https://uwmadison.box.com/shared/static/ye125xf8800zc5eh3rfeyzszagqkaswf.csv\") %>%\n  filter(terms == \"percent variance\")\n\n\nFirst, let’s see how much variance is explained by each dimension of the PCA.\nWithout detouring into the mathematical explanation, the main idea is that, the\nmore rapid the dropoff in variance explained, the better the low-dimensional\napproximation. For example, if the data actually lie on a 2D plane in a\nhigh-dimensional space, then the first two bars would contain all the variance\n(the rest would be zero).\n\n\nggplot(variances) +\n  geom_col(aes(component, value))\n\n\n\nFigure 1: Proportion of variance explained by each component in the PCA.\n\n\n\nWe can interpret components by looking at the linear coefficients of the\nvariables used to define them. From the plot below, we see that the first PC\nmostly captures variation related to whether the drink is made with powdered\nsugar or simple syrup. Drinks with high values of PC1 are usually to be made\nfrom simple syrup, those with low values of PC1 are usually made from powdered\nsugar. From the two largest bars in PC2, we can see that it highlights the\nvermouth vs. non-vermouth distinction.\n\n\nggplot(components, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~component, nrow = 1) +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\nFigure 2: The top 5 principal components associated with the cocktails dataset.\n\n\n\nIt is often easier read the components when the bars are sorted according to\ntheir magnitude. The usual ggplot approach to reordering axes labels, using\neither reorder() or releveling the associated factor, will reorder all the\nfacets in the same way. If we want to reorder each facet on its own, we can use\nthe reorder_within function coupled with scale_*_reordered, both from the\ntidytext package.\n\n\ncomponents_ <- components %>%\n  filter(component %in% str_c(\"PC\", 1:3)) %>%\n  mutate(terms = reorder_within(terms, abs(value), component))\nggplot(components_, aes(value, terms)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  theme(axis.text = element_text(size = 7))\n\n\n\nFigure 3: The top 3 principal components, with defining variables sorted by the magnitude of their coefficient.\n\n\n\nNext, we can visualize the scores of each sample with respect to these\ncomponents. The plot below shows \\(\\left(z_{i1}, z_{i2}\\right)\\). Suppose that the\ncolumns of \\(\\Phi\\) are \\(\\varphi_{1}, \\dots, \\varphi_{K}\\). Then, since \\(x_{i}\n\\approx \\varphi_{1}z_{i1} + \\varphi_{2} z_{i2}\\), the samples have large values\nfor variables with large component values in the coordinate directions where\n\\(z_{i}\\) is farther along.\n\n\nggplot(scores, aes(PC1, PC2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\nFigure 4: The scores associated with the cocktails dataset.\n\n\n\nFor example, El Nino has high value for PC1, which means it has a high value\nof variables that are positive for PC1 (like simple syrup) and low value for\nthose variables that are negative (like powdered sugar). Similarly, since\n\\(\\varphi_{2}\\) puts high positive weight on vermouth-related variables, so H. P.\nW. Cocktail has many vermouth-related ingredients.\nIn practice, it will often be important to visualize several pairs of PC\ndimensions against one another, not just the top 2.\nLet’s examine the original code in a little more detail. We are using\ntidymodels, which is a package for decoupling the definition and execution of a\ndata pipeline. This compartmentalization makes it easier to design and reuse\nacross settings.\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\npca_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_pca(all_predictors())\n\npca_prep <- prep(pca_rec)\n\n\nHere is how you would apply PCA without the tidymodels package. You have to\nfirst split the data into the “metadata” that is used to interpret the scores\nand the numerical variables used as input to PCA. Then at the end, you have to\njoin the metadata back in. It’s not impossible, but the code is not as readable.\n\n\n# split name and category out of the data frame\npca_result <- cocktails_df %>%\n  select(-name, -category) %>%\n  scale() %>%\n  princomp()\n\n# join them back into the PCA result\nmetadata <- cocktails_df %>%\n  select(name, category)\nscores_direct <- cbind(metadata, pca_result$scores)\n\nggplot(scores_direct, aes(Comp.1, Comp.2, label = name)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 1.5) +\n  geom_text_repel(check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) # rescale axes to reflect variance\n\n\n\nFigure 5: A plot of the PCA scores made without using tidymodels.\n\n\n\nThe equivalent tidymodels implementation handles the difference between\nsupplementary and modeling data less bluntly, setting the name and category\nvariables to id roles, so that all_predictors() knows to skip them.\nWe conclude with some characteristics of PCA, which can guide the choice\nbetween alternative dimensionality reduction methods.\nGlobal structure: Since PCA is looking for high-variance overall, it tends\nto focus on global structure.\nLinear: PCA can only consider linear combinations of the original\nfeatures. If we expect nonlinear features to be more meaningful, then another\napproach should be considered.\nInterpretable features: The PCA components exactly specify how to\nconstruct each of the derived features.\nFast: Compared to most dimensionality reduction methods, PCA is quite\nfast. Further, it is easy to implement approximate versions of PCA that scale\nto very large datasets.\nDeterministic: Some embedding algorithms perform an optimization process,\nwhich means there might be some variation in the results due to randomness in\nthe optimization. In contrast, PCA is deterministic, with the components being\nunique up to sign (i.e., you could reflect the components across an axis, but\nthat is the most the results might change).\n\n\n\n",
    "preview": "posts/2024-12-27-week10-3/2024-12-27-week10-3_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2025-01-13T19:02:45+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1152
  },
  {
    "path": "posts/2024-12-27-week10-4/",
    "title": "Uniform Manifold Approximation and Projection",
    "description": "An overview of the UMAP algorithm.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(embed)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nNonlinear dimension reduction methods can give a more faithful representation\nthan PCA when the data don’t lie on a low-dimensional linear subspace.\nFor example, suppose the data were shaped like this. There is no\none-dimensional line through these data that separate the groups well. We will\nneed an alternative approach to reducing dimensionality if we want to preserve\nnonlinear structure.\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 1: An example nonlinear dataset where projections onto any straight line will necessarily cause the classes to bleed together.\n\n\n\nFrom a high-level, the intuition behind UMAP is to (a) build a graph joining\nnearby neighbors in the original high-dimensional space, and then (b) layout the\ngraph in a lower-dimensional space.\nFor example, consider the 2-dimensional sine wave below. If we build a graph,\nwe can try to layout the resulting nodes and edges on a 1-dimensional line in a\nway that approximately preserves the ordering.\n\n\n\nFigure 2: UMAP (and many other nonlinear methods) begins by constructing a graph in the high-dimensional space, whose layout in the lower dimensional space will ideally preserve the essential relationships between samples.\n\n\n\nA natural way to build a graph is to join each node to its \\(K\\) closest\nneighbors. The choice of \\(K\\) will influence the final reduction, and it is\ntreated as a hyperparameter of UMAP.\n\n\n\nFigure 3: When using fewer nearest neighbors, the final dimensionality reduction will place more emphasis on effectively preserving the relationships between points in local neighborhoods.\n\n\n\nLarger values of \\(K\\) prioritize preservation of global structure, while\nsmaller \\(K\\) will better reflect local differences. This property is not obvious\na priori, but is suggested by the simulations described in the reading.\n\n\n\nFigure 4: When using larger neighborhoods, UMAP will place more emphasis on preserving global structure, sometimes at the cost of local relationships between points.\n\n\n\nOne detail in the graph construction: In UMAP, the edges are assigned weights\ndepending on the distance they span, normalized by the distance to the closest\nneighbor. Neighbors that are close, relative to the nearest neighbors, are\nassigned higher weights than those that are far away, and points that are linked\nby high weight edges are pulled together with larger force in the final graph\nlayout. This is what the authors mean by using a “fuzzy” nearest neighbor graph.\nThe fuzziness allows the algorithm to distinguish neighbors that are very close\nfrom those that are far, even though they all lie within a\n\\(K\\)-nearest-neighborhood.\nOnce the graph is constructed, there is the question of how the graph layout\nshould proceed. UMAP uses a variant of force-directed layout, and the global\nstrength of the springs is another hyperparameter. Lower tension on the springs\nallow the points to spread out more loosely, higher tension forces points closer\ntogether. This is a second hyperparameter of UMAP.\n\n\n\nThese two hyperparameters — the number of nearest neighbors \\(K\\) and the\nlayout tension — are the only two hyperparameters of UMAP.\nYou can see more examples of what this algorithm does to toy datasets in the\nreading. Note in particular\nthe properties that the algorithm does not preserve. The distance between\nclusters should not be interpreted, since it just means that the graph\ncomponents were not connected. Similarly, the density of points is not\npreserved.\nIn R, we can implement this using almost the same code as we used for PCA.\nThe step_umap command is available through the embed package.\n\n\ncocktails_df <- read_csv(\"https://uwmadison.box.com/shared/static/qyqof2512qsek8fpnkqqiw3p1jb77acf.csv\")\numap_rec <- recipe(~., data = cocktails_df) %>%\n  update_role(name, category, new_role = \"id\") %>%\n  step_normalize(all_predictors()) %>%\n  step_umap(all_predictors(), neighbors = 20, min_dist = 0.1)\numap_prep <- prep(umap_rec)\n\n\nUMAP returns a low-dimensional atlas relating the points, but it does not\nprovide any notion of derived features.\n\n\nggplot(juice(umap_prep), aes(UMAP1, UMAP2)) +\n  geom_point(aes(color = category), alpha = 0.7, size = 0.8) +\n  geom_text(aes(label = name), check_overlap = TRUE, size = 3, hjust = \"inward\")\n\n\n\nFigure 5: The learned UMAP representation of the cocktails dataset.\n\n\n\nWe can summarize the properties of UMAP,\nGlobal or local structure: The number of nearest neighbors \\(K\\) used\nduring graph construction can be used modulate the emphasis of global vs.\nlocal structure.\nNonlinear: UMAP can reflect nonlinear structure in high-dimensions.\nNo interpretable features: UMAP only returns the map between points, and\nthere is no analog of components to describe how the original features were\nused to construct the map.\nSlower: While UMAP is much faster than comparable nonlinear\ndimensionality reduction algorithms, it is still slower than linear\napproaches.\nNondeterministic: The output from UMAP can change from run to run, due\nto randomness in the graph layout step. If exact reproducibility is required,\na random seed should be set.\n\n\n\n",
    "preview": "posts/2024-12-27-week10-4/2024-12-27-week10-4_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-13T19:03:29+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week10-5/",
    "title": "PCA and UMAP Examples",
    "description": "More examples of dimensionality reduction using PCA and UMAP.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(embed)\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(tidymodels)\nlibrary(tidytext)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\nset.seed(479)\n\n\nThese notes don’t introduce any new conceptual material. Instead, they give a\nfew examples of how PCA and UMAP can be used.\nFrom two dimensions to one\nWe presented the two moons dataset earlier as an example where UMAP, but not\nPCA, would be able to discover a one-dimensional representation that separates\nthe groups. The implication is that, if we anticipate some sort of nonlinearity\nin higher-dimensions (which we can’t directly visualize), then UMAP would be a\nmore suitable choice.\n\n\nmoons <- read_csv(\"https://uwmadison.box.com/shared/static/kdt9qqvonhcz2ssb599p1nqganrg1w6k.csv\")\nggplot(moons, aes(X, Y, col = Class)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 1: The original two moons dataset. We will ask both PCA and UMAP to recover a 1D reduction of these 2D data.\n\n\n\nThe code block below defines both the PCA and UMAP recipes. There is no need\nto normalize the data, since the two dimensions are already on the same scale.\n\n\nmoons_ <- recipe(~ ., data = moons) %>%\n  update_role(Class, new_role = \"id\")\n\npca_rec <- step_pca(moons_, all_predictors(), num_comp = 1)\numap_rec <- step_umap(moons_, all_predictors(), num_comp = 1)\n\n\nThe block below shows both the UMAP and PCA representations. The PCA\nrepresentation seems to mostly reflect the variation on the \\(x\\)-axis of the\noriginal data, and the two classes mix together. On the other hand, the UMAP\nclearly separates the groups. This is expected, since the nearest neighborhood\ngraph that defines UMAP is likely separated into two major components, one for\neach moon.\n\n\nscores <- bind_cols(\n  prep(umap_rec) %>% juice() %>% mutate(UMAP1 = scale(UMAP1)),\n  prep(pca_rec) %>% juice() %>% select(-Class)\n) %>%\n  pivot_longer(-Class, names_to = \"method\")\n\nggplot(scores, aes(value, method, col = Class)) +\n  geom_point(position = position_jitter(h = 0.1), alpha = 0.8) +\n  scale_color_brewer(palette = \"Set2\") \n\n\n\nFigure 2: 1D PCA and UMAP representations of the 2D two moons dataset.\n\n\n\nAntibiotics Dataset\nWe can apply dimensionality reduction to the antibiotics dataset described in\nlecture 2 - 1.\nThere, we had filtered down to the 6 most abundant bacteria. Now we will\nconsider 147 most abundant, which means that each sample can be imagined as a\nvector in a 147-dimensional space. Ideally, a dimensionality-reduction procedure\nshould be able to place samples close to one another when they have similar\nspecies profiles. In addition to loading species counts, we load taxonomic\ninformation about each species, in the taxa variable.\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/t1lifegdz8s0a8lgckber32ytyh9hu4r.csv\")\ntaxa <- read_csv(\"https://uwmadison.box.com/shared/static/ng6y6etk79lrm0gtsgw2u0yq6gqcozze.csv\")\n\n\nWe now define a PCA recipe. Since the counts are relatively skewed, we\nlog-transform1, using step_log. The rest of the definition is like\nthe 2D example above.\n\n\nantibiotic_ <- recipe(~ ., data = antibiotic) %>%\n  update_role(sample:antibiotic, new_role = \"id\") %>%\n  step_log(all_predictors(), offset = 1) %>%\n  step_normalize(all_predictors())\n\npca_rec <- step_pca(antibiotic_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\nWe generate a map of the PCA scores below. The primary difference is between\nthe three study participants – D, E, and F. Within each person, there is some\nvariation between the antibiotic periods, as indicated by the points’ colors.\n\n\nscores <- juice(pca_prep) \nvariances <- tidy(pca_prep, 2, type = \"variance\")\nggplot(scores, aes(PC1, PC2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), check_overlap = TRUE, size = 3) +\n  coord_fixed(sqrt(variances$value[2] / variances$value[1])) + \n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 3: PCA scores for the antibiotics dataset. The main difference is between study participants, with some secondary variation related to whether the participant was taking the antibiotic at that timepoint.\n\n\n\nWhich species contribute the most to the principal components? We can analyze\nthis like we did with the cocktails dataset. In addition to plotting the raw\ncomponent value, we also join in the taxa information. This allows us to color\nin each bar by the species group that each bacteria belongs to. For example, we\nsee that samples on the right side of the plot above (i.e., high PC1) likely\nhave more Firmicutes than Bacteroidetes. PC2 seems to pick up on two species\nthat have higher abundance when the rest drop-off.\n\n\ncomponents_ <- tidy(pca_prep, 3) %>%\n  filter(component %in% str_c(\"PC\", 1:6)) %>%\n  mutate(terms_ = reorder_within(terms, abs(value), component)) %>%\n  group_by(component) %>%\n  top_n(20, abs(value)) %>%\n  left_join(taxa)\n\nggplot(components_, aes(value, terms_, fill = Phylum)) +\n  geom_col() +\n  facet_wrap(~ component, scales = \"free_y\") +\n  scale_y_reordered() +\n  labs(y = NULL) +\n  scale_fill_brewer(palette = \"Set1\") +\n  theme(axis.text = element_text(size = 5))\n\n\n\nFigure 4: The first six principal components associated with the antibiotics dataset.\n\n\n\nWe can use similar code to compute a UMAP embedding. The UMAP seems to\nseparate the different timepoints more clearly. However, there is no analog of\ncomponents with which to interpret the different axes. Instead, a typical\napproach to interpret the representation is to find points that are close\ntogether (e.g., using \\(K\\)-means) and take their average species profile.\n\n\numap_rec <- step_umap(antibiotic_, all_predictors(), min_dist = 1.5)\numap_prep <- prep(umap_rec)\n\nscores <- juice(umap_prep) \nggplot(scores, aes(UMAP1, UMAP2, col = antibiotic)) +\n  geom_point(aes(shape = ind), size = 1.5) +\n  geom_text_repel(aes(label = sample), max.overlaps = 10) +\n  scale_color_brewer(palette = \"Set2\")\n\n\n\nFigure 5: The UMAP representation associated with the antibiotics dataset.\n\n\n\nImage Data\nBoth PCA and UMAP can be used on image data. Here, each pixel in an image is\nconsidered a different feature. For example, the FashionMNIST dataset includes\n60,000 28 x 28 images of fashion objects. We can think of image as a\nvector2 \\(x_{i} \\in \\mathbb{R}^{784}\\). The goal of dimensionality\nreduction in this context is to build an atlas of images, where images with\nsimilar overall pixel values should be located next to one another. First, we\nread in the data and subsample it, so the code doesn’t take so long to run.\n\n\nfashion <- read_csv(\"https://uwmadison.box.com/shared/static/aur84ttkwa2rqvzo99qo7yhxemoc6om0.csv\") %>%\n  sample_frac(0.2) %>%\n  mutate(\n    image = row_number(),\n    label = as.factor(label)\n  )\n\n\nEach row of the matrix above is a separate image. We can prepare a PCA\nrecipe just like in the two examples above. Note that we are not normalizing the\nfeatures – the pixels are already on a common scale.\n\n\nfashion_ <- recipe(~ ., data = fashion) %>%\n  update_role(label, image, new_role = \"id\")\n\npca_rec <- step_pca(fashion_, all_predictors())\npca_prep <- prep(pca_rec)\n\n\n\n\n\nThe code below extracts the PCA scores and visualizes them as a cloud of\npoints. Each point corresponds to an image, and the different types of fashion\nitems are indicated by color. It seems that the types are well separated, but\nthe labels are not informative… to understand what the colors mean, we need to\nlook at the images.\n\n\nscores <- juice(pca_prep) %>%\n  rename(x = PC1, y = PC2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\nFigure 6: Principal component scores from the fashion dataset.\n\n\n\nThe block below overlays the first 300 images from the dataset at the\nlocations from the previous plot. We have prepared a function to generate this\nsort of image using ggplot2; however, we have hidden it to avoid cluttering\nthese notes. You can view the function in the rmarkdown link at the top of this\ndocument.\n\n\npivot_scores(scores, fashion) %>%\n  overlay_images()\n\n\n\nFigure 7: A subset of principal component scores expressed as the corresponding images.\n\n\n\nFinally, we can repeat the exercise above with UMAP. The first plot shows\nthe UMAP scores and the second overlays the same set of 300 images onto these\nnew coordinates. It seems that UMAP can more clearly separate shoes and pants\nfrom shirts and sweaters.\n\n\numap_rec <- step_umap(fashion_, all_predictors(), num_comp = 2, min_dist = 0.5)\numap_prep <- prep(umap_rec)\nscores <- juice(umap_prep) %>%\n  rename(x = UMAP1, y = UMAP2)\nggplot(scores, aes(x, y, col = label)) +\n  geom_point() +\n  scale_color_brewer(palette = \"Set3\") +\n  coord_fixed()\n\n\n\nFigure 8: The locations of all the images according to UMAP. Each color is a different class.\n\n\n\n\n\npivot_scores(scores, fashion, scale_factor = 0.05) %>%\n  overlay_images(scale_factor = 0.05)\n\n\n\nFigure 9: A sample of the images at the locations determined by UMAP.\n\n\n\n\nSpecifically, we use a \\(\\log\\left(1 + x\\right)\\) transform, since\nthere are many 0 counts.↩︎\n28 * 28 = 784↩︎\n",
    "preview": "posts/2024-12-27-week10-5/2024-12-27-week10-5_files/figure-html5/unnamed-chunk-14-1.png",
    "last_modified": "2025-01-13T19:04:43+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 960
  },
  {
    "path": "posts/2024-12-27-week11-1/",
    "title": "Introduction to Topic Models",
    "description": "An overview of dimensionality reduction via topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nTopic modeling is a type of dimensionality reduction method that is\nespecially useful for high-dimensional count matrices. For example, it can be\napplied to,\nText data analysis, where each row is a document and each column is a word.\nThe \\(ij^{th}\\) entry contains the count of the \\(j^{th}\\) word in the \\(i^{th}\\)\ndocument.\nGene expression analysis, where each row is a biological sample and each\ncolumn is a gene. The \\(ij^{th}\\) entry measures the amount of gene \\(j\\)\nexpressed in sample \\(i\\).\n\nFor clarity, we will refer to samples as documents and features as words.\nHowever, keep in mind that these methods can be used more generally – we will\nsee a biological application three lectures from now.\nThese models are useful to know about because they provide a compromise\nbetween clustering and PCA.\nIn clustering, each document would have to be assigned to a single topic.\nIn contrast, topic models allow each document to partially belong to several\ntopics simultaneously. In this sense, they are more suitable when data do\nnot belong to distinct, clearly-defined clusters.\nPCA is also appropriate when the data vary continuously, but it does not\nprovide any notion of clusters. In contrast, topic models estimate \\(K\\)\ntopics, which are analogous to a cluster centroids (though documents are\ntypically a mix of several centroids).\n\nWithout going into mathematical detail, topic models perform dimensionality\nreduction by supposing,\nEach document is a mixture of topics.\nEach topic is a mixture of words.\n\n\n\n\nFigure 1: An overview of the topic modeling process. Topics are distributions over words, and the word counts of new documents are determined by their degree of membership over a set of underlying topics. In an ordinary clustering model, the bars for the memberships would have to be either pure purple or orange. Here, each document is a mixture.\n\n\n\nTo illustrate the first point, consider modeling a collection of newspaper\narticles. A set of articles might belong primarily to the “politics” topic, and\nothers to the “business” topic. Articles that describe a monetary policy in the\nfederal reserve might belong partially to both the “politics” and the “business”\ntopic.\nFor the second point, consider the difference in words that would\nappear in politics and business articles. Articles about politics might\nfrequently include words like “congress” and “law,” but only rarely words\nlike “stock” and “trade.”\nGeometrically, LDA can be represented by the following picture. The corners\nof the simplex1\nrepresent different words (in reality, there would be \\(V\\) different corners to\nthis simplex, one for each word). A topic is a point on this simplex. The closer\nthe topic is to one of the corners, the more frequently that word appears in the\ntopic.\n\n\n\nFigure 2: A geometric interpretation of LDA, from the original paper by Blei, Ng, and Jordan.\n\n\n\nA document is a mixture of topics, with more words coming from the topics\nthat it is close to. More precisely, a document that is very close to a\nparticular topic has a word distribution just like that topic. A document that\nis intermediate between two topics has a word distribution that mixes between\nboth topics. Note that this is different from a clustering model, where all\ndocuments would lie at exactly one of the corners of the topic simplex. Finally,\nnote that the topics form their own simplex, since each document can be\ndescribed as a mixture of topics, with mixture weights summing up to 1.\n\nA simplex is the geometric object describing the set of\nprobability vectors over \\(V\\) elements. For example, if \\(V = 3\\), then \\(\\left(0.1,\n0, 0.9\\right)\\) and \\(\\left(0.2, 0.3, 0.5\\right)\\) belong to the simplex, but not\n\\(\\left(0.3, 0.1, 9\\right)\\), since it sums to a number larger than 1.↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/3shdh2f5vqarkwjucmmebigj2rwm4wyh.png",
    "last_modified": "2025-01-13T19:04:49+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-2/",
    "title": "Fitting Topic Models",
    "description": "Data preparation and model fitting code for topics.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"ggplot2\")\nlibrary(\"gutenbergr\")\nlibrary(\"stringr\")\nlibrary(\"tidyr\")\nlibrary(\"tidytext\")\nlibrary(\"topicmodels\")\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nThere are several packages in R that can be used to fit topic models. We will\nuse LDA as implemented in the topicmodels package, which expects input to be\nstructured as a DocumentTermMatrix, a special type of matrix that stores the\ncounts of words (columns) across documents (rows). In practice, most of the\neffort required to fit a topic model goes into transforming the raw data into a\nsuitable DocumentTermMatrix.\nTo illustrate this process, let’s consider the “Great Library Heist” example\nfrom the reading. We imagine that a thief has taken four books — Great\nExpectations, Twenty Thousand Leagues Under The Sea, War of the Worlds, and\nPride & Prejudice — and torn all the chapters out. We are left with pieces of\nisolated pieces of text and have to determine from which book they are from. The\nblock below downloads all the books into an R object.\n\n\ntitles <- c(\"Twenty Thousand Leagues under the Sea\", \n            \"The War of the Worlds\",\n            \"Pride and Prejudice\", \n            \"Great Expectations\")\nbooks <- gutenberg_works(title %in% titles) %>%\n  gutenberg_download(meta_fields = \"title\")\nbooks\n\n# A tibble: 6,372 × 3\n   gutenberg_id text                    title                \n          <int> <chr>                   <chr>                \n 1           36 \"cover \"                The War of the Worlds\n 2           36 \"\"                      The War of the Worlds\n 3           36 \"\"                      The War of the Worlds\n 4           36 \"\"                      The War of the Worlds\n 5           36 \"\"                      The War of the Worlds\n 6           36 \"The War of the Worlds\" The War of the Worlds\n 7           36 \"\"                      The War of the Worlds\n 8           36 \"by H. G. Wells\"        The War of the Worlds\n 9           36 \"\"                      The War of the Worlds\n10           36 \"\"                      The War of the Worlds\n# ℹ 6,362 more rows\n\nSince we imagine that the word distributions are not equal across the books,\ntopic modeling is a reasonable approach for discovering the books associated\nwith each chapter. Note that, in principle, other clustering and dimensionality\nreduction procedures could also work.\nFirst, let’s simulate the process of tearing the chapters out. We split the\nraw texts anytime the word “Chapter” appears. We will keep track of the book\nnames for each chapter, but this information is not passed into the topic\nmodeling algorithm.\n\n\nby_chapter <- books %>%\n  group_by(title) %>%\n  mutate(\n    chapter = cumsum(str_detect(text, regex(\"chapter\", ignore_case = TRUE)))\n  ) %>%\n  group_by(title, chapter) %>%\n  mutate(n = n()) %>%\n  filter(n > 5) %>%\n  ungroup() %>%\n  unite(document, title, chapter)\n\n\nAs it is, the text data are long character strings, giving actual text from\nthe novels. To fit LDA, we only need counts of each word within each chapter –\nthe algorithm throws away information related to word order. To derive word\ncounts, we first split the raw text into separate words using the unest_tokens\nfunction in the tidytext package. Then, we can count the number of times each\nword appeared in each document using count, a shortcut for the usual\ngroup_by and summarize(n = n()) pattern.\n\n\nword_counts <- by_chapter %>%\n  unnest_tokens(word, text) %>%\n  anti_join(stop_words) %>%\n  count(document, word) # shortcut for group_by(document, word) %>% summarise(n = n())\n\nword_counts\n\n# A tibble: 8,825 × 3\n   document                word            n\n   <chr>                   <chr>       <int>\n 1 The War of the Worlds_0 10,000,000      1\n 2 The War of the Worlds_0 12              1\n 3 The War of the Worlds_0 140,000,000     1\n 4 The War of the Worlds_0 1894            1\n 5 The War of the Worlds_0 2               1\n 6 The War of the Worlds_0 35,000,000      1\n 7 The War of the Worlds_0 8th             1\n 8 The War of the Worlds_0 _been_          1\n 9 The War of the Worlds_0 _brutes_        1\n10 The War of the Worlds_0 _daily          3\n# ℹ 8,815 more rows\n\nThese words counts are still not in a format compatible with conversion to a\nDocumentTermMatrix. The issue is that the DocumentTermMatrix expects words\nto be arranged along columns, but currently they are stored across rows. The\nline below converts the original “long” word counts into a “wide”\nDocumentTermMatrix in one step. Across these 4 books, we have 65 chapters and\na vocabulary of size 18325.\n\n\nchapters_dtm <- word_counts %>%\n  cast_dtm(document, word, n)\nchapters_dtm\n\n<<DocumentTermMatrix (documents: 4, terms: 6352)>>\nNon-/sparse entries: 8825/16583\nSparsity           : 65%\nMaximal term length: 17\nWeighting          : term frequency (tf)\n\nOnce the data are in this format, we can use the LDA function to fit a\ntopic model. We choose \\(K = 4\\) topics because we expect that each topic will\nmatch a book. Different hyperparameters can be set using the control argument.\n\n\nchapters_lda <- LDA(chapters_dtm, k = 4, control = list(seed = 1234))\nchapters_lda\n\nA LDA_VEM topic model with 4 topics.\n\nThere are two types of outputs produced by the LDA model: the topic word\ndistributions (for each topic, which words are common?) and the document-topic\nmemberships (from which topics does a document come from?). For visualization,\nit will be easiest to extract these parameters using the tidy function,\nspecifying whether we want the topics (beta) or memberships (gamma).\n\n\ntopics <- tidy(chapters_lda, matrix = \"beta\")\nmemberships <- tidy(chapters_lda, matrix = \"gamma\")\n\n\nThis tidy approach is preferable to extracting the parameters directly from\nthe fitted model (e.g., using chapters_lda@gamma) because it ensures the\noutput is a tidy data.frame, rather than a matrix. Tidy data.frames are easier\nto visualize using ggplot2.\n\n\n# highest weight words per topic\ntopics %>%\n  arrange(topic, -beta)\n\n# A tibble: 25,408 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 martians 0.0118 \n 2     1 martian  0.0105 \n 3     1 time     0.0102 \n 4     1 looked   0.00950\n 5     1 day      0.00888\n 6     1 eyes     0.00888\n 7     1 machine  0.00756\n 8     1 food     0.00752\n 9     1 green    0.00732\n10     1 hill     0.00696\n# ℹ 25,398 more rows\n\n# topic memberships per document\nmemberships %>%\n  arrange(document, topic)\n\n# A tibble: 16 × 3\n   document                topic      gamma\n   <chr>                   <int>      <dbl>\n 1 The War of the Worlds_0     1 0.00000205\n 2 The War of the Worlds_0     2 0.00127   \n 3 The War of the Worlds_0     3 0.999     \n 4 The War of the Worlds_0     4 0.00000205\n 5 The War of the Worlds_1     1 0.0000204 \n 6 The War of the Worlds_1     2 1.00      \n 7 The War of the Worlds_1     3 0.0000204 \n 8 The War of the Worlds_1     4 0.0000204 \n 9 The War of the Worlds_2     1 0.219     \n10 The War of the Worlds_2     2 0.00000307\n11 The War of the Worlds_2     3 0.00000307\n12 The War of the Worlds_2     4 0.781     \n13 The War of the Worlds_3     1 0.998     \n14 The War of the Worlds_3     2 0.000599  \n15 The War of the Worlds_3     3 0.000599  \n16 The War of the Worlds_3     4 0.000599  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-13T19:04:50+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week11-3/",
    "title": "Visualizing Topic Models",
    "description": "Once we've fit a topic model, how should we inspect it?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading 1 and 2, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(ggrepel)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nIn the last set of notes, we fit a topic model to the “Great Library Heist”\ndataset, but we did not visualize or interpret the results. We’ll work on that\nhere. The code below reads in the tidy topic and membership data.frames from\nbefore.\n\n\nmemberships <- read_csv(\"https://uwmadison.box.com/shared/static/c5k5iinwo9au44fb3lc00vq6isbi72c5.csv\")\ntopics <- read_csv(\"https://uwmadison.box.com/shared/static/uh34hhc1wnp072zcryisvgr3z0yh25ad.csv\")\n\n\nVisualizing Topics\nA topic is a probability distribution across a collection of words. If the\nvocabulary isn’t too large, two appropriate visualization strategies are,\nFaceted barplot: Each facet corresponds to a topic. The height of each bar\ncorresponds to a given word’s probability within the topic. The sum of heights\nacross all bars is 1.\nHeatmap: Each row is a topic and each column is a word. The color of the\nheatmap cells gives the probability of the word within the given topic.\n\nWe can construct a faceted barplot using the tidied beta matrix. We’ve\nfiltered to only words with a probability of at least \\(0.0003\\) in at least one\ntopic, but there are still more words than we could begin to inspect.\nNonetheless, it seems that there are words that have relatively high probability\nin one topic, but not others.\n\n\nggplot(topics %>% filter(beta > 3e-4), aes(term, beta)) +\n  geom_col() +\n  facet_grid(topic ~ .) +\n  theme(axis.text.x = element_blank())\n\n\n\nFigure 1: A faceted barplot view of the original topic distributions, with only very limited filtering.\n\n\n\nFor the heatmap, we need to pivot the topics, so that words appear along\ncolumns. From there, we can use superheatmap. The advantage of the heatmap is\nthat it takes up less space, and while it obscures comparisons between word\nprobabilities1 the main\ndifferences of interest are between low and high probability words.\n\n\ntopics %>%\n  filter(beta > 3e-4) %>%\n  pivot_wider(names_from = \"term\", values_from = \"beta\", values_fill = 0) %>%\n  select(-1) %>%\n  superheat(\n    pretty.order.cols = TRUE,\n    legend = FALSE\n  )\n\n\n\nFigure 2: An equivalent heatmap view of the above faceted barplot.\n\n\n\nNeither approach is very satisfactory since there are too many words for us\nto effectively label. A workaround is to restrict attention to a subset of\n“interesting” words. For example, we could filter to,\nTop words overall: We can consider only words whose probabilities are\nabove some threshold. This is the approach used in the visualizations above,\nthough the threshold is very low (there are still too many words to add\nlabels).\nTop words per topic: We can sort the words within each topic in order from\nhighest to lowest probability, and then keep only the \\(S\\) largest.\nMost discriminative words: Some words have high probability just because\nthey are common. They have high probability within each topic but aren’t\nactually interesting as far as characterizing the topics is concerned.\nInstead, we can focus on words that are common in some topics but rare in\nothers.\n\nWe can obtain the most probable words using the slice_max function, after\nfirst grouping by topic. Then, we use the same reorder_within function from\nthe PCA lectures to reorder words within each topic. The resulting plot is much\nmore interpretable. Judging from the words that are common in each topic’s\ndistribution, we can guess that the topics approximately correspond to: 1 ->\nGreat Expectations, 2 -> 20,000 Leagues Under the Sea, 3 -> Pride & Prejudice, 4\n-> War of the Worlds.\n\n\ntop_terms <- topics %>%\n  group_by(topic) %>%\n  slice_max(beta, n = 10) %>%\n  mutate(term = reorder_within(term, beta, topic))\n\nggplot(top_terms, aes(beta, term, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_y_reordered()\n\n\n\nFigure 3: The top words associated with the four fitted topics from the Great Library Heist example.\n\n\n\nTo visualize discriminative words, we first compute a discrimination measure\nfor each word and filter to those with the top score. The filtered results can\nbe used in either faceted barplots or heatmaps. Specifically, to find the words\nthat discriminate between topics \\(k\\) and \\(l\\), we compute\n\\[\\begin{align*}\nD\\left(k, l\\right) := \\beta_{kw}\\log\\left(\\frac{\\beta_{kw}}{\\beta_{lw}}\\right) + \\left(\\beta_{lw} - \\beta_{kw}\\right)\n\\end{align*}\\]\nfor each word \\(w\\). By maximizing over all pairs \\(k, l\\), we can determine whether\nthe word is discriminative between any pair of topics. This might seem like a\nmysterious formula, but it is just a function that is large when topic \\(k\\) has\nmuch larger probability than topic \\(l\\) (see the figure).\n\n\n\n\n\np <- seq(0.01, .99, length.out = 50)\ndf <- expand.grid(p, p) %>%\n  mutate(D = kl_div(Var1, Var2))\n\nggplot(df, aes(Var2, Var1)) +\n  geom_tile(aes(col = D, fill = D)) +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_fixed() +\n  scale_color_distiller(direction = 1) +\n  scale_fill_distiller(direction = 1) +\n  labs(\n    y = expression(beta[kw]),\n    x = expression(beta[lw])\n  )\n\n\n\nFigure 4: An illustration of the formula used for computing a word’s discrimination between topics. The value of D is large when topic k has much larger probability than topic l.\n\n\n\nAn example heatmap of discriminative words is shown below. This backs up our\ninterpretation from the figure above. It also has the advantage that it removes\ncommon words (e.g., hand, people, and time appeared in the plot above) and\nhighlights rarer words that are specific to individual topics (e.g., names of\ncharacters that appear in only one of the books).\n\n\ndiscriminative_terms <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 200) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_terms %>%\n  pivot_wider(names_from = \"topic\", values_from = \"beta\") %>%\n  column_to_rownames(\"term\") %>%\n  select(-D) %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\nFigure 5: A heatmap of the terms that are most discriminative across the four topics.\n\n\n\nVisualizing Memberships\nBesides the topics, it is useful to study the topic proportions for each\nchapter. One compact approach is to use a boxplot. The result below suggest that\neach chapter is very definitely assigned to one of the four topics, except for\nchapters from Great Expectations. Therefore, while the model had the flexibility\nto learn more complex mixtures, it decided that a clustering structure made the\nmost sense for Pride & Prejudice, War of the Worlds, and 20,000 Leagues Under\nthe Sea.\n\n\nmemberships <- memberships %>%\n  mutate(\n    book = str_extract(document, \"[^_]+\"),\n    topic = factor(topic)\n  )\n\nggplot(memberships, aes(topic, gamma)) +\n  geom_boxplot() +\n  facet_wrap(~book)\n\n\n\nFigure 6: A boxplot of the document memberships. It seems that most documents are definitively assigned to one of the four topics.\n\n\n\nThe boxplot considers the collection of documents in aggregate. If we want\nto avoid aggregation and visualize individual documents, we can use a heatmap or\njittered scatterplot. These approaches are useful because heatmap cells and\nindividual points can be drawn relatively small — anything requiring more space\nwould become unwieldy as the number of documents grows. For example, the plot\nbelow shows that chapter 119 of Great Expectations has unusually high membership\nin Topic 2 and low membership in topic 3.\n\n\nggplot(memberships, aes(topic, gamma, col = book)) +\n  geom_point(position = position_jitter(h = 0.05, w = 0.3)) +\n  geom_text_repel(aes(label = document), size = 3) +\n  facet_wrap(~ book) +\n  scale_color_brewer(palette = \"Set1\")\n\n\n\nFigure 7: A jittered scatterplot of the topic memberships associated with each document.\n\n\n\nAlternatively, we can use a “structure” plot. This is a type of stacked\nbarplot where the colors of each bar corresponds to a topic. We’ve sorted the\ndocuments using the result of a hierarchical clustering on their proportion\nvectors – this is like how superheatmap orders rows using a dendrogram when\nusing pretty.order.rows. The takeaways here are similar to those in the\nscatterplot above.\n\n\ngamma <- memberships %>%\n  pivot_wider(names_from = topic, values_from = gamma)\n\n\n\n\nhclust_result <- hclust(dist(gamma[, 3:6]))\ndocument_order <- gamma$document[hclust_result$order]\nmemberships <- memberships %>%\n  mutate(document = factor(document, levels = document_order))\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(book ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(axis.text.y = element_blank())\n\n\n\nFigure 8: A structure plot view of each chapter’s topic memberships.\n\n\n\n\nColor is in general harder to compare than bar height.↩︎\n",
    "preview": "posts/2024-12-27-week11-3/2024-12-27-week11-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2025-01-13T19:04:51+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week11-4/",
    "title": "Topic Modeling Case Study",
    "description": "An application to a gene expression dataset.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(superheat)\nlibrary(tidytext)\nlibrary(topicmodels)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nWe have used text data analysis to motivate and illustrate the use of topic\nmodels. However, these models can be used whenever we have high-dimensional\ncount data1. To illustrate this broad applicability, this\nlecture will consider an example from gene expression analysis.\nThe dataset we consider comes from the GTEX\nconsortium. A variety of tissue\nsamples have been subject to RNA-seq analysis, which measures how much of each\ntype of gene is expressed within each sample. Intuitively, we relate,\nDocuments → Tissue samples\nWords → Genes\nWord Counts → Gene expression levels\n\n\n\nx <- read_csv(\"https://uwmadison.box.com/shared/static/fd437om519i5mrnur14xy6dq3ls0yqt2.csv\")\nx\n\n# A tibble: 2,000,000 × 6\n   sample                 gene  tissue tissue_detail Description value\n   <chr>                  <chr> <chr>  <chr>         <chr>       <dbl>\n 1 GTEX-NFK9-0926-SM-2HM… ENSG… Heart  Heart - Left… FGR           368\n 2 GTEX-OXRO-0011-R10A-S… ENSG… Brain  Brain - Fron… FGR           593\n 3 GTEX-QLQ7-0526-SM-2I5… ENSG… Heart  Heart - Left… FGR           773\n 4 GTEX-POMQ-0326-SM-2I5… ENSG… Heart  Heart - Left… FGR           330\n 5 GTEX-QESD-0526-SM-2I5… ENSG… Heart  Heart - Left… FGR           357\n 6 GTEX-OHPN-0011-R4A-SM… ENSG… Brain  Brain - Amyg… FGR           571\n 7 GTEX-OHPK-0326-SM-2HM… ENSG… Heart  Heart - Left… FGR           391\n 8 GTEX-OIZG-1126-SM-2HM… ENSG… Heart  Heart - Left… FGR           425\n 9 GTEX-O5YW-0326-SM-2I5… ENSG… Heart  Heart - Left… FGR           172\n10 GTEX-REY6-1026-SM-2TF… ENSG… Heart  Heart - Left… FGR           875\n# ℹ 1,999,990 more rows\n\nThe goal here is to find sets of genes that tend to be expressed together,\nbecause these co-expression patterns might be indications of shared biological\nprocesses. Unlike clustering, which assumes that each sample is described by one\ngene expression profile, a topic model will be able to model each tissue sample\nas a mixture of profiles (i.e., a mixture of underlying biological processes).\nAs a first step in our analysis, we need to prepare a DocumentTermMatrix\nfor use by the topicmodels package. Since the data were in tidy format, we can\nuse the cast_dtm function to spreaed genes across columns. From there, we can\nfit an LDA model. However, we’ve commented out the code (it takes a while to\nrun) and instead just download the results that we’ve hosted on Box.\n\n\nx_dtm <- cast_dtm(x, sample, gene, value)\n#fit <- LDA(x_dtm, k = 10, control = list(seed = 479))\n#save(fit, file = \"lda_gtex.rda\")\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/ifgo6fbvm8bdlshzegb5ty8xif5istn8.rda\", f)\nfit <- get(load(f))\n\n\nLet’s extract the tidy topic and memberships data. For the memberships, we\nwill also join in the tissue from which each biological sample belonged.\n\n\ntissue_info <- x %>%\n  select(sample, starts_with(\"tissue\")) %>%\n  unique()\n\ntopics <- tidy(fit, matrix = \"beta\") %>%\n  mutate(topic = factor(topic))\nmemberships <- tidy(fit, matrix = \"gamma\") %>%\n  mutate(topic = factor(topic)) %>%\n  left_join(tissue_info, by = c(\"document\" = \"sample\"))\n\n\nWe can now visualize the topics. Let’s consider the genes with the highest\ndiscrimination between topics, using the same discrimination score as in the\nprevious notes. Each row in the heatmap below is a gene, and each column is a\ntopic. The intensity of color represents the gene’s probability within the\ncorresponding topic. Since only discriminative genes are shown, it’s not\nsurprising that most genes are only active within a subset of topics.\n\n\n\n\n\ndiscriminative_genes <- topics %>%\n  group_by(term) %>%\n  mutate(D = discrepancy(beta)) %>%\n  ungroup() %>%\n  slice_max(D, n = 400) %>%\n  mutate(term = fct_reorder(term, -D))\n\ndiscriminative_genes %>%\n  pivot_wider(names_from = topic, values_from = beta) %>%\n  column_to_rownames(\"term\") %>%\n  superheat(\n    pretty.order.rows = TRUE,\n    left.label.size = 1.5,\n    left.label.text.size = 3,\n    bottom.label.size = 0.05,\n    legend = FALSE\n  )\n\n\n\nFigure 1: A heatmap of the most discriminative genes across the 10 estimated topics.\n\n\n\nNow, let’s see what tissues are related to which topics. We can use a\nstructure plot. Before making the plot, we prepare the data appropriately.\nFirst, there are some tissues with very few samples, so we will filter those\ntissues away. Second, we will reorder the samples so that those samples with\nsimilar topic profiles are placed next to one another. This is accomplished by\nrunning a hierarchical clustering on the topic membership vectors and extracting\nthe order of the resulting dendrogram leaves.\n\n\nkeep_tissues <- memberships %>%\n  count(tissue) %>%\n  filter(n > 70) %>%\n  pull(tissue)\n\nhclust_result <- hclust(dist(fit@gamma))\ndocument_order <- fit@documents[hclust_result$order]\nmemberships <- memberships %>%\n  filter(tissue %in% keep_tissues) %>%\n  mutate(document = factor(document, levels = document_order))\n\n\nNext, we can generate the structure plot. The first three lines are the key\nlines: they create a stacked barchart for each sample and then facet across\ntissues. The remaining lines simply refine the appearance of the plot.\n\n\nggplot(memberships, aes(gamma, document, fill = topic, col = topic)) +\n  geom_col(position = position_stack()) +\n  facet_grid(tissue ~ ., scales = \"free\", space = \"free\") +\n  scale_x_continuous(expand = c(0, 0)) +\n  scale_color_brewer(palette = \"Set3\", guide = \"none\") +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(x = \"Topic Membership\", y = \"Sample\", fill = \"Topic\") +\n  theme(\n    panel.spacing = unit(0.5, \"lines\"),\n    strip.switch.pad.grid = unit(0, \"cm\"),\n    strip.text.y = element_text(size = 8, angle = 0),\n    axis.text.y = element_blank(),\n  )\n\n\n\nFigure 2: A structure plot, showing the topic memberships across all tissue samples in the dataset.\n\n\n\nFrom this plot, we can see clearly that different tissues express different\ncombinations of topics. For example, pancreas tissue typically expresses genes\nwith high probability in topics 3 and 8. Further, within tissues, there can be\ndifferences in the types of genes expressed – certain blood cells are almost\nentirely summarized by topic 1, but most require some mixture of topics 1 and 6.\nFinally, we see that there are some topics that are common across several\ntissues. For example, topic 4 is key component of thyroid, skin, muscle, lung,\nand some brain tissue.\n\nIn fact, topic models are an example of a larger family of models,\ncalled mixed-membership models. All of these models generalize clustering, and\ndifferent variants can be applied to other data types, like continuous,\ncategorical, and network data.↩︎\n",
    "preview": "posts/2024-12-27-week11-4/2024-12-27-week11-4_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2025-01-13T19:04:51+02:00",
    "input_file": {},
    "preview_width": 960,
    "preview_height": 1536
  },
  {
    "path": "posts/2024-12-27-week12-1/",
    "title": "Partial Dependence Profiles I",
    "description": "An introduction to partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(caret)\nlibrary(tidyverse)\nlibrary(DALEX)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nAs more complex models become more common in practice, visualization has\nemerged as a key way for (a) summarizing their essential structure and (b)\nmotivating further modeling refinements.\nIn modern machine learning, it’s common to use a function \\(f\\) to approximate\nthe relationship between a \\(D\\)-dimensional input \\(\\mathbf{x}\\) and a univariate\nresponse \\(y\\). We are given a sample of \\(n\\) pairs \\(\\left(\\mathbf{x}_{i},\ny_{i}\\right)\\) with which to learn this relationship, and we hope that the\nfunction we learn will generalize to future observations.\nSome further notation: We will write \\(x_{j}\\) for the \\(j^{th}\\) coordinate of\n\\(\\mathbf{x}\\). We will write \\(\\mathbf{x}^{j\\vert = z}\\) to denote the observation\n\\(\\mathbf{x}\\) with the \\(j^{th}\\) coordinate set to \\(z\\).\n\n\n\nFigure 1: Illustration of the \\(\\mathbf{x}^{j \\vert = z}\\) operation. The \\(j^{th}\\) coordinate (1 in this case) for a selected observation is set equal to \\(z\\).\n\n\n\nLinear models are simple enough that they don’t require any follow-up visual\ninspection. Since they assume \\(f\\left(\\mathbf{x}\\right) =\n\\hat{\\beta}^{T}\\mathbf{x}\\), they are completely described by the vector of\ncoefficients \\(\\hat{\\beta}\\). We can exactly describe what happens to \\(f\\) when we\nincrease \\(x_{j}\\) by one unit: we just increase the prediction by\n\\(\\hat{\\beta}_{j}\\).\nMore complex models — think random forests or neural networks — don’t have\nthis property. While these models often have superior performance, it’s hard to\nsay how changes in particular input features will affect the prediction.\nPartial dependence plots try to address this problem. They provide a\ndescription for how changing the \\(j^{th}\\) input feature affects the predictions\nmade by complex models.\nTo motivate the definition, consider the toy example below. The surface is\nthe fitted function \\(f\\left(\\mathbf{x}\\right)\\), mapping a two dimensional input\n\\(\\mathbf{x}\\) to a real-valued response. How would you summarize the relationship\nbetween \\(x_{1}\\) and \\(y\\)? The main problem is that the shape of the relationship\ndepends on which value of \\(x_{2}\\) we start at.\n\n\n\nFigure 2: An example of why it is difficult to summarize the relationship between an input variable and a fitted surface for nonlinear models.\n\n\n\nOne idea is to consider the values of \\(x_{2}\\) that were observed in our\ndataset. Then, we can evaluate our model over a range of values \\(x_{1}\\) after\nfixing those values of \\(x_{2}\\). These curves are called Ceteris Paribus\nprofiles1.\nThe same principle holds in higher dimensions. We can fix \\(D - 1\\) coordinates\nof an observation and then evaluate what happens to a sample’s predictions when\nwe vary coordinate \\(j\\). Mathematically, this is expressed by \\(h_{x}^{f,\nj}\\left(z\\right) := f\\left(\\mathbf{x}^{j\\vert= z}\\right)\\).\n\n\n\nFigure 3: Visual intuition behind the CP profile. Varying the \\(j^{th}\\) coordinate for an observation traces out a curve in the prediction surface.\n\n\n\nFor example, let’s consider how CP can be used to understand a model fitted\nto the Titanic dataset. This is a dataset that was used to understand what\ncharacteristics survivors of the Titanic disaster had in common. It’s not\nobvious in advance which characteristics of passengers made them more likely to\nsurvive, so a model is fitted to predict survival.\n\n\ndata(titanic)\ntitanic <- select(titanic, -country) %>%\n  na.omit()\n\nx <- select(titanic, -survived)\nhyper <- data.frame(n.trees = 100, interaction.depth = 8, shrinkage = 0.1, n.minobsinnode = 10)\nfit <- train(x = x, y = titanic$survived, method = \"gbm\", tuneGrid = hyper, verbose = F)\n\n\nNext, we can compute the CP profiles. We are showing the relationship\nbetween age and survival, though any subset of variables could be requested. The\nbold curve is a Partial Dependence (PD) profile, which we will discuss below.\nEach of the other curves corresponds to a passenger, though only a subsample is\nshown. The curves are obtained by fixing all the characteristics of the\npassanger except for age, and then seeing what happens to the prediction when\nthe age variable is increased or decreased.\n\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  (  default  )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.01199248 , mean =  0.326649 , max =  0.9868015  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \n\nprofile <- model_profile(explainer = explanation)\nplot(profile, geom = \"profiles\", variables = \"age\") +\n  theme479\n\n\n\nFigure 4: CP and PDP profiles for age, for a GBM fitted to the Titanic dataset.\n\n\n\nIt seems that children had the highest probability2 of survival. The relationship is\nfar from linear, with those between 40 and 60 all having about the same\nprobabilities. Notice that the profiles are vertically offset from one passenger\nto another. This is because, aside from age, each passenger had characteristics\nthat made them more or less likely to survive.\nWe used the DALEX package to produce these curves. The explain function\ntakes the fitted model and original dataset as input. It returns an object with\nmany kinds of model summaries. To extract the CP profiles from these summaries,\nwe use model_profile. The output of this function has been designed so that\ncalling plot with geom = \"profiles\" will show the CP profiles.\nThe PD profile is computed by averaging across all the CP profiles. It is a\nmore concise alternative to CP profiles, showing one curve per features, rather\nthan one curve per sample.\n\n\nplot(profile, geom = \"aggregates\") +\n  theme479\n\n\n\nNot only are the PD plots simpler to read than the full collection of CP\nprofiles — by performing this aggregation step, subtle patterns may become more\nsalient, for the same reason that an average carries more information than any\nsubset of observations.\n\nCeteris Paribus means « All Else Held Equal. »↩︎\nTechnically, these are\nall predicted probabilities from the model.↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/mpe45nor6xm4gt1idhedayw9ik754c2k.png",
    "last_modified": "2025-01-13T19:05:06+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week12-2/",
    "title": "Partial Dependence Profiles II",
    "description": "Discovering richer structure in partial dependence profiles.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(caret)\nlibrary(DALEX)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nPartial dependence (PD) plots help answer the question, « How is feature \\(j\\)\nused by my model \\(f\\)? » Slight variations on PD plots are useful for some\nrelated followup questions,\nHas my model \\(f\\) learned interactions between features \\(j\\) and \\(j^{\\prime}\\)?\nHow do the models \\(f\\) and \\(f^{\\prime}\\) differ in the way that they use feature \\(j\\)?\n\nThe variants, called Grouped and Contrastive PD plots, reduce the original CP\nprofiles less aggressively than PD plots, but without becoming overwhelmingly\ncomplicated.\nInteractions\nWe say that there is an interaction between variables \\(j\\) and \\(j^{\\prime}\\) if\nthe relationship between \\(x_{j}\\) and \\(y\\) is modulated by variable \\(j^{\\prime}\\).\nFor example, in the figure below, the slope of cross-sections across \\(j\\) depends\non \\(j^{\\prime}\\).\nUsing the language of CP profiles, the figure above means that the shape of\nthe CP profile in \\(j\\) depends on the particular setting of \\(j^{\\prime}\\). This\nmotivates the use of Grouped PD profiles — we compute several PD profiles in\n\\(j\\), restricting attention to CP profiles whose value \\(x_{j^{\\prime}}\\) lies\nwithin a prespecified range.\nTo illustrate, we revisit the CP profiles for age from the Titanic dataset.\nBelow, the profiles are grouped according to the class of the ticket holder. The\nresult shows that the relationship between age and survival was not the same\nacross all passengers. For all classes, there was a decrease in survival\nprobability for adults, but the dropoff was most severe for crew members.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/nau695mppsoxx0f6bns1ieo7kh1bje0j.rda\", f)\nfit <- get(load(f))\n\ndata(titanic)\ntitanic <- titanic %>%\n  select(-country) %>%\n  na.omit()\nx <- select(titanic, -survived)\n\nexplanation <- explain(model = fit, data = x, y = titanic$survived)\n\nPreparation of a new explainer is initiated\n  -> model label       :  train  (  default  )\n  -> data              :  2179  rows  7  cols \n  -> target variable   :  2179  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 6.0.94 , task classification (  default  ) \n  -> model_info        :  Model info detected classification task but 'y' is a factor .  (  WARNING  )\n  -> model_info        :  By deafult classification tasks supports only numercical 'y' parameter. \n  -> model_info        :  Consider changing to numerical vector with 0 and 1 values.\n  -> model_info        :  Otherwise I will not be able to calculate residuals or loss function.\n  -> predicted values  :  numerical, min =  0.007271972 , mean =  0.3239071 , max =  0.9885397  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  NA , mean =  NA , max =  NA  \n  A new explainer has been created!  \n\nprofiles <- model_profile(explainer = explanation, groups = \"class\")\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 1: Grouping the CP profiles by ticket class reveals an interaction effect with age in the Titanic dataset.\n\n\n\nWhat should we do if there are many input variables and we don’t have a\npriori knowledge about which variables \\(j^{\\prime}\\) might be interacting with\n\\(j\\)? One idea is to try to discover relevant interactions by clustering the\noriginal set of CP profiles.\nIn more detail, we can compute the CP profiles for all the samples, and then\nsee whether there are subsets of profiles that all look similar. If we find\nfeatures \\(j^{\\prime}\\) that characterize these groupings, then we have found\nfeatures that interact with \\(j\\) (with respect to the fitted model \\(f\\)). The plot\nbelow shows the same profiles as above, but clustering directly. It seems to\nrecover the interaction between age and class, even though we have not\nexplicitly provided this grouping variable.\n\n\nprofiles <- model_profile(explainer = explanation, variables = \"age\", k = 3)\nplot(profiles, geom = \"profiles\", variables = \"age\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 2: Discovered groupings in the CP profiles for age reveals an interaction effect.\n\n\n\nModel Comparison\nThe comparison of different models’ PD profiles can be used to,\nValidate a simple model\nGuide the design of new features, and\nCharacterizing overfitting\nPD profiles that are used to compare different models are sometimes called\n« Contrastive » PD profiles.\n\nTo validate a simple model, we can compare its PD profiles with those of a\nmore sophisticated model. We will illustrate this by fitting linear and random\nforest models to a dataset of apartment prices1. Given various properties of\nan apartment, the goal is to determine its price. The code below fits the two\nmodels and extracts their CP and PD profiles.\n\n\ndata(apartments)\nx <- select(apartments, -m2.price)\nprofiles_lm <- train(x, apartments$m2.price, method = \"lm\") %>%\n  explain(x, apartments$m2.price, label = \"LM\") %>%\n  model_profile()\n\nPreparation of a new explainer is initiated\n  -> model label       :  LM \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 6.0.94 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  1781.848 , mean =  3487.019 , max =  6176.032  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -247.4728 , mean =  -1.07525e-12 , max =  469.0023  \n  A new explainer has been created!  \n\nprofiles_rf <- train(x, apartments$m2.price, method = \"rf\", tuneGrid = data.frame(mtry = 10)) %>%\n  explain(x, apartments$m2.price, label = \"RF\") %>%\n  model_profile()\n\nPreparation of a new explainer is initiated\n  -> model label       :  RF \n  -> data              :  1000  rows  5  cols \n  -> target variable   :  1000  values \n  -> predict function  :  yhat.train  will be used (  default  )\n  -> predicted values  :  No value for predict function target column. (  default  )\n  -> model_info        :  package caret , ver. 6.0.94 , task regression (  default  ) \n  -> predicted values  :  numerical, min =  1660.306 , mean =  3486.588 , max =  6380.007  \n  -> residual function :  difference between y and yhat (  default  )\n  -> residuals         :  numerical, min =  -220.5063 , mean =  0.4311905 , max =  245.2902  \n  A new explainer has been created!  \n\nThe PD profile below shows that the random forest learns linear\nrelationships with price for both the surface and floor variables. If all the\neffects were like this, then we would have a good reason for preferring the\nlinear model.\n\n\nplot(profiles_lm, profiles_rf, variables = c(\"surface\", \"floor\")) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 3: A contrastive PD display suggests that the floor and surface features are linearly related with apartment price.\n\n\n\n11.. When making the comparison between a simple and a complex model, certain\ndiscrepancies might become apparent. For example, important nonlinearities or\ninteractions might be visible from the PD profiles of the complex model. This\ninformation can guide the design of new features in the simpler model, so that\nit can continue to be used. This is exactly the case in the apartments dataset\nabove – there is a strong nonlinear relationship for the construction year\nvariables. This suggests that, if a linear model is still desired, then a new\nfeature should be defined that identifies whether the apartment was built\nbetween 1935 and 1990.\n\n\nplot(profiles_lm, profiles_rf, variables = \"construction.year\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme479\n\n\n\nFigure 4: The random forest learns a nonlinear relationship between construction year and apartment price. This suggests designing new features to include in the linear model.\n\n\n\nSuppose you have found that a model is overfitting (e.g., by finding that\nit’s training error is much lower than its test error). One way to address this\noverfitting is to compare the PD profiles between the simple and complex models.\nIf the profiles are very different for one of the features, then that feature\nmay be the source of overfitting.\n\nIt is a simulated dataset, but\ndesigned to reflect properties of a real dataset.↩︎\n",
    "preview": "posts/2024-12-27-week12-2/2024-12-27-week12-2_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-13T19:05:39+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week12-3/",
    "title": "Visualization for Model Building",
    "description": "The relationship between exploratory analysis and model development.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(rstan)\nlibrary(tidyverse)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nExploratory data analysis and model building complement each other well. In\npractical problems, visualization can guide us towards more plausible models.\nWe rarely know the exact form of a model in advance, but usually have a few\nreasonable candidates. Exploratory analysis can rule out some candidates and\nsuggest new, previously unanticipated, relationships.\nWe will illustrate these ideas using an example. A researcher is interested\nin monitoring the level of PM2.5, a type of small air particlute that can be bad\nfor public health. High quality data are available from weather stations\nscattered around the world, but their data only apply locally. On the other\nhand, low quality data, available from satellites, are available everywhere. A\nmodel is desired that uses the weather station measurements to calibrate the\nsatellite data. If it works well, it could be used to monitor PM2.5 levels at\nglobal scale.\n\n\nf <- tempfile()\ndownload.file(\"https://github.com/jgabry/bayes-vis-paper/blob/master/bayes-vis.RData?raw=true\", f)\nGM <- get(load(f))\nGM@data <- GM@data %>% \n  mutate(\n    log_pm25 = log(pm25), \n    log_sat = log(sat_2014)\n  )\n\n\nThe simplest model simply fits \\(\\text{station} = a + b \\times\n\\text{satellite}\\) at locations where they are both available. This model was\nused in practice by the Global Burden of Disease project until 2016.\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.8, alpha = 0.7) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\nFigure 1: The relationship between satellite and ground station estimates of PM2.5.\n\n\n\nHowever, when we plot these two variables against one another, we notice that\nthere is still quite a bit of heterogeneity. The residuals are large — what\nfeatures might be correlated with these residuals, which if included, would\nimprove the model fit?\nThe error \\(\\epsilon_{i}\\) in a model \\(y_i = f\\left(x_{i}\\right) +\n\\epsilon_{i}\\) represents out our ignorance of the myriad of unmeasured\nfactors that determine the relationship between \\(x\\) and \\(y\\).\nFor example, desert sand is known to increase PM2.5, but it is not visible\nfrom space. The residuals are probably correlated with whether the model is in\na desert area (we underpredict PM2.5 in deserts), and so would be improved if\nwe included a term with this feature.\n\nOne hypothesis is that country region is an important factor. Below, we fit\nregression lines separately for different country super-regions, as specified by\nthe WHO. The fact that the slopes are not the same in each region means that we\nshould modify our model to have a different slope in each region1.\n\n\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = super_region_name), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = super_region_name), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"WHO Region\"\n  ) +\n  coord_fixed()\n\n\n\nFigure 2: The relationship between these variables is not the same across regions.\n\n\n\nThe WHO categorizations are somewhat arbitrary. Maybe there are better\ncountry groupings, tailored specifically to the PM2.5 problem? One idea is to\ncluster the ground stations based on PM2.5 level and use these clusters as a\ndifferent region grouping.\n\n\naverage <- GM@data %>% \n  group_by(iso3) %>% \n  summarise(pm25 = mean(pm25))\n\nclust <- dist(average) %>%\n  hclust() %>%\n  cutree(k = 6)\n\nGM@data$cluster_region <- map_chr(GM@data$iso3, ~ clust[which(average$iso3 == .)])\nggplot(GM@data, aes(log_sat, log_pm25)) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.7) +\n  geom_smooth(aes(col = cluster_region), method = \"lm\", se = F, size = 2) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    x = \"log(satellite)\",\n    y = \"log(ground station)\",\n    col = \"Cluster Region\"\n  ) +\n  coord_fixed()\n\n\n\nFigure 3: We can define clusters of regions on our own, using a hierarchical clustering.\n\n\n\nWe now have a « network » of models. We’re going to want more refined tools\nfor distinguishing between them. This is the subject of the next two lectures.\n\nViewed\ndifferently, this is like adding an interaction between the satellite\nmeasurements and WHO region.↩︎\n",
    "preview": "posts/2024-12-27-week12-3/2024-12-27-week12-3_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-13T19:05:43+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week12-4/",
    "title": "Prior and Posterior Predictives",
    "description": "Simulating data to evaluate model quality.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(rstan)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\ntheme_set(theme479)\n\n\nFrom the previous notes, we see that an exploratory analysis can motivate few\nplausible models for a dataset. How should we go about choosing between them?\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\", f)\nGM <- get(load(f))\n\n\nIn some of your other classes, you might have seen people use\ncross-validation / test set error. While this is useful (and relatively\nautomatic), it can be a black box. An alternative which often brings mores\ninsight into the structure of the problem is to use prior and posterior\npredictive distributions for (visual) model comparison.\nPrior predictive distributions\nThe prior predictive distribution can be used to decide whether certain model\nfamilies are reasonable candidates for a problem, before formally incorporating\nthe evidence coming from the data.\nThe idea is that if we can write down a generative model, then we can\nsimulate different datasets with it, even before estimating the model\nparameters. This is often the case in Bayesian models, where we can (a) sample\nparameters from the prior, and (b) simulate data from the model with those\nparameters.\nIf the simulated datasets are plausible, then the overall model class is a\nreasonable one. If they are not, then the model class should be modified. Either\nthe prior or the likelihood might need revision.\nFor example, in the example below, we simulate datasets using both a vague\nand an informative prior. Vague priors are often recommended because they are\n« more objective » in some sense. However, in this case, we see that the\nsimulated datasets are not even remotely plausible.\n\n\n# function to simulate from vague prior\nprior1 <- function(Nsim) {\n  tau0 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  tau1 <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  sigma <- 1 / sqrt(rgamma(1, 1, rate = 100))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 100)\n  beta1 <- rnorm(1, 0, 100)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\n\n\n\nprior1_data <- map_dfr(1:12, ~ prior1(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior1_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\nFigure 1: Prior predictive samples from the vague prior are on a completely implausible scale.\n\n\n\nThe block below instead simulates from a subjective, informative prior. The\nresulting samples are much more plausible, lying in a comparable range to the\ntrue data. However, note, the samples from the prior predictive do not need to\nlook exactly like the observed data — if they did, there would be no need to fit\nmodel parameters! Instead, they should look like plausible datasets that might\nhave been observed.\n\n\n# function to simulate from informative prior\nprior2 <- function(Nsim) {\n  tau0 <- abs(rnorm(1, 0, 1))\n  tau1 <- abs(rnorm(1, 0, 1))\n  sigma <- abs(rnorm(1, 0, 1))\n  beta0i <- rnorm(7, 0, tau0)\n  beta1i <- rnorm(7, 0, tau1)\n  beta0 <- rnorm(1, 0, 1)\n  beta1 <- rnorm(1, 1, 1)\n  \n  epsilon <- rnorm(Nsim, 0, sigma)\n  data.frame(\n    log_pm25 = GM$log_pm25,\n    region = GM$super_region_name,\n    sim = beta0 + beta0i[GM$super_region] + (beta1 + beta1i[GM$super_region]) * GM$log_sat + epsilon\n  )\n}\n\nprior2_data <- map_dfr(1:12, ~ prior2(Nsim = nrow(GM@data)), .id = \"replicate\")\nggplot(prior2_data, aes(x = log_pm25, y = sim)) + \n  geom_abline(slope = 1) +\n  geom_point(aes(col = region), alpha = 0.1, size = 0.4) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\nFigure 2: Prior predictive samples from the weakly informative prior seem more plausible, though they do not (and should not) exactly fit the true data.\n\n\n\nPhilosophically, this prior predictive analysis is based on the idea that,\nthough probability is subjective, evidence can be used to update our beliefs.\nThe idea of the prior predictive is to visually encode subjective beliefs about\nthe problem under study before gathering new evidence.\nPosterior predictive distributions\nOnce the prior predictive is calibrated, we can fit the model. To evaluate\nit’s quality, we can use the posterior predictive.\nThe posterior predictive is just like the prior predictive, except that it\nsamples model parameters from the data-informed posterior, rather than the\ndata-ignorant prior.\nFormally, it is the distribution of new datasets when drawing parameters\nfrom the posterior. The simulation mechanism is (a) draw model parameters from\nthe posterior and (b) simulate a dataset using parameters from (a).\nThe code below fits the three models. We are using the rstan package to\nfit three Bayesian models. The first model lm is a Bayesian linear regression,\nassuming the same slope across all regions. The two other models assume\ndifferent slopes for different regions1, but use\nthe WHO and cluster-based region definitions, respectively. You do not need to\nworry about how the rstan code, which is sourced into the stan_model\nfunction, is written. It is enough to be able to fit these two types of models\nas if they are built-in function in R.\n\n\n# Define the input datasets for the lm, region-based, and cluster-based models\ndatasets <- list(\n  \"lm\" = with(GM@data, list(N = length(log_pm25), y = log_pm25, x = log_sat)),\n  \"regions\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = super_region, R = n_distinct(super_region))\n  ),\n  \"clusters\" = with(\n    GM@data, \n    list(N = length(log_pm25), y = log_pm25, x = log_sat, group = as.integer(cluster_region), R = n_distinct(cluster_region))\n  )\n)\n\n# Define the two types of Bayesian models\nmodel_def <- list(\n  \"lm\" = stan_model(\"https://uwmadison.box.com/shared/static/hoq1whain301bj6gj670itxabnnhvcy7.stan\"),\n  \"hier\" = stan_model(\"https://uwmadison.box.com/shared/static/lvouz9jj4rbkmrx5osj2dtrhj2ycdll8.stan\")\n)\n\n\n\n\n# Fit the models above to the three datasets of interest\ncontrols <- list(max_treedepth = 15, adapt_delta = 0.99)\nmodels <- list(\n  \"lm\" = sampling(model_def$lm, data = datasets$lm, chains = 1, control = controls),\n  \"regions\" = sampling(model_def$hier, data = datasets$regions, chains = 1, control = controls),\n  \"clusters\" = sampling(model_def$hier, data = datasets$clusters, chains = 1, control = controls)\n)\n\n\nThe code above takes a little while to run (about 10 minutes for the last\ntwo models). To save some time, you can download the fitted models from the link\nbelow. The models object is a list whose elements are fitted STAN models for\nthe three model definitions above. The fitted model objects include posterior\nsamples for the region slopes as well as simulated ground station PM2.5 data,\nbased on those posterior slopes.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\", f)\nmodels <- get(load(f))\n\n\nThe block below simulates station PM2.5 data from the fitted posterior of\nthe cluster-based model. Note that, compared to the prior predictive, the\nposterior predictive is much more closely related to the true underlying\ndataset.\n\n\n# extract 12 samples and reshape it to \"long\" format\nposterior_samples <- as.matrix(models$clusters, pars = \"y_sim\")[950:961, ] %>%\n  t() %>%\n  as_tibble() %>%\n  bind_cols(GM@data) %>%\n  pivot_longer(V1:V12, names_to = \"replicate\", values_to = \"log_pm25_sim\")\n\nggplot(posterior_samples, aes(log_pm25, log_pm25_sim)) +\n  geom_abline(slope = 1) +\n  geom_point(aes(col = cluster_region), size = 0.4, alpha = 0.1) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(x = \"True station data\", y = \"Simulated station data\") +\n  facet_wrap(~ replicate, scale = \"free_y\")\n\n\n\nFigure 3: Samples from the posterior predictive in the cluster-based model.\n\n\n\nWe can verify that features of the real dataset are accurately captured by\nfeatures of the posterior predictive. One subtlety is that there is a danger of\noverfitting features in the posterior predictive. It is best to choose features\nof the data that are not directly modeled (e.g., if you use slope in the model\nestimation, then don’t evaluate the posterior predictive using the slope, since\nby definition this will be well-captured). In the block below, we compute the\nskewness for each simulated station dataset from the three different models.\nThese skewnesses are plotted as histograms, with the true dataset’s skewness\nindicated by a vertical line. It seems that the model that uses clustering to\ndefine regions is able to simulate datasets with skewness similar to that in the\nreal dataset.\n\n\napply_stat <- function(x, f) {\n  z <- as.matrix(x, pars = \"y_sim\")\n  tibble(\n    \"replicate\" = seq_len(nrow(z)),\n    \"statistic\" = apply(z, 1, f)\n  )\n}\n\nskew <- function(x) {\n  xdev <- x - mean(x)\n  n <- length(x)\n  r <- sum(xdev^3) / sum(xdev^2)^1.5\n  r * sqrt(n) * (1 - 1/n)^1.5\n}\n\nposteriors <- map_dfr(models, ~ apply_stat(., skew), .id = \"model\")\ntruth <- skew(GM@data$log_pm25)\nggplot(posteriors, aes(statistic)) +\n  geom_histogram(aes(fill = model), binwidth = 0.01) +\n  geom_vline(xintercept = truth, col = \"red\") +\n  scale_fill_brewer(palette = \"Set3\")\n\n\n\nFigure 4: Posterior simulated skewness according to the three different models.\n\n\n\n\nBayesian regression models that allow\ndifferent slopes for different groups are called hierarchical models.↩︎\n",
    "preview": "posts/2024-12-27-week12-4/2024-12-27-week12-4_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-01-13T19:07:11+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week12-5/",
    "title": "Pointwise Diagnostics",
    "description": "Evaluating the fit at particular observations in Bayesian models.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(tidyverse)\nlibrary(loo)\nlibrary(ggrepel)\nlibrary(rstan)\ntheme479 <- theme_minimal() + \n  theme(\n    panel.grid.minor = element_blank(),\n    panel.background = element_rect(fill = \"#f7f7f7\"),\n    panel.border = element_rect(fill = NA, color = \"#0c0c0c\", size = 0.6),\n    legend.position = \"bottom\"\n  )\n\n\nAll the model visualization strategies we’ve looked at in the last few\nlectures have been dataset-wide. That is, we looked at properties of the\ndataset as a whole, and whether the model made sense globally, across the whole\ndataset. Individual observations might warrant special attention, though.\nThe block below loads in the fitted models from the previous set of notes.\n\n\ndownloader <- function(link) {\n  f <- tempfile()\n  download.file(link, f)\n  get(load(f))\n}\n\nmodels <- downloader(\"https://uwmadison.box.com/shared/static/x7dotair443mhx34yzie3m3lrsvhk19a.rda\")\nGM <- downloader(\"https://uwmadison.box.com/shared/static/2pzgdu7gyobhl5tezo63tns7by1aiy6d.rda\")\n\n\nA first diagnostic to consider is the leave-one-out predictive distribution.\nThis is the probability \\(p\\left(y_{i} \\vert y_{-i}\\right)\\) of sample \\(i\\) after\nhaving fitted a model to all samples except \\(i\\). Ideally, most observations in\nthe dataset to have high predictive probability.\nNote that this can be used for model comparison. Some models might have\nbetter per-sample leave-one-out predictive probabilities for almost all\nobservations.\nThis is similar to a leave-one-out residual.\n\nIf we use rstan to fit a Bayesian model, then these leave-one-out\nprobabilities can be estimated using the loo function in the loo package.\nThe code below computes these probabilities for each model, storing the\ndifference in predictive probabilities for models two and three in the diff23\nvariable.\n\n\nelpd <- map(models, ~ loo(., save_psis = TRUE)$pointwise[, \"elpd_loo\"])\nelpd_diffs <- GM@data %>%\n  mutate(\n    ID = row_number(),\n    diff23 = elpd[[3]] - elpd[[2]]\n  )\n\noutliers <- elpd_diffs %>%\n  filter(abs(diff23) > 6)\n\n\nWe plot the difference between these predictive probabilities below. The\ninterpretation is that Ulaanbataar has much higher leave-one-out probability\nunder the cluster-based model, perhaps because that model is able to group the\ncountries with large deserts together with one another. On the other hand, Santo\nDomingo is better modeled by model 2, since it has higher leave-one-out\nprobability in that model.\n\n\nggplot(elpd_diffs, aes(ID, diff23)) +\n  geom_point(\n    aes(col = super_region_name),\n    size = 0.9, alpha = 0.8\n    ) +\n  geom_text_repel(\n    data = outliers,\n    aes(label = City_locality),\n    size = 3 \n  ) +\n  scale_color_brewer(palette = \"Set2\") +\n  labs(\n    y = \"Influence (Model 2 vs. 3)\",\n    col = \"WHO Region\"\n  )\n\n\n\nFigure 1: The difference in leave one out predictive probabilities for each sample, according to the WHO-region and cluster based hierarchical models.\n\n\n\nAnother diagnostic is to consider the influence of an observation. Formally,\nthe influence is a measure of how much the posterior predictive distribution\nchanges when we leave one sample out. The idea is to measure the difference\nbetween the posterior predictives using a form of KL divergence, and note down\nthe observations that lead to a very large difference in divergence.\n\n\n\nFigure 2: Visual intuition about the influence of observations. If the posterior predictive distributions shift substantially when an observation is included or removed, then it is an influential observation.\n\n\n\nWhen using rstan, the influence measure can be computed by the psis\nfunction. The pareto_k diagnostic summarizes how much the posterior predictive\nshifts when an observation is or isn’t included. For example, in the figure\nbelow, observation 2674 (Ulaanbaatar again) is highly influential.\n\n\nloglik <- map(models, ~ as.matrix(., pars = \"log_lik\"))\nkdata <- GM@data %>%\n  mutate(\n    k_hat = psis(loglik[[2]])$diagnostics$pareto_k,\n    Index = row_number()\n  )\noutliers <- kdata %>%\n  filter(k_hat > 0.25)\n\nggplot(kdata, aes(x = Index, y = k_hat)) + \n  geom_point(aes(col = super_region_name), size = 0.5, alpha = 0.9) + \n  scale_color_brewer(palette = \"Set2\") +\n  geom_text_repel(data = outliers, aes(label = Index)) +\n  labs(y = \"k-hat\")\n\n\n\nFigure 3: The influence of each sample on the final posterior distribution.\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week12-5/2024-12-27-week12-5_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2025-01-13T19:07:16+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week14-1/",
    "title": "Final Takeaways",
    "description": "Some major themes from STAT 436, in a nutshell.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nRecording, Rmarkdown\nWe’ve covered many practical visualization strategies in this course.\nHowever, I hope a few overarching themes have come across as well. In these\nnotes, I will try articulating a few of these themes, and they are also the\nsubject of the readings reviewed in the next few notes.\nFirst, creating a visualization is in many ways like writing an essay. A\ngreat visualization cannot simply be summoned on demand. Instead, visualizations\ngo through drafts, as the ideal graphical forms and questions of interest become\nclearer.\nVisualization is not simply for consumption by external stakeholders. In data\nscience, visualization can be used to support the process of exploration and\ndiscovery, before any takeaways have yet been found.\nVisualization can be used throughout the data science process, from data\nquality assessment to model prediction analysis. In fact, I would be wary of any\ndata science workflow that did not make use of visualization at multiple\nintermediate steps.\nIt pays dividends to think carefully about data format. A design can be easy\nto implement when the data are in tidy, but not wide, format, and vice versa.\nStructured, nontabular data – think time series, spatial formats, networks,\ntext, and images – are everywhere, and specific visualization idioms are\navailable for each.\nSimilarly, high-dimensional data are commonplace, but a small catalog of\ntechniques, like faceting, dynamic linking, PCA, and UMAP, are able to take us\nquite far.\nFinally, data visualization can be enriching. Visual thinking has helped me\nnavigate and appreciate complexity in many contexts. Through this course, I have\nshared techniques that I’ve found useful over the years, and I hope you find\nthem handy as you go off and solve real-world data science problems.\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-13T20:27:35+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-2/",
    "title": "Design Process Case Study",
    "description": "Tracing the refinement of questions and design.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nHow does a visualization expert go about creating a data visualization?\nPerhaps the most useful lesson from this reading is that good visualizations\ndon’t materialize out of thin air – there is always a creative process\ninvolved, steps where it’s unclear what the final result will be, even for a\ndata visualization genius like Shirley Wu. We’re lucky that she has documented\nthis process for us, so that we might be able to take away a few lessons for our\nown reflection.\nHer visualization, “655 Frustrations of Data\nVisualization”, is based on an online\ndata visualization survey. It had 45 questions (“How many years have you been\ndoing data visualization? What percent of your day is focused on data prep work?\n…). There are 981 responses, probably mostly submitted by the survey\ninitiator’s internet following.\n\n\n\nFigure 1: A few entries from the data visualization survey. The full data are publicly available here.\n\n\n\nQuestion Formulation\nAt the start of the project – before writing code – there is the problem of\nchoosing a guiding question. Her initial question was “Why might people leave\nthe field?” This was a timely question, because, after a few years of high\nactivity and visibility in industry, data visualization seemed to be cooling\ndown. However, this question was not directly answerable with the data at hand,\nso instead, she focused on the proxy question, “Do you want to spend more time\nor less time visualizing data in the future?”\nThis is an important lesson: there is often a distinction between what we\nreally want to know and what the data can tell us.\nWe will revisit this theme of asking sharper questions in the next reading.\nData analysis should be driven by curiosity about the world, not simply the\ndata that happen to be conveniently accessible.\n\nTo see what within the data are relevant to this guiding question, she then\nconducted an exploratory analysis1, studying the marginal\ndistributions of all the available questions. For example, visualizing the\n“percentage of time” questions, it became clear that most people worked on a mix\nof multiple data-related tasks in their work – the particular mix might help\nunderstand whether people want to stay in the field.\n\n\n\nFigure 2: Example exploratory displays of the ‘percentage of time’ questions on the data science survey.\n\n\n\nThe exploratory analysis also revealed that there are some\nquestions\nthat would not be useful for answering the guiding question. For example, many\nof the qualitative responses were not useful2. It’s easy to feel responsible for\nvisualizing all the data that are available, but this is not necessary. It’s far\nmore important to focus on the guiding question(s).\nInitial Design\nThe initial design answered whether there is a relationship between (a) the\nsurvey respondent wanting to do more data visualization in the future, and (b)\nthe current fraction of time spent on design. This was visually encoded using a\nstacked barchart. However, the display was not that informative, because most\nrespondents wanted to do more data visualization in the future3.\n\n\n\nFigure 3: The initial design used a bar chart to see whether experience was related to interest in further work in data visualization.\n\n\n\nIn this situation, it seemed like perhaps additional context would help.\nHowever, the resulting faceted barchart was difficult to make sense of, again\nbecause any relationships between variables were weak or nonexistent.\n\n\n\nFigure 4: The faceted barchart did not add much information relevant to the guiding question.\n\n\n\nRedesigns\nAt this point, two significant changes to the design were made, one to the\nquestion, and one to the design. The question was reframed from “do you want to\ncontinue working in data visualization” to “do you experience any frustrations\nwith data visualization.” This question is still related to the guiding\nquestion, but shows much more variation across respondents. For the design, the\nencoding changed from a stacked barchart to a beeswarm plot. Unlike the\nbarchart, which aggregates responses into bins, the beeswarm makes it possible\nto see every single respondent.\n\n\n\nFigure 5: A redesigned plot. The left and right panels separate respondents with and without frustrations, vertical position encodes job role, and color gives number of years of experience.\n\n\n\nA few more refinements were made. Instead of placing those with and without\nfrustrations far apart on the page, they were rearranged to share the same\n\\(x\\)-axis4. Also, instead of coloring circles by\nyears of experience, color was used to represent the percentage of the day spent\non data visualization. Again, these changes reflect sharpening of both design\nand questions.\nIn the final version of the static display, a boxplot was introduced to\nsummarize the most salient characteristics of each beeswarm. Then, instead of\njust plotting the points in two parallel regions, they were made to “rise” and\n“fall” off the boxplots, depending on whether the respondents experienced\nfrustrations. This kind of visual metaphor takes the visualization to another\nlevel; it becomes more than functional, it becomes evocative.\n\n\n\nFigure 6: The final version of the static visualization.\n\n\n\nInterpretation\nOnly at this point is interactivity introduced into the visualization. The\ninteractivity is simple – views transition into one another depending on\nselected questions – but provides an effective alternative to simply faceting\nall pairs of questions.\n\n\n\nFigure 7: The visualization above with interactivity added in.\n\n\n\nFinally, this interactive visualization is used for an extensive\nexploration. This is often when the effectiveness of a visualization can be\nevaluated. Ultimately, visualization should help inform our body of beliefs,\nguiding the actions we take (either in the short or long-term). If it’s hard to\ndraw these sorts of inferences, then a visualization is not particularly\nfunctional.\n\n\n\nFigure 8: A screenshot of notes from the designer’s exploration of the resulting visualization.\n\n\n\nTo guide the reader, this investigative work was then incorporated into the\nvisualization. These additional details allow the visualization to stand alone,\nit becomes a self-explanatory intellectual artifact.\nConclusion\nWrapping up, the final visualization is clearly the culmination of\nsubstantial intellectual labor over the course of weeks (if not months). The\nresult is both beautiful and informative. This is an ideal to strive for – the\ncrafting of data visualizations that can guide discovery and change.\nOne final note. It’s often useful to study the development of projects that\nyou find interesting. Sometimes, authors share their code on github, or earlier\nversions are available through technical reports or recorded talks. This\nadditional context can shed light on the overall inspiration and intention of\nthe project, and especially when starting out, imitation can be an effective\nstrategy for learning.\n\nUsing vega-lite!↩︎\nAre there really 100x more pie\ncharts than scatterplots in industry??↩︎\nPerhaps an\ninstance of selection bias.↩︎\nThe less our eyes have to travel across the page to make a comparison,\nthe more efficient the visualization.↩︎\n",
    "preview": "https://uploads-ssl.webflow.com/5fa9d88da1855554db37b1e1/5fc61cc4356e0d312965c372_split_beeswarm.png",
    "last_modified": "2025-01-13T20:29:48+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-3/",
    "title": "Asking Better Questions",
    "description": "What is the purpose of data analysis?",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nIn these notes, we will review “Tukey, Design Thinking, and Better Questions,”\na blog post by Roger Peng. In the post, he discusses the importance of\nformulating good questions in data analysis. While the process of refining\nquestions is central to any successful, real-world analysis, it is rarely\ndiscussed in formal statistics textbooks, which usually restrict themselves to\ndevelopments in theory, methodology, or computation.\nThe post itself is based on a line of thinking from the statistician John\nTukey’s 1962 paper, “The Future of Data Analysis.” Perhaps the most famous\nquote from this paper is,\n\nFar better an approximate answer to the right question, which is often vague,\nthan an exact answer to the wrong question, which can always be made precise.\n\nand throughout the article, Tukey argues that data analysts should not (1) be\ncalled upon to provide stamps of authority and (2) be distracted by the allure\nof proving optimal algorithms in settings that are likely not that realistic.\nThis of course begs the question – what should data analysts be doing? Tukey\nnever directly discusses this, but Peng’s thesis is that the purpose of data\nanalysis is to spark the formulation of better questions. This idea is captured\nin the diagram below. The idea is that algorithms (visual, predictive, and\ninferential) can help us strengthen evidence that certain patterns exist.\nHowever, if those patterns answer irrelevant questions, then there is little\npoint in the analysis. With so many algorithms available, it’s easy to spend\nsubstantial effort gathering evidence to answer low quality questions. Instead,\na good data scientist should focus on improving the quality of the questions\nasked, even if that means that the answers will be less definitive.\n\n\n\nFigure 1: Peng’s summary of misplaced priorities in the data science process.\n\n\n\nWhat does it mean to ask better questions? Peng argues that better questions\nare “sharper,” providing more discriminating information and guiding research to\nmore promising directions. In my view, a sharper question is one that helps\ninform decisions that have to be made (all data aside), either by adding to our\nbody of beliefs or bringing new uncertainties into focus.\nThe implication is that data analysis is not about finding “the answer.”\nInstead, it should be about clarifying what can and cannot be answered based on\nthe data. But more, it can help clarify what questions we really want answered\n– in Peng’s words, “we can learn more about ourselves by looking at the data.”\nThe final example in the reading is that the residuals in an analysis are\nthemselves data worth examining. They inform whether a model (and the associated\nway of conceptualizing the problem) is really appropriate, or whether a\nreformulation would be beneficial.\nAt the end, the main idea is that data analysis equips us to ask better\nquestions.\n\n\n\n",
    "preview": "https://raw.githubusercontent.com/krisrs1128/stat436_s23/main/activities/figure/question_evidence.png",
    "last_modified": "2025-01-13T20:29:54+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week14-4/",
    "title": "A History of Data Visualization up to 1900",
    "description": "A look at the origins of the field.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2025-01-13",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nThere could probably be an entire class taught on the history of data\nvisualization. The reason it is worth covering in an otherwise\npractically-oriented class is that,\nHistorical study can illuminate the core intellectual foundation on which the\nentire discipline of data visualization is built.\nThe limits, biases, and trends of our current era become clear when\nconsidering it within its full historical context.\nIt’s often possible to draw inspiration from past masters.\nIt can be humbling to realize that now-commonplace ideas had to be discovered.\nIf it took until 1833 until the first scatterplot was published, then what ideas\nhave we yet to find?\n\nThe reading divides up the history of data visualization into 8 epochs. In\nthese notes, we will consider the 5 epochs before 1900.\nBefore 1600: Early Maps and Diagrams\nIt might come as a surprise, but visualization has been around since the\ninvention of writing. The Egyptians made maps, and there are examples multiple\ntime series plots from the 10th century.\n\n\n\nFigure 1: Possibly the earliest multiple time series visualization, showing the movement of various planets, from In Somnium Scripionus.\n\n\n\nHowever, most visualizations were focused on physical geographical or\nastronomical quantities, and even these quantities were only imprecisely\nmeasured. Without more formal data gathering instruments, there could not be\nmuch data visualization.\n1600 - 1700: Measurement and Theory\nThe situation begins to change around 1600, at the dawn of the scientific\nrevolution. All of a sudden, precise measurement of physical space become\npossible. Also, important new mathematical ideas were introduced, like\nprobability, calculus, and reasoning about functions. This created the right\nenvironment for the design of some of the first truly sophisticated data\nvisualizations, like\nChristopher Scheiner’s 1630 visualization of sunspots over\ntime (a first instance of faceting),\n\n\n\nFigure 2: Scheiner’s visualization of sunspots, which played a role in his dialogues with Galileo.\n\n\n\nEdmond Halley’s plot of barometric pressure against altitude (an first\ninstance of one feature being plotted against another), and\n\n\n\nFigure 3: One of the first bivariate plots, relating barometric pressure to altitude. Note that absence of the true observed data.\n\n\n\nEdmond Halley’s plot of wind speed over the ocean (a first visualization of a\nvector field).\n\n\n\nFigure 4: A plot of trade winds, appearing in An Historical Account of the Trade Winds, and Monsoons, Observable in the Seas between and near the Tropicks, with an Attempt to Assign the Phisical Cause of the Said Wind.\n\n\n\nDuring this period, the scientific value of visual thinking was more or less\nestablished. However, there were still only relatively few graphical forms\navailable, and the focus continued to remain on visualizing physical quantities,\nrather than more general social, economic, biological, or ecological data.\n1700 - 1800\nDuring this period, many new graphical forms were invented, including\ntimelines, cartograms, functional interpolations, line graphs, bar charts, and\npie charts. Further, forms from earlier, like maps and function plots, became\nmore firmly established.\nThis was also when three-color printing was invented. Before this point,\ncolor could not be used as an encoding channel.\nIn this century, governments also began large-scale data collection of social\nand economic statistics1. One of\nthe most prolific inventors of data visualizations, William Playfair2, was used\nvisualization to study a variety of economic problems. In addition to inventing\nline, bar, and pie charts, he experimented with original ways of composing\nmultiple graphs to suggest the relationships between variables.\n\n\n\nFigure 5: One of William Playfair’s data visualizations, juxtaposing the price of wheat with growth in wages.\n\n\n\n1800 - 1850\nThis was a period of maturation for the field of data visualization. By this\npoint, visualization had become standard in scientific publications. Advances in\nprinting technology also made it also became easier to mass produce\nvisualizations.\nIt was also around this time that the scope of problems studied through\nvisualization expanded far beyond display of purely physical (geographical and\nastronomical) applications. Two areas in particular flourished, applications to\nsocial science and to engineering.\nVisualization in the social sciences began to emerged in response to\ngovernment sponsored collection of social statistics – data about crime,\nbirths, and deaths, among other topics. This wasn’t a purely intellectual\nexercise: understanding demographic trends was important for countries that\nwere often at war with another.\n\n\n\nFigure 6: A visualization of property crime statistics, by Andre-Michel Guerry (1829).\n\n\n\nIn engineering, the idea that visualization could serve as a computational\naid become more and more common. For example, rather the chart below, by Charles\nJoseph Minard, displays the cost of transporting goods across different\nstretches of a canal. Vertical breaks correspond to cities along the canal, and\nthe area of the square between cities encodes the cost of transportation between\nthose cities. While the information could be stored in a table, it becomes\neasier to perform mental computations (and make guesstimates) using the display.\n\n\n\nFigure 7: Minard’s 1844 visualization of the transport costs across the Canal du Centre in France.\n\n\n\n1850 - 1900: The Golden Age\nIt might be counterintuitive that there was a golden age of visualization a\ncentury before the first computers were invented. However, a look at the\nvisualizations from this period demonstrate that this was a period where\nvisualizations inspired scientific discoveries, informed commercial decisions,\nand guided social reform.\nFor example, in public health, Florence Nightingale invented new\nvisualizations to demonstrate the impact of sanitary practices in\nhospital-induced infections and death. Similarly, it was a\nvisualization that guided John\nSnow to the source of the 1855 cholera epidemic.\n\n\n\nFigure 8: Florence Nightingale’s visualization of hospital mortality statistics from the Crimean War, used to support a campaign for sanitary reforms.\n\n\n\nSome of the graphical innovations include,\n3D function plots. The plot below, by Luigi Perozzo, shows population size\nbroken down into age groups and traced over time.\n\n\n\nFigure 9: Luigi Perrozo’s 1879 3D visualizations of population over time.\n\n\n\nFlow diagrams. Charles-Joseph Minard, who we met before with the canal\nvisualization, was a master of these displays. One in particular is widely\nconsidered a masterpiece, it shows the size of Napoleon’s army during it’s\nRussian Campaign.\n\n\n\nFigure 10: Minard’s flow display of the size of Napoleon’s army during the Russia Campaign.\n\n\n\nMultivariate visualization. Francis Galton made some of the first efforts to\nvisualize more than 3 variables at a time. His Meteorographica, published in\n1863, contained over 600 visualizations of weather data that had been collected\nfor decades, but never visualized. One plot, shown below, led to the discovery\nof anticyclones.\n\n\n\nFigure 11: Galton’s display of weather patterns. Low pressure (black) areas tend to have clockwise wind patterns, while high pressure (red) tends to have anticlockwise wind patterns.\n\n\n\nThis was also an age of state-sponsored atlases. More than sponsoring the\ncollection of data, governments assembled teams to visualize the results for\nofficial publication. From 1879 to 1897, the French Ministry of Public Works\npublished the Albums de Statistique Graphique, which under the guidance of\nÉmile Cheysson, developed some of the most imaginative and ambitious\nvisualizations of the era.\n\n\n\nFigure 12: A visualization of the flow of passengers and goods through railways from Paris. Each square shows the breakdown to cities further away, and color encodes the railines.\n\n\n\n\nThe word “statistics” comes from the same root as\n“state”, since it originally focused on data collected for governance.↩︎\nHe has\nbeen described as an “engineer, political economist and scoundrel.”↩︎\n",
    "preview": "https://uwmadison.box.com/shared/static/c79fbmfc3mff9ota1tzxtvv4gp7e3pul.jpeg",
    "last_modified": "2025-01-13T20:31:03+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week07-01/",
    "title": "Spatial Data Formats",
    "description": "An overview of common formats, with illustrative examples.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-01-10",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nknitr::opts_chunk$set(cache = FALSE, message = FALSE, warning = FALSE, echo = TRUE, eval = TRUE)\n\n\n\n\nlibrary(ceramic)\nlibrary(raster)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\nlibrary(tmap)\ntheme_set(theme_minimal())\n\n\nSpatial data come in two main formats: vector and raster. We’ll examine them\nin detail in the next few lectures, but this lecture motivates the high-level\ndistinction and gives a few examples. It also shows how to read and write data\nto and from these formats.\nVector Data\nVector data formats are used to store geometric information, like the\nlocations of hospitals (points), trajectories of bus routes (lines), or\nboundaries of counties (polygons). It’s useful to think of the associated data\nas being spatially enriched data frames, with each row corresponding to one of\nthese geometric features.\nVector data are usually stored in .geojson, .wkt, .shp, or .topojson\nformats. Standard data.frames cannot be used because then important spatial\nmetadata would be lost, like the Coordinate Reference System (to be explained in\nthe fourth lecture this week).\nIn R, these formats can be read using read_sf in the sf package. They can\nbe written using the write_sf function. Here, we’ll read in a vector dataset\ncontaining the boundaries of lakes in Madison.\n\n\nlakes <- read_sf(\"https://uwmadison.box.com/shared/static/duqpj0dl3miltku1676es64d5zmygy92.geojson\")\n \n lakes %>%\n   dplyr::select(id, name, geometry)\n\nSimple feature collection with 10 features and 2 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -89.54084 ymin: 42.94762 xmax: -89.17699 ymax: 43.2051\nGeodetic CRS:  WGS 84\n# A tibble: 10 × 3\n   id               name                                      geometry\n   <chr>            <chr>                                <POLYGON [°]>\n 1 relation/1997948 Lake Monona    ((-89.37974 43.0714, -89.37984 43.…\n 2 relation/3361036 Lake Mendota   ((-89.46885 43.08266, -89.46864 43…\n 3 relation/3447583 Upper Mud Lake ((-89.31364 43.04483, -89.31361 43…\n 4 relation/4090134 Hook Lake      ((-89.33198 42.94909, -89.33161 42…\n 5 relation/4231143 Lake Wingra    ((-89.4265 43.05514, -89.4266 43.0…\n 6 relation/6183370 Lake Waubesa   ((-89.32949 42.99166, -89.32908 42…\n 7 relation/7083727 Lake Kegonsa   ((-89.2648 42.9818, -89.26399 42.9…\n 8 relation/9668475 Lower Mud Lake ((-89.28011 42.98486, -89.2794 42.…\n 9 way/21415784     Goose Lake     ((-89.53957 42.97967, -89.53947 42…\n10 way/28721778     Brazee Lake    ((-89.18562 43.19529, -89.18533 43…\n\n#write_sf(lakes, \"output.geojson\", driver = \"GeoJSON\")\n\n\nWe’ll discuss plotting in the next lecture, but for a preview, this is how you\ncan visualize the lakes using ggplot2.\n\n\nlakes <- lakes %>%\n   group_by(id) %>%\n   mutate(\n     longitude = st_coordinates(geometry)[1, 1],\n     latitude = st_coordinates(geometry)[1, 2]\n   )\n \n tm_shape(lakes) +\n   tm_polygons(col = \"#00ced1\")\n\n\n\nWith a little extra effort, we can overlay the features onto public map\nbackgrounds (these are often called “basemaps”).\n\n\n# you can get your own at https://account.mapbox.com/access-tokens/\n Sys.setenv(MAPBOX_API_KEY=\"pk.eyJ1Ijoia3Jpc3JzMTEyOCIsImEiOiJjbDYzdjJzczQya3JzM2Jtb2E0NWU1a3B3In0.Mk4-pmKi_klg3EKfTw-JbQ\")\n basemap <- cc_location(loc= c(-89.401230, 43.073051), buffer = 15e3)\n \n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(lakes) +\n   tm_polygons(col = \"#00ced1\")\n\n\n\nThere is a surprising amount of public vector data available online. Using this query1, I’ve downloaded locations of all hospital clinics in Madison.\n\n\nclinics <- read_sf(\"https://uwmadison.box.com/shared/static/896jdml9mfnmza3vf8bh221h9hlvh70v.geojson\")\n \n # how would you overlay the names of the clinics, using geom_text?\n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(clinics) +\n   tm_dots(col = \"red\", size = 1)\n\n\n\nUsing this query, I’ve downloaded all the\nbus routes.\n\n\nbus <- read_sf(\"https://uwmadison.box.com/shared/static/5neu1mpuh8esmb1q3j9celu73jy1rj2i.geojson\")\n \n tm_shape(basemap) +\n   tm_rgb() +\n   tm_shape(bus) +\n   tm_lines(col = \"#bc7ab3\", size = 1)\n\n\n\nFor the boundaries of the lakes above, I used this\nquery.\nMany organizations prepare geojson data themselves and make it publicly\navailable; e.g., the boundaries of\nrivers\nor\nglaciers.\nDon’t worry about how to visualize these data at this point — I just want to\ngive some motivating examples.\nRaster Data\nRaster data give a measurement along a spatial grid. You can think of them as\nspatially enriched matrices, where the metadata says where on the earth each\nentry of the matrix is associated with.\nRaster data are often stored in tiff format. They can be read in using the\nrast function in the terra library, and can be written using\nwriteRaster.\n\n\nshanghai <- rast(\"https://uwmadison.box.com/shared/static/u4na56w3r4eqg232k2ma3eqbvehfiaoq.tif\")\n #writeRaster(shanghai, \"output.tiff\", driver = \"GeoTIFF\")\n\n\nSome of the most common types of public raster data are satellite images or\nderived measurements, like elevation maps. For example, the code below shows an\nimage of a neighborhood outside Shanghai.\n\n\ntm_shape(shanghai / 1636 * 255) + # rescale the max value to 255\n  tm_rgb()\n\n|---------|---------|---------|---------|=========================================                                          |---------|---------|---------|---------|=========================================                                          \n\n\nThere’s actually quite a bit of information in this image. We can zoom in…\n\n\nbbox <- ext(121.66, 121.665, 30.963, 30.968)\nshanghai_ <- crop(shanghai, bbox)\n\ntm_shape(shanghai_ / 1636 * 255) +\n  tm_rgb()\n\n\n\nHere are is data on elevation in Zion national park.\n\n\nf <- system.file(\"raster/srtm.tif\", package = \"spDataLarge\")\nzion <- rast(f)\ntm_shape(zion) +\n  tm_raster(palette = \"PuBuGn\") +\n  tm_layout(legend.position = c(\"left\", \"bottom\"))\n\n\n\nInstallation\nA note about R packages: for historical reasons, spatial data libraries in R\nreference a few command line programs, like gdal and proj. Since these\ncommand line programs are not themselves a part of R, they need to be installed\nbefore the corresponding R packages. The process will differ from operating\nsystem to operating system, and the experience can be frustrating, especially\nwhen the R packages don’t recognize the underlying system installation. I\nrecommend following the instructions on this\npage and reaching out early if you have any\nissues.\n\nIt can be constructed easily using the wizard↩︎\n",
    "preview": "posts/2024-12-27-week07-01/2024-12-27-week07-01_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2025-01-14T17:32:36+02:00",
    "input_file": "2024-12-27-week07-01.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week13-1/",
    "title": "Introduction to Feature Learning",
    "description": "An introduction to compositional feature learning.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-01-07",
    "categories": [],
    "contents": "\n\nReading,\nRecording,\nRmarkdown\n\n\n\nlibrary(keras)\n\n\n\n\nIn classical machine learning, we assume that the features most relevant\nto prediction are already available. E.g., when we want to predict home\nprice, we already have features about square feet and neighborhood\nincome, which are clearly relevant to the prediction task.\n\n\n\n\nIn many modern problems though, we have only access to data where the\nmost relevant features have not been directly encoded.\n\n\nIn image classification, we only have raw pixel values. We may want to\npredict whether a pedestrian is in an image taken from a self-driving\ncar, but we have only the pixels to work with. It would be useful to\nhave an algorithm come up with labeled boxes like those in the examples\nbelow.\n\n\nFor sentiment analysis, we want to identify whether a piece of text is a\npositive or negative sentiment. However, we only have access to the raw\nsequence of words, without any other context. Examples from the IMDB\ndataset are shown below.\n\n\nIn both of these examples, this information could be encoded manually,\nbut it would a substantial of effort, and the manual approach could not\nbe used in applications that are generating data constantly. In a way,\nthe goal of these algorithms is to distill the raw data down into a\nsuccinct set of descriptors that can be used for more classical machine\nlearning or decision making.\n\n\n\n\n\n\nFigure 1: An example of the types of labels that would be useful to\nhave, starting from just the raw image.\n\n\n\n\nExample reviews from the IMDB dataset:\n\n  positive,\"A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"\"has got all the polari ....\"\n  positive,\"I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be ...\"\n  negative,\"Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to ...\"\n  positive,\"Petter Mattei's \"\"Love in the Time of Money\"\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a ...\"\n\n\nIn these problems, the relevant features only arise as complex\ninteractions between the raw data elements.\n\n\nTo recognize a pedestrian, we need many adjacent pixels to have a\nparticular configuration of values, leading to combinations of edges and\nshapes, which when viewed together, become recognizable as a person.\n\n\nTo recognize a positive or negative sentiment, we need to recognize\ninteractions between words. “The movie was good until” clearly has bad\nsentiment, but you cannot tell that from the isolated word counts alone.\n\n\n\n\nThe main idea of deep learning is to learn these more complex features\none layer at a time. For image data, the first layer recognizes\ninteractions between individual pixels. Specifically, individual\nfeatures are designed to “activate” when particular pixel interactions\nare present. The second layer learns to recognize interactions between\nfeatures in the first layer, and so on, until the learned features\ncorrespond to more “high-level” concepts, like sidewalk or pedestrian.\n\n\n\n\nBelow is a toy example of how an image is processed into feature\nactivations along a sequence of layers. Each pixel within the feature\nmaps correspond to a patch of pixels in the original image – those later\nin the network have a larger field of view than those early on. A pixel\nin a feature map has a large value if any of the image features that it\nis sensitive to are present within its field of vision.\n\n\n\n\n\n\nFigure 2: A toy diagram of feature maps from the model loaded below.\nEarly layers have fewer, but larger feature maps, while later layers\nhave many, but small ones. The later layers typically contain\nhigher-level concepts used in the final predictions.\n\n\n\n\n\nAt the end of the feature extraction process, all the features are\npassed into a final linear or logistic regression module that completes\nthe regression or classification task, respectively.\n\n\n\n\nIt is common to refer to each feature map as a neuron. Different neurons\nactivate when different patterns are present in the original, underlying\nimage.\n\n\n\n\n\n\nFigure 3: An illustration of the different spatial contexts of feature\nmaps at different layers. An element of a feature map has a large value\n(orange in the picture) if the feature that it is sensitive to is\npresent in its spatial context. Higher-level feature maps are smaller,\nbut each pixel within it has a larger spatial context.\n\n\n\n\nBelow, we load a model to illustrate the concept of multilayer networks.\nThis model has 11 layers followed by a final logistic regression layer.\nThere are many types of layers. Each type of layer contributes in a\ndifferent way to the feature learning goal, and learning how design and\ncompose these different types of layers is one of the central concerns\nof deep learning. The Output Shape column describes the number and shape\nof feature maps associated with each layer. For example, the first layer\nhas 32 feature maps, each of size \\(148 \\times 148\\). Deeper parts of\nthe network have more layers, but each is smaller. We will see how to\nload and inspect these features in the next lecture.\n\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\nmodel\n\nModel: \"sequential_1\"\n______________________________________________________________________\n Layer (type)                  Output Shape                Param #    \n======================================================================\n conv2d_7 (Conv2D)             (None, 148, 148, 32)        896        \n max_pooling2d_7 (MaxPooling2  (None, 74, 74, 32)          0          \n D)                                                                   \n conv2d_6 (Conv2D)             (None, 72, 72, 64)          18496      \n max_pooling2d_6 (MaxPooling2  (None, 36, 36, 64)          0          \n D)                                                                   \n conv2d_5 (Conv2D)             (None, 34, 34, 128)         73856      \n max_pooling2d_5 (MaxPooling2  (None, 17, 17, 128)         0          \n D)                                                                   \n conv2d_4 (Conv2D)             (None, 15, 15, 128)         147584     \n max_pooling2d_4 (MaxPooling2  (None, 7, 7, 128)           0          \n D)                                                                   \n flatten_1 (Flatten)           (None, 6272)                0          \n dropout (Dropout)             (None, 6272)                0          \n dense_3 (Dense)               (None, 512)                 3211776    \n dense_2 (Dense)               (None, 1)                   513        \n======================================================================\nTotal params: 3453121 (13.17 MB)\nTrainable params: 3453121 (13.17 MB)\nNon-trainable params: 0 (0.00 Byte)\n______________________________________________________________________\n\n\n\nWhile we will only consider image data in this course, the idea of\nlearning complex features by composing a few types of layers is a\ngeneral one. For example, in sentiment analysis, the first layer learns\nfeatures that activate when specific combinations of words are present\nin close proximity to one another. The next layer learns interactions\nbetween phrases, and later layers are responsive to more sophisticated\ngrammar.\n\n\n\n\nDeep learning is often called a black box because these intermediate\nfeatures are often complex and not directly interpretable according to\nhuman concepts. The problem is further complicated by the fact that\nfeatures are “distributed” in the sense that a single human concept can\nbe encoded by a configuration of multiple features. Conversely, the same\nmodel feature can encode several human concepts.\n\n\n\n\nFor this reason, a literature has grown around the question of\ninterpreting neural networks. The field relies on visualization and\ninteraction to attempt to understand the learned representations, with\nthe goal of increasing the safety and scientific usability of deep\nlearning models. While our class will not discuss how to design or\ndevelop deep learning models, we will get a taste of the\ninterpretability literature in the next few lectures.\n\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2025-01-13T19:07:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2024-12-27-week13-2/",
    "title": "Visualizing Learned Features",
    "description": "A first look at activations in a deep learning model.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-01-07",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"keras\")\nlibrary(\"purrr\")\nlibrary(\"RColorBrewer\")\n\n\nIn the last lecture, we discussed the conceptual foundations of feature\nlearning. In this lecture, we’ll see how to extract and visualize features\nlearned by a computer vision model.\nWe will inspect a model that was trained1 to\ndistinguish between photos of cats and dogs. We’ve included a subsample of the\ntraining dataset below – the full dataset can be downloaded\nhere. From the printout, you can\nsee that we have saved 20 images, each of size \\(150 \\times 150\\) pixels, and with\nthree color channels (red, green, and blue).\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/o7t3nt77iv3twizyv7yuwqnca16f9nwi.rda\", f)\nimages <- get(load(f))\ndim(images) # 20 sample images\n\n[1]  20 150 150   3\n\nLet’s take a look at a few examples from the training dataset. We’ve randomly\nsampled 10 dogs and 10 cats. The command par allows us to plot many images\nside by side (in this case, in a \\(4 \\times 5\\) grid).\n\n\npar(mfrow = c(4, 5), mai = rep(0.00, 4))\nout <- images %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\nFigure 1: A sample of 20 random images from the dog vs. cat training dataset.\n\n\n\nThe array_tree function above splits the 4D array into a collection of 3D\nslices. Each of these 3D slices corresponds to one image — the three channels\ncorrespond to red, green, and blue colors, respectively. The next map line\nplots each of the resulting 3D arrays\nNext, let’s consider what types of features the model has learned, in order\nto distinguish between cats and dogs. Our approach will be to compute\nactivations on a few images and visualize them as 2D feature maps. These\nvisualizations will help us see whether there are systematic patterns in what\nleads to an activation for a particular neuron.\nTo accomplish this, we will create an R object to retrieve all the\nintermediate feature activations associated with an input image. Every time we\ncall this object on a new image, it will return the activations for features at\nall layers.\n\n\n# download model\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\nlayer_outputs <- map(model$layers, ~ .$output)\nactivation_model <- keras_model(inputs = model$input, outputs = layer_outputs)\nfeatures <- predict(activation_model, images)\n\n\nEach element of features corresponds to a different layer. Within a single\nlayer, the 3D array provides the activations of each feature across different\nspatial windows. For example, for the first layer, there are 32 features with\nactivations spread across a 148 x 148 grid, each grid element with its own\nspatial context.\n\n\ndim(features[[1]])\n\n[1]  20 148 148  32\n\nThe block below visualizes the first feature map in the first layer. We plot\nthe associated input image next to it. This feature seems to be a horizontal\nedge detector – it activates whenever there are transitions from dark to light\nareas when moving vertically. For example, when the white leash goes over the\nshadow in the background, this feature has some of its highest activations.\n\n\nplot_feature <- function(feature) {\n  rotate <- function(x) t(apply(x, 2, rev))\n  image(rotate(feature), axes = FALSE, asp = 1, col = brewer.pal(4, \"Blues\"))\n}\n\nix <- 3\npar(mfrow = c(1, 2), mai = rep(0.00, 4))\nplot(as.raster(images[ix,,, ], max = 255))\nplot_feature(features[[1]][ix,,, 1])\n\n\n\nFigure 2: An image and its activations for the first neuron in layer 1.\n\n\n\nLet’s visualize a few more of these features. We see more vertical and\nhorizontal edge detectors — features with high values at sharp changes in color\nin the underlying images. This is consistent with our earlier claim that the\nfirst layer of a network learns to recognize pixel-level interactions.\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[2]][ix,,,] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\nFigure 3: Activations for a collection of neurons at layer 2, for the same image as given above.\n\n\n\nNext, we visualize features at a higher level in the network. At this point,\neach activation corresponds to a larger spatial context in the original image,\nso there are fewer activations per feature. There are more feature maps total,\nbut each is smaller. It’s not so clear what these feature maps correspond to,\nbut there do seem to be a few that are clearly activated within the dog, and\nothers that are sensitive to the foliage in the background.\n\n\npar(mfrow = c(6, 7), mai = rep(0.00, 4))\nout <- features[[6]][ix,,,1:40] %>%\n  array_branch(margin = 3) %>%\n  map(~ plot_feature(.))\n\n\n\nFigure 4: Activations for a collection of neurons at layer 6.\n\n\n\nWhile we had some interpretations for these higher-level features, it’s hard\nto know definitively, since we are only considering a single image. In the next\nset of notes, we will examine the same neuron across many dataset examples, and\nthis will give us more confidence in our interpretations of individual neurons.\n\nFor those who are curious, the\ntraining code is\nhere).\nOf course, you will not be responsible for understanding this step.↩︎\n",
    "preview": "posts/2024-12-27-week13-2/2024-12-27-week13-2_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2025-01-12T20:54:49+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week13-3/",
    "title": "Collections of Features",
    "description": "Analyzing feature activations across datasets",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-01-07",
    "categories": [],
    "contents": "\nReading (1, 2), Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"keras\")\nlibrary(\"magrittr\")\nlibrary(\"pdist\")\nlibrary(\"purrr\")\nlibrary(\"stringr\")\nlibrary(\"superheat\")\nlibrary(\"tidymodels\")\nset.seed(479)\n\n\nThe previous notes gave us a look into the features learned by a deep\nlearning model. However, we could only look at one feature within one layer at a\ntime. We also only studied an individual image. If we want to better understand\nthe representations learned by a network, we will need ways of analyzing\ncollections of features taken from throughout the network, across entire\ndatasets.\nThis seems like an impossible task, but it turns out that, in real-world\nmodels, the learned features tend to be highly correlated. Certain patterns of\nactivation tend to recur across similar images. This kind of structure makes it\npossible to use clustering and dimensionality-reduction to begin to make sense\nof the learned representations of individual networks.\nTo illustrate this idea, we will download the same model from before along\nwith a larger subsample of images used in training.\n\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/dxibamcr0bcmnj7xazqxnod8wtew70m2.rda\", f)\nimages <- get(load(f))\n\nf <- tempfile()\ndownload.file(\"https://uwmadison.box.com/shared/static/9wu6amgizhgnnefwrnyqzkf8glb6ktny.h5\", f)\nmodel <- load_model_hdf5(f)\n\n\nThe code block below save features from layers 6 and 8 from this model, for\nall the images we downloaded. The code for extracting features is the same as\nfrom the previous lecture, except instead of extracting features from all\nlayers, we’re only considering these later ones. The reason we’ve focused on\nthese deeper layers is that (1) they are smaller, so we will consume less memory\non our computers, and (2) they correspond to the higher-level concepts which are\nmore difficult to understand directly, unlike the simple edge detectors in the\ninitial layers.\n\n\nl <- c(model$layers[[6]]$output, model$layers[[8]]$output)\nactivation_model <- keras_model(inputs = model$input, outputs = l)\nfeatures <- predict(activation_model, images)\n\n\nideally, we could work with a matrix of samples by features. the \\(ij^{th}\\)\nelement would be the activation of feature \\(j\\) on observation \\(i\\).\nthis is unfortunately not immediately available. as we saw before, each\nfeature map is actually a small array across spatial contexts, not a single\nnumber. there is no single way to aggregate across a feature map, and it is\ncommon to see people use the maximum, average, norm, or variance of a feature\nmap as a summary for how strongly that feature activates on a given image. we\nwill take the mean of activations.\n\n\nfeature_means <- function(h) {\n  apply(h, c(1, 4), mean) %>%\n    as_tibble()\n}\n\nh <- map_dfc(features, feature_means) %>%\n  set_colnames(str_c(\"feature_\", 1:ncol(.))) %>%\n  mutate(id = row_number())\n\n\nGiven this array, we can ask questions like, which neurons are most activated\nfor a particular image? or, which images induce the largest activations for a\nparticular neuron? In the block below, we find the 20 images that activate the\nmost for the third feature map in layer 6. This neuron seems to have learned to\nrecognize grass. Perhaps unsurprisingly, all the images are of dogs.\n\n\ntop_ims <- h %>%\n  slice_max(feature_3, n = 20) %>%\n  pull(id)\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nout <- images[top_ims,,,] %>% \n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\nFigure 1: The 20 images in the training dataset with the highest activations for neuron 3 in layer 6. This neuron seems to be sensitive to the presence of grass in an image (which happens to be correlated with whether a dog is present).\n\n\n\nThis particular example should serve as a kind of warning. While it’s easy to\nimbue models with human-like characteristics, they often arrive at the answers\nthey need in unexpected ways. We asked the model to distinguish between cats and\ndogs, but it is using whether the image has grass in it as a predictor. While\nfor this dataset this may be accurate, I would expect this model to fail on an\nimage of a cat in a grassy field.\nInstead of only investigating one neuron, we can consider all the images and\nneurons simultaneously. The code below makes a heatmap of the average feature\nactivations from before. Each row is an image and each column is a feature from\neither layers 6 or 8. A similar example is given in the reading, where\ncoordinated views reveal that certain patterns of neuron activation encode the\nlifts of the pen or specific curve shapes in a handwriting generation network.\n\n\nsuperheat(\n  h %>% select(-id),\n  pretty.order.rows = TRUE,\n  pretty.order.cols = TRUE,\n  legend = FALSE\n)\n\n\n\nFigure 2: A heatmap of feature map activations for layers 6 and 8, across the entire dataset. Each row is an image, and each column is a neuron. There is limited clustering structure, but there are substantial differences in how strongly different neurons activate on average.\n\n\n\nWe can also apply clustering and dimensionality reduction ideas to understand\nthe collection of mean feature activations. For example, below we run \\(K\\)-means\nacross images, with the hope of finding images that are viewed similarly\naccording to the model. The 20 images closest to the centroid of cluster 3 are\nprinted below. It seems like the model has learned to cluster all the orange\nimages together1. This\nis a little surprising, considering that there are both cats and dogs that are\norange, so this isn’t a particularly discriminating feature. It also suggests a\nway to improve the model – we could train it on recolorized input images, so\nthat it is forced to discover features that are unrelated to color2.\n\n\nsub <- function(x) {\n  select(x, starts_with(\"feature\"))\n}\n\ncluster_result <- kmeans(sub(h), centers = 25, nstart = 20)\ncentroids <- tidy(cluster_result)\nD <- pdist(sub(centroids), sub(h)) %>%\n  as.matrix()\n\npar(mfrow = c(5, 4), mai = rep(0.00, 4))\nnear_centroid <- order(D[3, ])[1:20]\nout <- images[near_centroid,,, ] %>%\n  array_tree(1) %>%\n  map(~ plot(as.raster(., max = 255)))\n\n\n\nFigure 3: The 20 images closest to the centroid of cluster 3 in the feature activation space. This cluster seems to include images with many orange pixels.\n\n\n\nIn general, we can treat the activations generated by a deep learning model\nas themselves the object of data analysis. This can help us determine whether\nthe kinds of features that we want it to (high-level concepts, rather than just\ncolors or textures). It can also highlight instances where the model learns\nfeatures associated with concepts that we would rather it be invariant to (e.g.,\nchanges in season, variations in lighting).\n\nSometimes, it’s just looking at the color of the floor!↩︎\nIn the deep\nlearning community, this would be called using color augmentation to enforce\ninvariance.↩︎\n",
    "preview": "posts/2024-12-27-week13-3/2024-12-27-week13-3_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2025-01-12T20:54:49+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 1344
  },
  {
    "path": "posts/2024-12-27-week13-4/",
    "title": "Optimizing Feature Maps",
    "description": "Interpreting neurons by finding optimal inputs",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      }
    ],
    "date": "2024-01-07",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\n\n\nlibrary(\"dplyr\")\nlibrary(\"purrr\")\nlibrary(\"keras\")\nlibrary(\"tensorflow\")\n\n\nSo far, we’ve visualized neural networks by analyzing the activations of\nlearned features across observed samples. A complementary approach is to ask\ninstead — is there a hypothetical image that would maximize the activation of a\nparticular neuron? If we can construct such an image, then we might have a\nbetter sense of the types of image concepts to which a neuron is highly\nsensitive.\nWe will illustrate these ideas on a network that has been trained on\nImagenet. This is a large image dataset with many (thousands of) class labels,\nand it is often used to evaluate image classification algorithms. The network is\nloaded below.\n\n\nmodel <- application_vgg16(weights = \"imagenet\", include_top = FALSE)\n\n\nThe main idea is to setup an optimization problem that searches through image\nspace for an image that maximizes the activation for a particular neuron. The\nfunction below computes the average activation of a one of the feature maps. The\ngoal is to find an image that maximizes this value for a given feature.\n\n\nmean_activation <- function(image, layer, ix=1) {\n  h <- layer(image)\n  k_mean(h[,,, ix])\n}\n\n\nTo implement this, we can compute the gradient of a neuron’s average\nactivation with respect to input image pixel values. This is a measure of how\nmuch the activation would change when individual pixel values are perturbed. The\nfunction below moves an input image in the direction of steepest ascent for the\nmean_activation function above.\n\n\ngradient_step <- function(image, layer, ix=1, lr=1e-3) {\n  with(tf$GradientTape() %as% tape, {\n    tape$watch(image)\n    objective <- mean_activation(image, layer, ix)\n  })\n  grad <- tape$gradient(objective, image)\n  image <- image + lr * grad\n}\n\n\n\n\n\nFigure 1: Starting from a random image, we can take a gradient step in the image space to increase a given neuron’s mean activation.\n\n\n\nOnce these gradients can be computed, it’s possible to perform gradient\nascent to solve the activation maximization problem. This ascent is encoded by\nthe function below. We initialize with a random uniform image and then take\nn_iter gradient steps in the direction that maximizes the activation of\nfeature ix.\n\n\nrandom_image <- function() {\n  tf$random$uniform(map(c(1, 150, 150, 3), as.integer))\n}\n\ngradient_ascent <- function(layer, ix = 1, n_iter = 100, lr = 10) {\n  im_seq <- array(0, dim = c(n_iter, 150, 150, 3))\n  image <- random_image()\n  for (i in seq_len(n_iter)) {\n    image <- gradient_step(image, layer, ix, lr)\n    im_seq[i,,,] <- as.array(image[1,,,])\n  }\n  \n  im_seq\n}\n\n\n\n\n\nFigure 2: Taking many gradient steps leads us towards an image that optimizes a neuron’s activation.\n\n\n\nBelow, we visualize the images that optimize the activations for a few\nneurons in layer 3. These neurons seem to be most responsive particular colors\nand edge orientations.\n\n\nsquash <- function(x) {\n  (x - min(x)) / (max(x) - min(x))\n}\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[3]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\nFigure 3: The hypothetical images that maximize the activations for 40 different neurons. These neurons seem to pull out features related to color and edge orientations.\n\n\n\nWe can think of these features as analogous to a collection of basis\nfunctions. At the first layer, the network is representing each image as a\ncombination of basis images, related to particular color or edge patterns.\nWe can compare these activation maximizing inputs with those associated with\nlater layers. It seems that the basis images at this level are more intricate,\nreflecting textures and common objects across this dataset. For example, the\npolka dot pattern may be strongly activated by cat eyes.\n\n\npar(mfrow = c(5, 8), mai = rep(0.00, 4))\nactivation_model <- keras_model(inputs = model$input, outputs = model$layers[[8]]$output)\nfor (i in seq_len(40)) {\n  im_seq <- gradient_ascent(activation_model, ix = i)\n  plot(as.raster(squash(im_seq[100,,,])))\n}\n\n\n\nFigure 4: The results of the corresponding optimization for 40 neurons in layer 8.\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week13-4/2024-12-27-week13-4_files/figure-html5/unnamed-chunk-9-1.png",
    "last_modified": "2025-01-12T20:54:49+02:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2024-12-27-week03-01/",
    "title": "Faceting",
    "description": "Using small multiples to create information dense plots.",
    "author": [
      {
        "name": "Kris Sankaran",
        "url": {}
      },
      {
        "name": "",
        "url": {}
      }
    ],
    "date": "2023-01-10",
    "categories": [],
    "contents": "\nReading, Recording, Rmarkdown\nIt might seem like we’re limited with the total number of variables we can\ndisplay at a time. While there are many types of encodings we could in theory\nuse, only a few them are very effective, and they can interfere with one\nanother.\nNot all is lost, though! A very useful idea for visualizing high-dimensional\ndata is the idea of small multiples. It turns out that our eyes are pretty good\nat making sense of many small plots, as long as there is some shared structure\nacross the plots.\n\nLet’s see these ideas in action. These are libraries we need.\n\n\nlibrary(tidyverse)\nlibrary(dslabs)\ntheme_set(theme_bw())\n\n\nIn ggplot2, we can implement this idea using the facet_wrap and facet_grid\ncommands. We specify the column in the data.frame along which we want to\ngenerate comparable small multiples.\n\n\nyears <- c(1962, 1980, 1990, 2000, 2012)\ncontinents <- c(\"Europe\", \"Asia\")\ngapminder_subset <- gapminder %>%\n  filter(year %in% years, continent %in% continents)\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(. ~ year) +\n  theme(legend.position = \"bottom\")\n\n\n\nIn facet grid, you specify whether you want the plot to be repeated across rows\nor columns, depending on whether you put the variable before or after the tilde.\n\n\nggplot(\n    gapminder_subset, \n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ .) +\n  theme(legend.position = \"bottom\")\n\n\n\nYou can also facet by more than one variable at a time, specifying which\nvariables should go in rows and which should go in columns again using the\ntilde.\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(fertility, life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_grid(year ~ continent) +\n  theme(legend.position = \"bottom\")\n\n\n\nSometimes, you just want to see the display repeated over groups, but you don’t\nreally need them to all appear in the same row or column. In this case, you can\nuse facet_wrap.\n\n\nggplot(\n    gapminder %>% filter(year %in% years),\n    aes(x = fertility, y = life_expectancy, col = continent)\n  ) +\n  geom_point() +\n  facet_wrap(~ year)\n\n\n\nJust to illustrate, faceting makes sense for datasets other than scatterplots.\nThis example also shows that faceting will apply to multiple geom layers at\nonce.\nThe dataset shows the abundances of five different bacteria across three\ndifferent subjects over time, as they were subjected to antibiotics. The data\nwere the basis for this study.\n\n\nantibiotic <- read_csv(\"https://uwmadison.box.com/shared/static/5jmd9pku62291ek20lioevsw1c588ahx.csv\")\nhead(antibiotic)\n\n# A tibble: 6 × 7\n  species  sample value ind    time svalue antibiotic     \n  <chr>    <chr>  <dbl> <chr> <dbl>  <dbl> <chr>          \n1 Unc05qi6 D1         0 D         1   NA   Antibiotic-free\n2 Unc05qi6 D2         0 D         2   NA   Antibiotic-free\n3 Unc05qi6 D3         0 D         3    0   Antibiotic-free\n4 Unc05qi6 D4         0 D         4    0   Antibiotic-free\n5 Unc05qi6 D5         0 D         5    0   Antibiotic-free\n6 Unc05qi6 D6         0 D         6    0.2 Antibiotic-free\n\nI have also separately computed running averages for each of the variables –\nthis is in the svalue column. We’ll discuss ways to do this during the week on\ntime series visualization.\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind) +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\nIt seems like some of the species are much more abundant than others. In this\nsituation, it might make sense to rescale the \\(y\\)-axis. Though, this is always\na risky decision – people might easily misinterpret the plot and conclude that\nthe different species all have the same abundances. Nonetheless, it can’t hurt\nto try, using the scale argument to facet_grid.\n\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\nUnlike the years example, the facets don’t automatically come with their own\nnatural order. We can define an order based on the average value of the\nresponses over the course of the survey, and then change the factor levels of\nthe Species column to reorder the panels.\n\n\nspecies_order <- antibiotic %>%\n  group_by(species) %>%\n  summarise(avg_value = mean(value)) %>%\n  arrange(desc(avg_value)) %>%\n  pull(species)\n\nantibiotic <- antibiotic %>%\n  mutate(species = factor(species, levels = species_order))\n\nggplot(antibiotic, aes(x = time)) +\n  geom_line(aes(y = svalue), size = 1.2) +\n  geom_point(aes(y = value, col = antibiotic), size = 0.5, alpha = 0.8) +\n  facet_grid(species ~ ind, scale = \"free_y\") +\n  scale_color_brewer(palette = \"Set2\") +\n  theme(strip.text.y = element_text(angle = 0))\n\n\n\n\n\n\n",
    "preview": "posts/2024-12-27-week03-01/2024-12-27-week03-01_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2025-01-14T17:37:49+02:00",
    "input_file": "2024-12-27-week03-01.knit.md",
    "preview_width": 1344,
    "preview_height": 576
  }
]
